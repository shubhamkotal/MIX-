{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of clear_implementation_LayoutLM.ipynb","provenance":[{"file_id":"1Q9Xg6TrpMRB5SX--TnBoBkT2PJH3TtDE","timestamp":1597226239129}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"e93b24f7a0b54667ab2f55cab884a217":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_eac4981f5c9448cf8ea335eb6bf2a3cd","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c72a5c3430554796a51dacc357aa4774","IPY_MODEL_8ae47b774f3549b997e53a0a779a0bc7"]}},"eac4981f5c9448cf8ea335eb6bf2a3cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c72a5c3430554796a51dacc357aa4774":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3616b4ae16494628b8d6f427f7731c3c","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":440473133,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":440473133,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7b7087c4752c406c98fc826059da2fd4"}},"8ae47b774f3549b997e53a0a779a0bc7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e71e4a9f79b244b0a2b20b169f0a7a24","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 440M/440M [00:25&lt;00:00, 17.3MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_92861b059e224ef1ad9efb283bf18e64"}},"3616b4ae16494628b8d6f427f7731c3c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"7b7087c4752c406c98fc826059da2fd4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e71e4a9f79b244b0a2b20b169f0a7a24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"92861b059e224ef1ad9efb283bf18e64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"sE3Oy-IAOHTh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1597226316170,"user_tz":-330,"elapsed":28800,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"ed43a893-26ca-4fcb-99fb-ff02d8f9334d"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4YFvo6OBQkhv","colab_type":"text"},"source":["## getting the github repo files"]},{"cell_type":"code","metadata":{"id":"b9u5GAodOvhH","colab_type":"code","colab":{}},"source":["#!git clone https://github.com/microsoft/unilm.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nVLNZ7ggHCEP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597226370345,"user_tz":-330,"elapsed":1622,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"96163e4b-bcd8-40ee-8a51-891fa8696515"},"source":["%cd '/content/gdrive/My Drive/Microsoft_layoutlm/unilm/layoutlm/examples/seq_labeling/'"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/Microsoft_layoutlm/unilm/layoutlm/examples/seq_labeling\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zIoHzVydOzJm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":845},"executionInfo":{"status":"ok","timestamp":1597226387312,"user_tz":-330,"elapsed":14198,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"79b62990-1e42-4166-c1ad-1e3d0d26ec63"},"source":["!pip install seqeval transformers==2.9.0 tensorboardX"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting seqeval\n","  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n","Collecting transformers==2.9.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n","\u001b[K     |████████████████████████████████| 645kB 3.9MB/s \n","\u001b[?25hCollecting tensorboardX\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n","\u001b[K     |████████████████████████████████| 317kB 14.4MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n","Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (4.41.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (2.23.0)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 16.4MB/s \n","\u001b[?25hCollecting tokenizers==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 21.7MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (2019.12.20)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 40.3MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (0.7)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (3.0.4)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.0) (0.16.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (49.2.0)\n","Building wheels for collected packages: seqeval, sacremoses\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=df9d5921ec2f2f6d25fdc95b4436a1d2919d0541ea2a6016835435f4a15e624b\n","  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=3dd96bd4b426936ca737c93cc900b6a5364363806a6fc9a60864cab39754c79a\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built seqeval sacremoses\n","Installing collected packages: seqeval, sacremoses, tokenizers, sentencepiece, transformers, tensorboardX\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 seqeval-0.0.12 tensorboardX-2.1 tokenizers-0.7.0 transformers-2.9.0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"adKoTEKBRT_4","colab_type":"text"},"source":["**running the bash commands and downlaoding the dataset**"]},{"cell_type":"code","metadata":{"id":"XXIMtsufQ5Ps","colab_type":"code","colab":{}},"source":["#!wget https://guillaumejaume.github.io/FUNSD/dataset.zip\n","\n","#!unzip dataset.zip && mv dataset data && rm -rf dataset.zip __MACOSX\n","#!bash preprocess.sh"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"URP8tWz6TyYu","colab_type":"text"},"source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","**preprocess.py for Funsd dataset**\n"]},{"cell_type":"code","metadata":{"id":"8ZTbnZ0CQ5zb","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597226405156,"user_tz":-330,"elapsed":6352,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["import argparse\n","import json\n","import os\n","\n","from PIL import Image\n","from transformers import AutoTokenizer\n","\n","\n","def bbox_string(box, width, length):\n","    return (\n","        str(int(1000 * (box[0] / width)))\n","        + \" \"\n","        + str(int(1000 * (box[1] / length)))\n","        + \" \"\n","        + str(int(1000 * (box[2] / width)))\n","        + \" \"\n","        + str(int(1000 * (box[3] / length)))\n","    )\n","\n","\n","def actual_bbox_string(box, width, length):\n","    return (\n","        str(box[0])\n","        + \" \"\n","        + str(box[1])\n","        + \" \"\n","        + str(box[2])\n","        + \" \"\n","        + str(box[3])\n","        + \"\\t\"\n","        + str(width)\n","        + \" \"\n","        + str(length)\n","    )\n","\n","\n","def convert(args):\n","    with open(\n","        os.path.join(args.output_dir, args.data_split + \".txt.tmp\"),\n","        \"w\",\n","        encoding=\"utf8\",\n","    ) as fw, open(\n","        os.path.join(args.output_dir, args.data_split + \"_box.txt.tmp\"),\n","        \"w\",\n","        encoding=\"utf8\",\n","    ) as fbw, open(\n","        os.path.join(args.output_dir, args.data_split + \"_image.txt.tmp\"),\n","        \"w\",\n","        encoding=\"utf8\",\n","    ) as fiw:\n","        for file in os.listdir(args.data_dir):\n","            file_path = os.path.join(args.data_dir, file)\n","            with open(file_path, \"r\", encoding=\"utf8\") as f:\n","                data = json.load(f)\n","            image_path = file_path.replace(\"annotations\", \"images\")\n","            image_path = image_path.replace(\"json\", \"png\")\n","            file_name = os.path.basename(image_path)\n","            image = Image.open(image_path)\n","            width, length = image.size\n","            for item in data[\"form\"]:\n","                words, label = item[\"words\"], item[\"label\"]\n","                words = [w for w in words if w[\"text\"].strip() != \"\"]\n","                if len(words) == 0:\n","                    continue\n","                if label == \"other\":\n","                    for w in words:\n","                        fw.write(w[\"text\"] + \"\\tO\\n\")\n","                        fbw.write(\n","                            w[\"text\"]\n","                            + \"\\t\"\n","                            + bbox_string(w[\"box\"], width, length)\n","                            + \"\\n\"\n","                        )\n","                        fiw.write(\n","                            w[\"text\"]\n","                            + \"\\t\"\n","                            + actual_bbox_string(w[\"box\"], width, length)\n","                            + \"\\t\"\n","                            + file_name\n","                            + \"\\n\"\n","                        )\n","                else:\n","                    if len(words) == 1:\n","                        fw.write(words[0][\"text\"] + \"\\tS-\" + label.upper() + \"\\n\")\n","                        fbw.write(\n","                            words[0][\"text\"]\n","                            + \"\\t\"\n","                            + bbox_string(words[0][\"box\"], width, length)\n","                            + \"\\n\"\n","                        )\n","                        fiw.write(\n","                            words[0][\"text\"]\n","                            + \"\\t\"\n","                            + actual_bbox_string(words[0][\"box\"], width, length)\n","                            + \"\\t\"\n","                            + file_name\n","                            + \"\\n\"\n","                        )\n","                    else:\n","                        fw.write(words[0][\"text\"] + \"\\tB-\" + label.upper() + \"\\n\")\n","                        fbw.write(\n","                            words[0][\"text\"]\n","                            + \"\\t\"\n","                            + bbox_string(words[0][\"box\"], width, length)\n","                            + \"\\n\"\n","                        )\n","                        fiw.write(\n","                            words[0][\"text\"]\n","                            + \"\\t\"\n","                            + actual_bbox_string(words[0][\"box\"], width, length)\n","                            + \"\\t\"\n","                            + file_name\n","                            + \"\\n\"\n","                        )\n","                        for w in words[1:-1]:\n","                            fw.write(w[\"text\"] + \"\\tI-\" + label.upper() + \"\\n\")\n","                            fbw.write(\n","                                w[\"text\"]\n","                                + \"\\t\"\n","                                + bbox_string(w[\"box\"], width, length)\n","                                + \"\\n\"\n","                            )\n","                            fiw.write(\n","                                w[\"text\"]\n","                                + \"\\t\"\n","                                + actual_bbox_string(w[\"box\"], width, length)\n","                                + \"\\t\"\n","                                + file_name\n","                                + \"\\n\"\n","                            )\n","                        fw.write(words[-1][\"text\"] + \"\\tE-\" + label.upper() + \"\\n\")\n","                        fbw.write(\n","                            words[-1][\"text\"]\n","                            + \"\\t\"\n","                            + bbox_string(words[-1][\"box\"], width, length)\n","                            + \"\\n\"\n","                        )\n","                        fiw.write(\n","                            words[-1][\"text\"]\n","                            + \"\\t\"\n","                            + actual_bbox_string(words[-1][\"box\"], width, length)\n","                            + \"\\t\"\n","                            + file_name\n","                            + \"\\n\"\n","                        )\n","            fw.write(\"\\n\")\n","            fbw.write(\"\\n\")\n","            fiw.write(\"\\n\")\n","\n","\n","def seg_file(file_path, tokenizer, max_len):\n","    subword_len_counter = 0\n","    output_path = file_path[:-4]\n","    with open(file_path, \"r\", encoding=\"utf8\") as f_p, open(\n","        output_path, \"w\", encoding=\"utf8\"\n","    ) as fw_p:\n","        for line in f_p:\n","            line = line.rstrip()\n","\n","            if not line:\n","                fw_p.write(line + \"\\n\")\n","                subword_len_counter = 0\n","                continue\n","            token = line.split(\"\\t\")[0]\n","\n","            current_subwords_len = len(tokenizer.tokenize(token))\n","\n","            # Token contains strange control characters like \\x96 or \\x95\n","            # Just filter out the complete line\n","            if current_subwords_len == 0:\n","                continue\n","\n","            if (subword_len_counter + current_subwords_len) > max_len:\n","                fw_p.write(\"\\n\" + line + \"\\n\")\n","                subword_len_counter = current_subwords_len\n","                continue\n","\n","            subword_len_counter += current_subwords_len\n","\n","            fw_p.write(line + \"\\n\")\n","\n","\n","def seg(args):\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        args.model_name_or_path, do_lower_case=True\n","    )\n","    seg_file(\n","        os.path.join(args.output_dir, args.data_split + \".txt.tmp\"),\n","        tokenizer,\n","        args.max_len,\n","    )\n","    seg_file(\n","        os.path.join(args.output_dir, args.data_split + \"_box.txt.tmp\"),\n","        tokenizer,\n","        args.max_len,\n","    )\n","    seg_file(\n","        os.path.join(args.output_dir, args.data_split + \"_image.txt.tmp\"),\n","        tokenizer,\n","        args.max_len,\n","    )"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zwGxsygXkrph","colab_type":"text"},"source":["**preprocessing for custom microsoft annotation tool code**"]},{"cell_type":"code","metadata":{"id":"03x369JXkq42","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597226425980,"user_tz":-330,"elapsed":2126,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["import os\n","import sys\n","import json\n","from PIL import Image\n","\n","import argparse\n","import json\n","import os\n","\n","from PIL import Image\n","from transformers import AutoTokenizer\n","\n","\n","def bbox_string(box, width, length):\n","    return (\n","        str(int(1000 * (box[0] / width)))\n","        + \" \"\n","        + str(int(1000 * (box[1] / length)))\n","        + \" \"\n","        + str(int(1000 * (box[2] / width)))\n","        + \" \"\n","        + str(int(1000 * (box[3] / length)))\n","    )\n","\n","\n","def actual_bbox_string(box, width, length):\n","    return (\n","        str(box[0])\n","        + \" \"\n","        + str(box[1])\n","        + \" \"\n","        + str(box[2])\n","        + \" \"\n","        + str(box[3])\n","        + \"\\t\"\n","        + str(width)\n","        + \" \"\n","        + str(length)\n","    )\n","\n","#our custom function\n","def get_label_by_word(text,bounding_boxes,data_labelling1,width,length):\n","  for i in data_labelling1['labels']:\n","      for j in i['value']:\n","          #checking the word\n","          if(text == j['text']):               \n","              #checking the match of the coordinates\n","              if(sum([round(j['boundingBoxes'][0][0:2][0]*width),round(j['boundingBoxes'][0][0:2][1]*length),round(j['boundingBoxes'][0][4:6][0]*width),round(j['boundingBoxes'][0][4:6][1]*length)])/sum(bounding_boxes[0])<2.5):\n","                  return i['label']\n","\n","def convert(args):\n","    with open(\n","        os.path.join(args.output_dir, args.data_split + \".txt.tmp\"),\n","        \"w\",\n","        encoding=\"utf8\",\n","    ) as fw, open(\n","        os.path.join(args.output_dir, args.data_split + \"_box.txt.tmp\"),\n","        \"w\",\n","        encoding=\"utf8\",\n","    ) as fbw, open(\n","        os.path.join(args.output_dir, args.data_split + \"_image.txt.tmp\"),\n","        \"w\",\n","        encoding=\"utf8\",\n","    ) as fiw:\n","        for file in os.listdir(args.data_dir):\n","          #labelling\n","          file_path_labelling = os.path.join(args.data_dir, file)\n","          #OCR\n","          file_path_OCR = file_path_labelling.replace(\"Labelling_annotations\", \"OCR_annotations\")\n","          #reading labeling data\n","          with open(file_path_labelling, \"r\", encoding=\"utf8\") as f:\n","              data_labelling = json.load(f)\n","          \n","          #reading OCR data\n","          with open(file_path_OCR, \"r\", encoding=\"utf8\") as f:\n","              data_OCR1 = json.load(f)\n","\n","          #updating the path\n","          image_path = file_path_OCR.replace(\"OCR_annotations\", \"Image\")\n","          \n","          image_path = image_path.replace(\"json\", \"png\")\n","          file_name = os.path.basename(image_path)\n","          \n","          #reading image.\n","          image = Image.open(image_path)\n","          width, length = image.size\n","\n","          \n","\n","          for i in data_OCR1['analyzeResult']['readResults']:\n","            for j in i['lines']:\n","                \n","                #THE string is getting splitted over here.\n","                #print(\"\\ntext :- \",j['text'].split(\" \"))\n","                word_list = []\n","                label_list = []\n","                bounding_box = []\n","                \n","                #The word and each coordinates are being seperated over here.\n","                for k in j['words']:\n","                    #getting word\n","                    word = k['text']\n","                    word_list.append(word)\n","                    \n","                    #getting label for particular word\n","                    label_sub = get_label_by_word(k['text'],[[k['boundingBox'][0:2][0],k['boundingBox'][0:2][1],k['boundingBox'][4:6][0],k['boundingBox'][4:6][1]]],data_labelling,width, length)\n","                    label_list.append(label_sub)\n","                    \n","                    #getting bounding boxes\n","                    bounding_box.append([k['boundingBox'][0:2][0],k['boundingBox'][0:2][1],k['boundingBox'][4:6][0],k['boundingBox'][4:6][1]])\n","                \n","                #handling the special cases.\n","                #print(\"\\nThe word list :- \",word_list)\n","                #print(\"\\nThe label_list :- \",label_list)\n","                \n","                \n","                #condition 1 :- \n","                \n","                #If we have only one label in the entire string\n","                if(len(set(label_list))==1):\n","                    label = label_list[0]\n","                    words = []\n","                    for word_index in range(0,len(word_list)):\n","                        dic_buffer = {}\n","                        dic_buffer['box'] = bounding_box[word_index]\n","                        dic_buffer['text'] = word_list[word_index]\n","                        words.append(dic_buffer)\n","                    #print(\"\\n The label is this condition 1:- \",label)\n","                    #print(\"\\n The word is this condition 1:- \",words)\n","                    \n","\n","\n","\n","                    #write other logic THE OTHER PART OF CODE\n","                    words = [w for w in words if w[\"text\"].strip() != \"\"]\n","                    if len(words) == 0:\n","                        continue\n","                    if label == \"other\":\n","                        for w in words:\n","                            fw.write(w[\"text\"] + \"\\tO\\n\")\n","                            fbw.write(\n","                                w[\"text\"]\n","                                + \"\\t\"\n","                                + bbox_string(w[\"box\"], width, length)\n","                                + \"\\n\"\n","                            )\n","                            fiw.write(\n","                                w[\"text\"]\n","                                + \"\\t\"\n","                                + actual_bbox_string(w[\"box\"], width, length)\n","                                + \"\\t\"\n","                                + file_name\n","                                + \"\\n\"\n","                            )\n","                    else:\n","                        if len(words) == 1:\n","                            fw.write(words[0][\"text\"] + \"\\tS-\" + label.upper() + \"\\n\")\n","                            fbw.write(\n","                                words[0][\"text\"]\n","                                + \"\\t\"\n","                                + bbox_string(words[0][\"box\"], width, length)\n","                                + \"\\n\"\n","                            )\n","                            fiw.write(\n","                                words[0][\"text\"]\n","                                + \"\\t\"\n","                                + actual_bbox_string(words[0][\"box\"], width, length)\n","                                + \"\\t\"\n","                                + file_name\n","                                + \"\\n\"\n","                            )\n","                        else:\n","                            fw.write(words[0][\"text\"] + \"\\tB-\" + label.upper() + \"\\n\")\n","                            fbw.write(\n","                                words[0][\"text\"]\n","                                + \"\\t\"\n","                                + bbox_string(words[0][\"box\"], width, length)\n","                                + \"\\n\"\n","                            )\n","                            fiw.write(\n","                                words[0][\"text\"]\n","                                + \"\\t\"\n","                                + actual_bbox_string(words[0][\"box\"], width, length)\n","                                + \"\\t\"\n","                                + file_name\n","                                + \"\\n\"\n","                            )\n","                            for w in words[1:-1]:\n","                                fw.write(w[\"text\"] + \"\\tI-\" + label.upper() + \"\\n\")\n","                                fbw.write(\n","                                    w[\"text\"]\n","                                    + \"\\t\"\n","                                    + bbox_string(w[\"box\"], width, length)\n","                                    + \"\\n\"\n","                                )\n","                                fiw.write(\n","                                    w[\"text\"]\n","                                    + \"\\t\"\n","                                    + actual_bbox_string(w[\"box\"], width, length)\n","                                    + \"\\t\"\n","                                    + file_name\n","                                    + \"\\n\"\n","                                )\n","                            fw.write(words[-1][\"text\"] + \"\\tE-\" + label.upper() + \"\\n\")\n","                            fbw.write(\n","                                words[-1][\"text\"]\n","                                + \"\\t\"\n","                                + bbox_string(words[-1][\"box\"], width, length)\n","                                + \"\\n\"\n","                            )\n","                            fiw.write(\n","                                words[-1][\"text\"]\n","                                + \"\\t\"\n","                                + actual_bbox_string(words[-1][\"box\"], width, length)\n","                                + \"\\t\"\n","                                + file_name\n","                                + \"\\n\"\n","                            )\n","                    fw.write(\"\\n\")\n","                    fbw.write(\"\\n\")\n","                    fiw.write(\"\\n\")\n","                \n","                #condition 2 :- \n","                \n","                #If we have multiple label in the string where there is some label called others\n","                elif((\"other\" in label_list) or (\"others\" in label_list) or (\"Other\" in label_list) or (\"Others\" in label_list)):\n","                    #calculating the percentage of occurance of a label\n","                    percentage_occurance_labels=[label_list.count(i)/len(label_list) for i in label_list] \n","                    #getting max percentage labels\n","                    label = label_list[dict(zip(percentage_occurance_labels,range(len(percentage_occurance_labels))))[max(percentage_occurance_labels)]]\n","                    words = []\n","                    \n","                    for word_index in range(0,len(word_list)):\n","                        dic_buffer = {}\n","                        dic_buffer['box'] = bounding_box[word_index]\n","                        dic_buffer['text'] = word_list[word_index]\n","                        words.append(dic_buffer)\n","                        \n","                    \n","                    \n","                    \n","                    #print(\"\\n The label is this condition 2:-\",label)\n","                    #print(\"\\n The word is this condition 2\",words)\n","                    #write other logic\n","                    words = [w for w in words if w[\"text\"].strip() != \"\"]\n","                    if len(words) == 0:\n","                        continue\n","                    if label == \"other\":\n","                        for w in words:\n","                            fw.write(w[\"text\"] + \"\\tO\\n\")\n","                            fbw.write(\n","                                w[\"text\"]\n","                                + \"\\t\"\n","                                + bbox_string(w[\"box\"], width, length)\n","                                + \"\\n\"\n","                            )\n","                            fiw.write(\n","                                w[\"text\"]\n","                                + \"\\t\"\n","                                + actual_bbox_string(w[\"box\"], width, length)\n","                                + \"\\t\"\n","                                + file_name\n","                                + \"\\n\"\n","                            )\n","                    else:\n","                        if len(words) == 1:\n","                            fw.write(words[0][\"text\"] + \"\\tS-\" + label.upper() + \"\\n\")\n","                            fbw.write(\n","                                words[0][\"text\"]\n","                                + \"\\t\"\n","                                + bbox_string(words[0][\"box\"], width, length)\n","                                + \"\\n\"\n","                            )\n","                            fiw.write(\n","                                words[0][\"text\"]\n","                                + \"\\t\"\n","                                + actual_bbox_string(words[0][\"box\"], width, length)\n","                                + \"\\t\"\n","                                + file_name\n","                                + \"\\n\"\n","                            )\n","                        else:\n","                            fw.write(words[0][\"text\"] + \"\\tB-\" + label.upper() + \"\\n\")\n","                            fbw.write(\n","                                words[0][\"text\"]\n","                                + \"\\t\"\n","                                + bbox_string(words[0][\"box\"], width, length)\n","                                + \"\\n\"\n","                            )\n","                            fiw.write(\n","                                words[0][\"text\"]\n","                                + \"\\t\"\n","                                + actual_bbox_string(words[0][\"box\"], width, length)\n","                                + \"\\t\"\n","                                + file_name\n","                                + \"\\n\"\n","                            )\n","                            for w in words[1:-1]:\n","                                fw.write(w[\"text\"] + \"\\tI-\" + label.upper() + \"\\n\")\n","                                fbw.write(\n","                                    w[\"text\"]\n","                                    + \"\\t\"\n","                                    + bbox_string(w[\"box\"], width, length)\n","                                    + \"\\n\"\n","                                )\n","                                fiw.write(\n","                                    w[\"text\"]\n","                                    + \"\\t\"\n","                                    + actual_bbox_string(w[\"box\"], width, length)\n","                                    + \"\\t\"\n","                                    + file_name\n","                                    + \"\\n\"\n","                                )\n","                            fw.write(words[-1][\"text\"] + \"\\tE-\" + label.upper() + \"\\n\")\n","                            fbw.write(\n","                                words[-1][\"text\"]\n","                                + \"\\t\"\n","                                + bbox_string(words[-1][\"box\"], width, length)\n","                                + \"\\n\"\n","                            )\n","                            fiw.write(\n","                                words[-1][\"text\"]\n","                                + \"\\t\"\n","                                + actual_bbox_string(words[-1][\"box\"], width, length)\n","                                + \"\\t\"\n","                                + file_name\n","                                + \"\\n\"\n","                            )\n","                    fw.write(\"\\n\")\n","                    fbw.write(\"\\n\")\n","                    fiw.write(\"\\n\")\n","                \n","                \n","                # condition 3 :- \n","                \n","                #where important labels are present in the same string.\n","                else:\n","                    #seperating the words whic contain similar\n","                    for label_name in set(label_list):\n","                        label_index=[index for index,value in enumerate(label_list) if value == label_name]\n","                        #getting label name\n","                        label = label_name\n","                        words = []\n","                        for index in label_index:\n","                            dic_buffer = {}\n","                            dic_buffer['box'] = bounding_box[index]\n","                            dic_buffer['text'] = word_list[index]\n","                            words.append(dic_buffer)\n","                        #print(\"\\n The label is this condition 3:-\",label)\n","                        #print(\"\\n The word is this condition 3\",words)\n","                        \n","\n","                        #write other logic\n","                        words = [w for w in words if w[\"text\"].strip() != \"\"]\n","                        if len(words) == 0:\n","                            continue\n","                        if label == \"other\":\n","                            for w in words:\n","                                fw.write(w[\"text\"] + \"\\tO\\n\")\n","                                fbw.write(\n","                                    w[\"text\"]\n","                                    + \"\\t\"\n","                                    + bbox_string(w[\"box\"], width, length)\n","                                    + \"\\n\"\n","                                )\n","                                fiw.write(\n","                                    w[\"text\"]\n","                                    + \"\\t\"\n","                                    + actual_bbox_string(w[\"box\"], width, length)\n","                                    + \"\\t\"\n","                                    + file_name\n","                                    + \"\\n\"\n","                                )\n","                        else:\n","                            if len(words) == 1:\n","                                fw.write(words[0][\"text\"] + \"\\tS-\" + label.upper() + \"\\n\")\n","                                fbw.write(\n","                                    words[0][\"text\"]\n","                                    + \"\\t\"\n","                                    + bbox_string(words[0][\"box\"], width, length)\n","                                    + \"\\n\"\n","                                )\n","                                fiw.write(\n","                                    words[0][\"text\"]\n","                                    + \"\\t\"\n","                                    + actual_bbox_string(words[0][\"box\"], width, length)\n","                                    + \"\\t\"\n","                                    + file_name\n","                                    + \"\\n\"\n","                                )\n","                            else:\n","                                fw.write(words[0][\"text\"] + \"\\tB-\" + label.upper() + \"\\n\")\n","                                fbw.write(\n","                                    words[0][\"text\"]\n","                                    + \"\\t\"\n","                                    + bbox_string(words[0][\"box\"], width, length)\n","                                    + \"\\n\"\n","                                )\n","                                fiw.write(\n","                                    words[0][\"text\"]\n","                                    + \"\\t\"\n","                                    + actual_bbox_string(words[0][\"box\"], width, length)\n","                                    + \"\\t\"\n","                                    + file_name\n","                                    + \"\\n\"\n","                                )\n","                                for w in words[1:-1]:\n","                                    fw.write(w[\"text\"] + \"\\tI-\" + label.upper() + \"\\n\")\n","                                    fbw.write(\n","                                        w[\"text\"]\n","                                        + \"\\t\"\n","                                        + bbox_string(w[\"box\"], width, length)\n","                                        + \"\\n\"\n","                                    )\n","                                    fiw.write(\n","                                        w[\"text\"]\n","                                        + \"\\t\"\n","                                        + actual_bbox_string(w[\"box\"], width, length)\n","                                        + \"\\t\"\n","                                        + file_name\n","                                        + \"\\n\"\n","                                    )\n","                                fw.write(words[-1][\"text\"] + \"\\tE-\" + label.upper() + \"\\n\")\n","                                fbw.write(\n","                                    words[-1][\"text\"]\n","                                    + \"\\t\"\n","                                    + bbox_string(words[-1][\"box\"], width, length)\n","                                    + \"\\n\"\n","                                )\n","                                fiw.write(\n","                                    words[-1][\"text\"]\n","                                    + \"\\t\"\n","                                    + actual_bbox_string(words[-1][\"box\"], width, length)\n","                                    + \"\\t\"\n","                                    + file_name\n","                                    + \"\\n\"\n","                                )\n","                        fw.write(\"\\n\")\n","                        fbw.write(\"\\n\")\n","                        fiw.write(\"\\n\")    \n","            \n","\n","\n","def seg_file(file_path, tokenizer, max_len):\n","    subword_len_counter = 0\n","    output_path = file_path[:-4]\n","    with open(file_path, \"r\", encoding=\"utf8\") as f_p, open(\n","        output_path, \"w\", encoding=\"utf8\"\n","    ) as fw_p:\n","        for line in f_p:\n","            line = line.rstrip()\n","\n","            if not line:\n","                fw_p.write(line + \"\\n\")\n","                subword_len_counter = 0\n","                continue\n","            token = line.split(\"\\t\")[0]\n","\n","            current_subwords_len = len(tokenizer.tokenize(token))\n","\n","            # Token contains strange control characters like \\x96 or \\x95\n","            # Just filter out the complete line\n","            if current_subwords_len == 0:\n","                continue\n","\n","            if (subword_len_counter + current_subwords_len) > max_len:\n","                fw_p.write(\"\\n\" + line + \"\\n\")\n","                subword_len_counter = current_subwords_len\n","                continue\n","\n","            subword_len_counter += current_subwords_len\n","\n","            fw_p.write(line + \"\\n\")\n","\n","\n","def seg(args):\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        args.model_name_or_path, do_lower_case=True\n","    )\n","    seg_file(\n","        os.path.join(args.output_dir, args.data_split + \".txt.tmp\"),\n","        tokenizer,\n","        args.max_len,\n","    )\n","    seg_file(\n","        os.path.join(args.output_dir, args.data_split + \"_box.txt.tmp\"),\n","        tokenizer,\n","        args.max_len,\n","    )\n","    seg_file(\n","        os.path.join(args.output_dir, args.data_split + \"_image.txt.tmp\"),\n","        tokenizer,\n","        args.max_len,\n","    )                    "],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"gIA9J26eHX5v","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597226457197,"user_tz":-330,"elapsed":4631,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"a1a43e4b-6f85-4109-8b01-19d81fda52f9"},"source":["ls"],"execution_count":6,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mdata\u001b[0m/      \u001b[01;34moutput_dir\u001b[0m/    preprocess.sh  run_seq_labeling.py\n","\u001b[01;34mlayoutlm\u001b[0m/  preprocess.py  \u001b[01;34mruns\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"mz4fzziAV19V","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597226467276,"user_tz":-330,"elapsed":951,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["class preprocessing_data_input:\n","  def __init__(self,data_dir = \"data/training_data/annotations\",data_split = \"train\",output_dir=\"data\",model_name_or_path=\"bert-base-uncased\",max_len=510):\n","    self.data_dir = data_dir\n","    self.data_split = data_split\n","    self.output_dir = output_dir\n","    self.model_name_or_path = model_name_or_path\n","    self.max_len = max_len"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"msSuu3VYIWXf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1597226714342,"user_tz":-330,"elapsed":4569,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"9b2cb2d0-df99-4ff2-bc6b-9149d827c9f4"},"source":["ls"],"execution_count":8,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mdata\u001b[0m/      \u001b[01;34moutput_dir\u001b[0m/    preprocess.sh  run_seq_labeling.py\n","\u001b[01;34mlayoutlm\u001b[0m/  preprocess.py  \u001b[01;34mruns\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"9qf_1tlOYQkj","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1597226902219,"user_tz":-330,"elapsed":44035,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["args = preprocessing_data_input(\"data/custom_data/micro_soft_annotation_file/Labelling_annotations\",\"train\",\"data\",\"bert-base-uncased\",510)\n","convert(args)\n","seg(args)\n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ZfZ5fagpqHw","colab_type":"text"},"source":["**creating labels.txt**"]},{"cell_type":"code","metadata":{"id":"sxZ2a2R3p8pl","colab_type":"code","colab":{}},"source":["\n","!cat data/train.txt | cut -d$'\\t' -f 2 | grep -v \"^$\"| sort | uniq > data/labels.txt\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j77A2FlUqyNh","colab_type":"text"},"source":["**train the model**"]},{"cell_type":"markdown","metadata":{"id":"1V63aG5q-LJu","colab_type":"text"},"source":["***layoutlm.py***"]},{"cell_type":"code","metadata":{"id":"ripg2WCb97Zr","colab_type":"code","colab":{}},"source":["import logging\n","\n","import torch\n","from torch import nn\n","from torch.nn import CrossEntropyLoss, MSELoss\n","from transformers import BertConfig, BertModel, BertPreTrainedModel\n","from transformers.modeling_bert import BertLayerNorm\n","\n","logger = logging.getLogger(__name__)\n","\n","LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP = {}\n","\n","LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP = {}\n","\n","\n","class LayoutlmConfig(BertConfig):\n","    pretrained_config_archive_map = LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP\n","    model_type = \"bert\"\n","\n","    def __init__(self, max_2d_position_embeddings=1024, **kwargs):\n","        super().__init__(**kwargs)\n","        self.max_2d_position_embeddings = max_2d_position_embeddings\n","\n","\n","class LayoutlmEmbeddings(nn.Module): \n","    def __init__(self, config):\n","        super(LayoutlmEmbeddings, self).__init__()\n","        self.word_embeddings = nn.Embedding(\n","            config.vocab_size, config.hidden_size, padding_idx=0\n","        )\n","        self.position_embeddings = nn.Embedding(\n","            config.max_position_embeddings, config.hidden_size\n","        )\n","        self.x_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.y_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.h_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.w_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.token_type_embeddings = nn.Embedding(\n","            config.type_vocab_size, config.hidden_size\n","        )\n","\n","        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n","        # any TensorFlow checkpoint file\n","        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        token_type_ids=None,\n","        position_ids=None,\n","        inputs_embeds=None,\n","    ):\n","        seq_length = input_ids.size(1)\n","        if position_ids is None:\n","            position_ids = torch.arange(\n","                seq_length, dtype=torch.long, device=input_ids.device\n","            )\n","            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        words_embeddings = self.word_embeddings(input_ids)\n","        position_embeddings = self.position_embeddings(position_ids)\n","        left_position_embeddings = self.x_position_embeddings(bbox[:, :, 0])\n","        upper_position_embeddings = self.y_position_embeddings(bbox[:, :, 1])\n","        right_position_embeddings = self.x_position_embeddings(bbox[:, :, 2])\n","        lower_position_embeddings = self.y_position_embeddings(bbox[:, :, 3])\n","        h_position_embeddings = self.h_position_embeddings(\n","            bbox[:, :, 3] - bbox[:, :, 1]\n","        )\n","        print(\"\\n bbox shape :- \",bbox.size())\n","        print(\"\\nbbox[:,:,2] :- \", bbox[:, :, 2])\n","        print(\"\\nbbox[:,:,2] shape :- \", bbox[:, :, 2].shape)\n","        print(\"\\nbbox[:, :, 0] :- \", bbox[:, :, 0])\n","        print(\"\\nbbox[:, :, 0] shape :- \", bbox[:, :, 0].shape)\n","        print(\"\\n sub of two above matrix :- \",bbox[:, :, 2] - bbox[:, :, 0])\n","        print(\"\\n printing the w_position_embeddings:- \",self.w_position_embeddings)\n","        w_position_embeddings = self.w_position_embeddings(\n","            bbox[:, :, 2] - bbox[:, :, 0]\n","        )\n","        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n","\n","        embeddings = (\n","            words_embeddings\n","            + position_embeddings\n","            + left_position_embeddings\n","            + upper_position_embeddings\n","            + right_position_embeddings\n","            + lower_position_embeddings\n","            + h_position_embeddings\n","            + w_position_embeddings\n","            + token_type_embeddings\n","        )\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","        return embeddings\n","\n","\n","class LayoutlmModel(BertModel):\n","\n","    config_class = LayoutlmConfig\n","    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, config):\n","        super(LayoutlmModel, self).__init__(config)\n","        self.embeddings = LayoutlmEmbeddings(config)\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","    ):\n","        if attention_mask is None:\n","            attention_mask = torch.ones_like(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        # We create a 3D attention mask from a 2D tensor mask.\n","        # Sizes are [batch_size, 1, 1, to_seq_length]\n","        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n","        # this attention mask is more simple than the triangular masking of causal attention\n","        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n","        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n","\n","        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n","        # masked positions, this operation will create a tensor which is 0.0 for\n","        # positions we want to attend and -10000.0 for masked positions.\n","        # Since we are adding it to the raw scores before the softmax, this is\n","        # effectively the same as removing these entirely.\n","        extended_attention_mask = extended_attention_mask.to(\n","            dtype=next(self.parameters()).dtype\n","        )  # fp16 compatibility\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","        # Prepare head mask if needed\n","        # 1.0 in head_mask indicate we keep the head\n","        # attention_probs has shape bsz x n_heads x N x N\n","        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n","        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n","        if head_mask is not None:\n","            if head_mask.dim() == 1:\n","                head_mask = (\n","                    head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n","                )\n","                head_mask = head_mask.expand(\n","                    self.config.num_hidden_layers, -1, -1, -1, -1\n","                )\n","            elif head_mask.dim() == 2:\n","                head_mask = (\n","                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n","                )  # We can specify head_mask for each layer\n","            head_mask = head_mask.to(\n","                dtype=next(self.parameters()).dtype\n","            )  # switch to fload if need + fp16 compatibility\n","        else:\n","            head_mask = [None] * self.config.num_hidden_layers\n","\n","        print(\"\\n The arguments in the embedding layer :- \\n\\n\\n\")\n","        print(\"\\n input_ids in embedding layer :- \\n\\n\")\n","        embedding_output = self.embeddings(\n","            input_ids, bbox, position_ids=position_ids, token_type_ids=token_type_ids\n","        )\n","        encoder_outputs = self.encoder(\n","            embedding_output, extended_attention_mask, head_mask=head_mask\n","        )\n","        sequence_output = encoder_outputs[0]\n","        pooled_output = self.pooler(sequence_output)\n","\n","        outputs = (sequence_output, pooled_output) + encoder_outputs[\n","            1:\n","        ]  # add hidden_states and attentions if they are here\n","        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n","\n","\n","class LayoutlmForTokenClassification(BertPreTrainedModel):\n","    config_class = LayoutlmConfig\n","    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        self.bert = LayoutlmModel(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","    ):\n","\n","        outputs = self.bert(\n","            input_ids=input_ids,\n","            bbox=bbox,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","\n","        outputs = (logits,) + outputs[\n","            2:\n","        ]  # add hidden states and attention if they are here\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.num_labels)[active_loss]\n","                active_labels = labels.view(-1)[active_loss]\n","                loss = loss_fct(active_logits, active_labels)\n","            else:\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), scores, (hidden_states), (attentions)\n","\n","\n","class LayoutlmForSequenceClassification(BertPreTrainedModel):\n","    config_class = LayoutlmConfig\n","    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, config):\n","        super(LayoutlmForSequenceClassification, self).__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.bert = LayoutlmModel(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n","\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","    ):\n","\n","        outputs = self.bert(\n","            input_ids=input_ids,\n","            bbox=bbox,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","        )\n","\n","        pooled_output = outputs[1]\n","\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","\n","        outputs = (logits,) + outputs[\n","            2:\n","        ]  # add hidden states and attention if they are here\n","\n","        if labels is not None:\n","            if self.num_labels == 1:\n","                #  We are doing regression\n","                loss_fct = MSELoss()\n","                loss = loss_fct(logits.view(-1), labels.view(-1))\n","            else:\n","                loss_fct = CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x61CAAmf-hay","colab_type":"text"},"source":["***funsd.py***"]},{"cell_type":"code","metadata":{"id":"cQsu_uQj-W1l","colab_type":"code","colab":{}},"source":["import logging\n","import os\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","class FunsdDataset(Dataset):\n","    def __init__(self, args, tokenizer, labels, pad_token_label_id, mode):\n","        if args.local_rank not in [-1, 0] and mode == \"train\":\n","            torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","\n","        # Load data features from cache or dataset file\n","        cached_features_file = os.path.join(\n","            args.data_dir,\n","            \"cached_{}_{}_{}\".format(\n","                mode,\n","                list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n","                str(args.max_seq_length),\n","            ),\n","        )\n","        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n","            logger.info(\"Loading features from cached file %s\", cached_features_file)\n","            features = torch.load(cached_features_file)\n","        else:\n","            logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n","            examples = read_examples_from_file(args.data_dir, mode)\n","            features = convert_examples_to_features(\n","                examples,\n","                labels,\n","                args.max_seq_length,\n","                tokenizer,\n","                cls_token_at_end=bool(args.model_type in [\"xlnet\"]),\n","                # xlnet has a cls token at the end\n","                cls_token=tokenizer.cls_token,\n","                cls_token_segment_id=2 if args.model_type in [\"xlnet\"] else 0,\n","                sep_token=tokenizer.sep_token,\n","                sep_token_extra=bool(args.model_type in [\"roberta\"]),\n","                # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n","                pad_on_left=bool(args.model_type in [\"xlnet\"]),\n","                # pad on the left for xlnet\n","                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n","                pad_token_segment_id=4 if args.model_type in [\"xlnet\"] else 0,\n","                pad_token_label_id=pad_token_label_id,\n","            )\n","            if args.local_rank in [-1, 0]:\n","                logger.info(\"Saving features into cached file %s\", cached_features_file)\n","                torch.save(features, cached_features_file)\n","\n","        if args.local_rank == 0 and mode == \"train\":\n","            torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","\n","        self.features = features\n","        # Convert to Tensors and build dataset\n","        self.all_input_ids = torch.tensor(\n","            [f.input_ids for f in features], dtype=torch.long\n","        )\n","        self.all_input_mask = torch.tensor(\n","            [f.input_mask for f in features], dtype=torch.long\n","        )\n","        self.all_segment_ids = torch.tensor(\n","            [f.segment_ids for f in features], dtype=torch.long\n","        )\n","        self.all_label_ids = torch.tensor(\n","            [f.label_ids for f in features], dtype=torch.long\n","        )\n","        self.all_bboxes = torch.tensor([f.boxes for f in features], dtype=torch.long)\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, index):\n","        return (\n","            self.all_input_ids[index],\n","            self.all_input_mask[index],\n","            self.all_segment_ids[index],\n","            self.all_label_ids[index],\n","            self.all_bboxes[index],\n","        )\n","\n","\n","class InputExample(object):\n","    \"\"\"A single training/test example for token classification.\"\"\"\n","\n","    def __init__(self, guid, words, labels, boxes, actual_bboxes, file_name, page_size):\n","        \"\"\"Constructs a InputExample.\n","\n","        Args:\n","            guid: Unique id for the example.\n","            words: list. The words of the sequence.\n","            labels: (Optional) list. The labels for each word of the sequence. This should be\n","            specified for train and dev examples, but not for test examples.\n","        \"\"\"\n","        self.guid = guid\n","        self.words = words\n","        self.labels = labels\n","        self.boxes = boxes\n","        self.actual_bboxes = actual_bboxes\n","        self.file_name = file_name\n","        self.page_size = page_size\n","\n","\n","class InputFeatures(object):\n","    \"\"\"A single set of features of data.\"\"\"\n","\n","    def __init__(\n","        self,\n","        input_ids,\n","        input_mask,\n","        segment_ids,\n","        label_ids,\n","        boxes,\n","        actual_bboxes,\n","        file_name,\n","        page_size,\n","    ):\n","        assert (\n","            0 <= all(boxes) <= 1000\n","        ), \"Error with input bbox ({}): the coordinate value is not between 0 and 1000\".format(\n","            boxes\n","        )\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.label_ids = label_ids\n","        self.boxes = boxes\n","        self.actual_bboxes = actual_bboxes\n","        self.file_name = file_name\n","        self.page_size = page_size\n","\n","\n","def read_examples_from_file(data_dir, mode):\n","    file_path = os.path.join(data_dir, \"{}.txt\".format(mode))\n","    box_file_path = os.path.join(data_dir, \"{}_box.txt\".format(mode))\n","    image_file_path = os.path.join(data_dir, \"{}_image.txt\".format(mode))\n","    guid_index = 1\n","    examples = []\n","    with open(file_path, encoding=\"utf-8\") as f, open(\n","        box_file_path, encoding=\"utf-8\"\n","    ) as fb, open(image_file_path, encoding=\"utf-8\") as fi:\n","        words = []\n","        boxes = []\n","        actual_bboxes = []\n","        file_name = None\n","        page_size = None\n","        labels = []\n","        for line, bline, iline in zip(f, fb, fi):\n","            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n","                if words:\n","                    examples.append(\n","                        InputExample(\n","                            guid=\"{}-{}\".format(mode, guid_index),\n","                            words=words,\n","                            labels=labels,\n","                            boxes=boxes,\n","                            actual_bboxes=actual_bboxes,\n","                            file_name=file_name,\n","                            page_size=page_size,\n","                        )\n","                    )\n","                    guid_index += 1\n","                    words = []\n","                    boxes = []\n","                    actual_bboxes = []\n","                    file_name = None\n","                    page_size = None\n","                    labels = []\n","            else:\n","                splits = line.split(\"\\t\")\n","                bsplits = bline.split(\"\\t\")\n","                isplits = iline.split(\"\\t\")\n","                assert len(splits) == 2\n","                assert len(bsplits) == 2\n","                assert len(isplits) == 4\n","                assert splits[0] == bsplits[0]\n","                words.append(splits[0])\n","                if len(splits) > 1:\n","                    labels.append(splits[-1].replace(\"\\n\", \"\"))\n","                    box = bsplits[-1].replace(\"\\n\", \"\")\n","                    box = [int(b) for b in box.split()]\n","                    boxes.append(box)\n","                    actual_bbox = [int(b) for b in isplits[1].split()]\n","                    actual_bboxes.append(actual_bbox)\n","                    page_size = [int(i) for i in isplits[2].split()]\n","                    file_name = isplits[3].strip()\n","                else:\n","                    # Examples could have no label for mode = \"test\"\n","                    labels.append(\"O\")\n","        if words:\n","            examples.append(\n","                InputExample(\n","                    guid=\"%s-%d\".format(mode, guid_index),\n","                    words=words,\n","                    labels=labels,\n","                    boxes=boxes,\n","                    actual_bboxes=actual_bboxes,\n","                    file_name=file_name,\n","                    page_size=page_size,\n","                )\n","            )\n","    return examples\n","\n","\n","def convert_examples_to_features(\n","    examples,\n","    label_list,\n","    max_seq_length,\n","    tokenizer,\n","    cls_token_at_end=False,\n","    cls_token=\"[CLS]\",\n","    cls_token_segment_id=1,\n","    sep_token=\"[SEP]\",\n","    sep_token_extra=False,\n","    pad_on_left=False,\n","    pad_token=0,\n","    cls_token_box=[0, 0, 0, 0],\n","    sep_token_box=[1000, 1000, 1000, 1000],\n","    pad_token_box=[0, 0, 0, 0],\n","    pad_token_segment_id=0,\n","    pad_token_label_id=-1,\n","    sequence_a_segment_id=0,\n","    mask_padding_with_zero=True,\n","):\n","    \"\"\" Loads a data file into a list of `InputBatch`s\n","        `cls_token_at_end` define the location of the CLS token:\n","            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n","            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n","        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n","    \"\"\"\n","\n","    label_map = {label: i for i, label in enumerate(label_list)}\n","\n","    features = []\n","    for (ex_index, example) in enumerate(examples):\n","        file_name = example.file_name\n","        page_size = example.page_size\n","        width, height = page_size\n","        if ex_index % 10000 == 0:\n","            logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n","\n","        tokens = []\n","        token_boxes = []\n","        actual_bboxes = []\n","        label_ids = []\n","        for word, label, box, actual_bbox in zip(\n","            example.words, example.labels, example.boxes, example.actual_bboxes\n","        ):\n","            word_tokens = tokenizer.tokenize(word)\n","            tokens.extend(word_tokens)\n","            token_boxes.extend([box] * len(word_tokens))\n","            actual_bboxes.extend([actual_bbox] * len(word_tokens))\n","            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n","            label_ids.extend(\n","                [label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1)\n","            )\n","\n","        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n","        special_tokens_count = 3 if sep_token_extra else 2\n","        if len(tokens) > max_seq_length - special_tokens_count:\n","            tokens = tokens[: (max_seq_length - special_tokens_count)]\n","            token_boxes = token_boxes[: (max_seq_length - special_tokens_count)]\n","            actual_bboxes = actual_bboxes[: (max_seq_length - special_tokens_count)]\n","            label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n","\n","        # The convention in BERT is:\n","        # (a) For sequence pairs:\n","        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n","        # (b) For single sequences:\n","        #  tokens:   [CLS] the dog is hairy . [SEP]\n","        #  type_ids:   0   0   0   0  0     0   0\n","        #\n","        # Where \"type_ids\" are used to indicate whether this is the first\n","        # sequence or the second sequence. The embedding vectors for `type=0` and\n","        # `type=1` were learned during pre-training and are added to the wordpiece\n","        # embedding vector (and position vector). This is not *strictly* necessary\n","        # since the [SEP] token unambiguously separates the sequences, but it makes\n","        # it easier for the model to learn the concept of sequences.\n","        #\n","        # For classification tasks, the first vector (corresponding to [CLS]) is\n","        # used as as the \"sentence vector\". Note that this only makes sense because\n","        # the entire model is fine-tuned.\n","        tokens += [sep_token]\n","        token_boxes += [sep_token_box]\n","        actual_bboxes += [[0, 0, width, height]]\n","        label_ids += [pad_token_label_id]\n","        if sep_token_extra:\n","            # roberta uses an extra separator b/w pairs of sentences\n","            tokens += [sep_token]\n","            token_boxes += [sep_token_box]\n","            actual_bboxes += [[0, 0, width, height]]\n","            label_ids += [pad_token_label_id]\n","        segment_ids = [sequence_a_segment_id] * len(tokens)\n","\n","        if cls_token_at_end:\n","            tokens += [cls_token]\n","            token_boxes += [cls_token_box]\n","            actual_bboxes += [[0, 0, width, height]]\n","            label_ids += [pad_token_label_id]\n","            segment_ids += [cls_token_segment_id]\n","        else:\n","            tokens = [cls_token] + tokens\n","            token_boxes = [cls_token_box] + token_boxes\n","            actual_bboxes = [[0, 0, width, height]] + actual_bboxes\n","            label_ids = [pad_token_label_id] + label_ids\n","            segment_ids = [cls_token_segment_id] + segment_ids\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","        # tokens are attended to.\n","        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n","\n","        # Zero-pad up to the sequence length.\n","        padding_length = max_seq_length - len(input_ids)\n","        if pad_on_left:\n","            input_ids = ([pad_token] * padding_length) + input_ids\n","            input_mask = (\n","                [0 if mask_padding_with_zero else 1] * padding_length\n","            ) + input_mask\n","            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n","            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n","            token_boxes = ([pad_token_box] * padding_length) + token_boxes\n","        else:\n","            input_ids += [pad_token] * padding_length\n","            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n","            segment_ids += [pad_token_segment_id] * padding_length\n","            label_ids += [pad_token_label_id] * padding_length\n","            token_boxes += [pad_token_box] * padding_length\n","\n","        assert len(input_ids) == max_seq_length\n","        assert len(input_mask) == max_seq_length\n","        assert len(segment_ids) == max_seq_length\n","        assert len(label_ids) == max_seq_length\n","        assert len(token_boxes) == max_seq_length\n","\n","        if ex_index < 5:\n","            logger.info(\"*** Example ***\")\n","            logger.info(\"guid: %s\", example.guid)\n","            logger.info(\"tokens: %s\", \" \".join([str(x) for x in tokens]))\n","            logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n","            logger.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n","            logger.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n","            logger.info(\"label_ids: %s\", \" \".join([str(x) for x in label_ids]))\n","            logger.info(\"boxes: %s\", \" \".join([str(x) for x in token_boxes]))\n","            logger.info(\"actual_bboxes: %s\", \" \".join([str(x) for x in actual_bboxes]))\n","\n","        features.append(\n","            InputFeatures(\n","                input_ids=input_ids,\n","                input_mask=input_mask,\n","                segment_ids=segment_ids,\n","                label_ids=label_ids,\n","                boxes=token_boxes,\n","                actual_bboxes=actual_bboxes,\n","                file_name=file_name,\n","                page_size=page_size,\n","            )\n","        )\n","    return features\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zVJQt2N0q46e","colab_type":"text"},"source":["****run_sequence_labeling.py****"]},{"cell_type":"code","metadata":{"id":"T5CKUcz8q4ao","colab_type":"code","colab":{}},"source":["\"\"\" Fine-tuning the library models for named entity recognition on CoNLL-2003 (Bert or Roberta). \"\"\"\n","\n","from __future__ import absolute_import, division, print_function\n","\n","import argparse\n","import glob\n","import logging\n","import os\n","import random\n","import shutil\n","\n","import numpy as np\n","import torch\n","from seqeval.metrics import (\n","    classification_report,\n","    f1_score,\n","    precision_score,\n","    recall_score,\n",")\n","from tensorboardX import SummaryWriter\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm import tqdm, trange\n","from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    BertConfig,\n","    BertForTokenClassification,\n","    BertTokenizer,\n","    RobertaConfig,\n","    RobertaForTokenClassification,\n","    RobertaTokenizer,\n","    get_linear_schedule_with_warmup,\n",")\n","'''\n","Other file imports {take only important files for deployment}\n","'''\n","#from layoutlm import FunsdDataset, LayoutlmConfig, LayoutlmForTokenClassification\n","\n","logger = logging.getLogger(__name__)\n","\n","ALL_MODELS = sum(\n","    (\n","        tuple(conf.pretrained_config_archive_map.keys())\n","        for conf in (BertConfig, RobertaConfig, LayoutlmConfig)\n","    ),\n","    (),\n",")\n","\n","MODEL_CLASSES = {\n","    \"bert\": (BertConfig, BertForTokenClassification, BertTokenizer),\n","    \"roberta\": (RobertaConfig, RobertaForTokenClassification, RobertaTokenizer),\n","    \"layoutlm\": (LayoutlmConfig, LayoutlmForTokenClassification, BertTokenizer),\n","}\n","\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if args.n_gpu > 0:\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","\n","def collate_fn(data):\n","    batch = [i for i in zip(*data)]\n","    for i in range(len(batch)):\n","        if i < len(batch) - 2:\n","            batch[i] = torch.stack(batch[i], 0)\n","    return tuple(batch)\n","\n","\n","def get_labels(path):\n","    with open(path, \"r\") as f:\n","        labels = f.read().splitlines()\n","    if \"O\" not in labels:\n","        labels = [\"O\"] + labels\n","    return labels\n","\n","\n","def train(  # noqa C901\n","    args, train_dataset, model, tokenizer, labels, pad_token_label_id\n","):\n","    \"\"\" Train the model \"\"\"\n","    if args.local_rank in [-1, 0]:\n","        tb_writer = SummaryWriter(logdir=\"runs/\" + os.path.basename(args.output_dir))\n","\n","    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n","    train_sampler = (\n","        RandomSampler(train_dataset)\n","        if args.local_rank == -1\n","        else DistributedSampler(train_dataset)\n","    )\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        sampler=train_sampler,\n","        batch_size=args.train_batch_size,\n","        collate_fn=None,\n","    )\n","\n","    if args.max_steps > 0:\n","        t_total = args.max_steps\n","        args.num_train_epochs = (\n","            args.max_steps\n","            // (len(train_dataloader) // args.gradient_accumulation_steps)\n","            + 1\n","        )\n","    else:\n","        t_total = (\n","            len(train_dataloader)\n","            // args.gradient_accumulation_steps\n","            * args.num_train_epochs\n","        )\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [\n","                p\n","                for n, p in model.named_parameters()\n","                if not any(nd in n for nd in no_decay)\n","            ],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\n","            \"params\": [\n","                p\n","                for n, p in model.named_parameters()\n","                if any(nd in n for nd in no_decay)\n","            ],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon\n","    )\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n","    )\n","    if args.fp16:\n","        try:\n","            from apex import amp\n","        except ImportError:\n","            raise ImportError(\n","                \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"\n","            )\n","        model, optimizer = amp.initialize(\n","            model, optimizer, opt_level=args.fp16_opt_level\n","        )\n","\n","    # multi-gpu training (should be after apex fp16 initialization)\n","    if args.n_gpu > 1:\n","        model = torch.nn.DataParallel(model)\n","\n","    # Distributed training (should be after apex fp16 initialization)\n","    if args.local_rank != -1:\n","        model = torch.nn.parallel.DistributedDataParallel(\n","            model,\n","            device_ids=[args.local_rank],\n","            output_device=args.local_rank,\n","            find_unused_parameters=True,\n","        )\n","\n","    # Train!\n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num examples = %d\", len(train_dataset))\n","    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n","    logger.info(\n","        \"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size\n","    )\n","    logger.info(\n","        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n","        args.train_batch_size\n","        * args.gradient_accumulation_steps\n","        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n","    )\n","    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n","    logger.info(\"  Total optimization steps = %d\", t_total)\n","\n","    global_step = 0\n","    tr_loss, logging_loss = 0.0, 0.0\n","    model.zero_grad()\n","    train_iterator = trange(\n","        int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n","    )\n","    set_seed(args)  # Added here for reproductibility (even between python 2 and 3)\n","    for _ in train_iterator:\n","        epoch_iterator = tqdm(\n","            train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0]\n","        )\n","        for step, batch in enumerate(epoch_iterator):\n","            model.train()\n","            inputs = {\n","                \"input_ids\": batch[0].to(args.device),\n","                \"attention_mask\": batch[1].to(args.device),\n","                \"labels\": batch[3].to(args.device),\n","            }\n","            if args.model_type in [\"layoutlm\"]:\n","                inputs[\"bbox\"] = batch[4].to(args.device)\n","            inputs[\"token_type_ids\"] = (\n","                batch[2].to(args.device) if args.model_type in [\"bert\", \"layoutlm\"] else None\n","            )  # RoBERTa don\"t use segment_ids\n","            outputs = model(**inputs)\n","            # model outputs are always tuple in pytorch-transformers (see doc)\n","            loss = outputs[0]\n","\n","            if args.n_gpu > 1:\n","                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            tr_loss += loss.item()\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                if args.fp16:\n","                    torch.nn.utils.clip_grad_norm_(\n","                        amp.master_params(optimizer), args.max_grad_norm\n","                    )\n","                else:\n","                    torch.nn.utils.clip_grad_norm_(\n","                        model.parameters(), args.max_grad_norm\n","                    )\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","\n","                if (\n","                    args.local_rank in [-1, 0]\n","                    and args.logging_steps > 0\n","                    and global_step % args.logging_steps == 0\n","                ):\n","                    # Log metrics\n","                    if (\n","                        args.local_rank in [-1, 0] and args.evaluate_during_training\n","                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n","                        results, _ = evaluate(\n","                            args,\n","                            model,\n","                            tokenizer,\n","                            labels,\n","                            pad_token_label_id,\n","                            mode=\"dev\",\n","                        )\n","                        for key, value in results.items():\n","                            tb_writer.add_scalar(\n","                                \"eval_{}\".format(key), value, global_step\n","                            )\n","                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n","                    tb_writer.add_scalar(\n","                        \"loss\",\n","                        (tr_loss - logging_loss) / args.logging_steps,\n","                        global_step,\n","                    )\n","                    logging_loss = tr_loss\n","\n","                if (\n","                    args.local_rank in [-1, 0]\n","                    and args.save_steps > 0\n","                    and global_step % args.save_steps == 0\n","                ):\n","                    # Save model checkpoint\n","                    output_dir = os.path.join(\n","                        args.output_dir, \"checkpoint-{}\".format(global_step)\n","                    )\n","                    if not os.path.exists(output_dir):\n","                        os.makedirs(output_dir)\n","                    model_to_save = (\n","                        model.module if hasattr(model, \"module\") else model\n","                    )  # Take care of distributed/parallel training\n","                    model_to_save.save_pretrained(output_dir)\n","                    tokenizer.save_pretrained(output_dir)\n","                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n","                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n","\n","            if args.max_steps > 0 and global_step > args.max_steps:\n","                epoch_iterator.close()\n","                break\n","        if args.max_steps > 0 and global_step > args.max_steps:\n","            train_iterator.close()\n","            break\n","\n","    if args.local_rank in [-1, 0]:\n","        tb_writer.close()\n","\n","    return global_step, tr_loss / global_step\n","\n","\n","def evaluate(args, model, tokenizer, labels, pad_token_label_id, mode, prefix=\"\"):\n","    eval_dataset = FunsdDataset(args, tokenizer, labels, pad_token_label_id, mode=mode)\n","    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n","    eval_sampler = SequentialSampler(eval_dataset)\n","    eval_dataloader = DataLoader(\n","        eval_dataset,\n","        sampler=eval_sampler,\n","        batch_size=args.eval_batch_size,\n","        collate_fn=None,\n","    )\n","\n","    # Eval!\n","    logger.info(\"***** Running evaluation %s *****\", prefix)\n","    logger.info(\"  Num examples = %d\", len(eval_dataset))\n","    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n","    eval_loss = 0.0\n","    nb_eval_steps = 0\n","    preds = None\n","    out_label_ids = None\n","    model.eval()\n","    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        with torch.no_grad():\n","            inputs = {\n","                \"input_ids\": batch[0].to(args.device),\n","                \"attention_mask\": batch[1].to(args.device),\n","                \"labels\": batch[3].to(args.device),\n","            }\n","            if args.model_type in [\"layoutlm\"]:\n","                inputs[\"bbox\"] = batch[4].to(args.device)\n","            inputs[\"token_type_ids\"] = (\n","                batch[2].to(args.device)\n","                if args.model_type in [\"bert\", \"layoutlm\"]\n","                else None\n","            )  # RoBERTa don\"t use segment_ids\n","            print(\"\\n\\n lenght :- \",len(inputs))\n","            print(\"\\n\\n keys :- \",inputs.keys())\n","            print(\"\\n\\n\\n\\n\",)\n","            outputs = model(**inputs)\n","            tmp_eval_loss, logits = outputs[:2]\n","\n","            if args.n_gpu > 1:\n","                tmp_eval_loss = (\n","                    tmp_eval_loss.mean()\n","                )  # mean() to average on multi-gpu parallel evaluating\n","\n","            eval_loss += tmp_eval_loss.item()\n","        nb_eval_steps += 1\n","        if preds is None:\n","            preds = logits.detach().cpu().numpy()\n","            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n","        else:\n","            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n","            out_label_ids = np.append(\n","                out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n","            )\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    preds = np.argmax(preds, axis=2)\n","\n","    label_map = {i: label for i, label in enumerate(labels)}\n","\n","    out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n","    preds_list = [[] for _ in range(out_label_ids.shape[0])]\n","\n","    for i in range(out_label_ids.shape[0]):\n","        for j in range(out_label_ids.shape[1]):\n","            if out_label_ids[i, j] != pad_token_label_id:\n","                out_label_list[i].append(label_map[out_label_ids[i][j]])\n","                preds_list[i].append(label_map[preds[i][j]])\n","\n","    results = {\n","        \"loss\": eval_loss,\n","        \"precision\": precision_score(out_label_list, preds_list),\n","        \"recall\": recall_score(out_label_list, preds_list),\n","        \"f1\": f1_score(out_label_list, preds_list),\n","    }\n","\n","    report = classification_report(out_label_list, preds_list)\n","    logger.info(\"\\n\" + report)\n","\n","    logger.info(\"***** Eval results %s *****\", prefix)\n","    for key in sorted(results.keys()):\n","        logger.info(\"  %s = %s\", key, str(results[key]))\n","\n","    return results, preds_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AmJKaB49d-rC","colab_type":"code","colab":{}},"source":["class sequence_labelling_data_input:\n","  def __init__(self,data_dir = None,model_type = None,model_name_or_path = None,output_dir = None,labels = \"\",config_name = \"\",tokenizer_name=\"\",cache_dir = \"\",max_seq_length = 512,do_train = False,do_eval = False,do_predict = False,evaluate_during_training = False,do_lower_case = False,per_gpu_train_batch_size = 8,per_gpu_eval_batch_size=8,gradient_accumulation_steps = 1,learning_rate = 5e-5,weight_decay = 0.0,adam_epsilon = 1e-8,max_grad_norm = 1.0,num_train_epochs = 3.0,max_steps = -1,warmup_steps = 0,logging_steps = 50,eval_all_checkpoints = False,no_cuda = False,overwrite_output_dir = False,overwrite_cache = False,seed = 42,fp16 = False,fp16_opt_level = \"01\",local_rank = -1,server_ip = \"\",server_port = \"\",save_steps = -1):\n","    #The input data dir. Should contain the training files for the CoNLL-2003 NER task.\n","    self.data_dir = data_dir\n","    #Model type selected in the list: \n","    self.model_type = model_type\n","    #Path to pre-trained model or shortcut name selected in the list: \n","    self.model_name_or_path = model_name_or_path \n","    #The output directory where the model predictions and checkpoints will be written.\n","    self.output_dir = output_dir\n","    #Path to a file containing all labels. If not specified, CoNLL-2003 labels are used\n","    self.labels = labels\n","    #Pretrained config name or path if not the same as model_name\n","    self.config_name = config_name\n","    #Pretrained tokenizer name or path if not the same as model_name\n","    self.tokenizer_name = tokenizer_name\n","    #Where do you want to store the pre-trained models downloaded from s3\n","    self.cache_dir = cache_dir\n","    #The maximum total input sequence length after tokenization. Sequences longer \"than this will be truncated, sequences shorter will be padded.\"\n","    self.max_seq_length = max_seq_length\n","    #Whether to run training.\n","    self.do_train = do_train\n","    #Whether to run eval on the dev set.\n","    self.do_eval = do_eval\n","    #Whether to run predictions on the test set.\n","    self.do_predict = do_predict\n","    #Whether to run evaluation during training at each logging step.\n","    self.evaluate_during_training = evaluate_during_training\n","    #Set this flag if you are using an uncased model.\n","    self.do_lower_case = do_lower_case\n","    #Batch size per GPU/CPU for training.\n","    self.per_gpu_train_batch_size = per_gpu_train_batch_size\n","    #Batch size per GPU/CPU for evaluation.\n","    self.per_gpu_eval_batch_size = per_gpu_eval_batch_size\n","    #Number of updates steps to accumulate before performing a backward/update pass.\n","    self.gradient_accumulation_steps = gradient_accumulation_steps\n","    #The initial learning rate for Adam.\n","    self.learning_rate = learning_rate\n","    #Weight decay if we apply some.\n","    self.weight_decay = weight_decay\n","    #Epsilon for Adam optimizer.\n","    self.adam_epsilon = adam_epsilon\n","    #Max gradient norm.\n","    self.max_grad_norm = max_grad_norm\n","    #Total number of training epochs to perform.\n","    self.num_train_epochs = num_train_epochs\n","    #If > 0: set total number of training steps to perform. Override num_train_epochs.\n","    self.max_steps = max_steps\n","    #Linear warmup over warmup_steps.\n","    self.warmup_steps = warmup_steps\n","    #Log every X updates steps.\n","    self.logging_steps = logging_steps\n","    #Save checkpoint every X updates steps.\n","    self.save_steps = save_steps\n","    #Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\n","    self.eval_all_checkpoints = eval_all_checkpoints\n","    #Avoid using CUDA when available\n","    self.no_cuda = False\n","    #Overwrite the content of the output directory\n","    self.overwrite_output_dir =False\n","    #Overwrite the cached training and evaluation sets\n","    self.overwrite_cache = overwrite_cache\n","    #random seed for initialization\n","    self.seed = seed\n","    #Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\n","    self.fp16 = fp16\n","    #For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\"See details at https://nvidia.github.io/apex/amp.html\"\n","    self.fp16_opt_level = fp16_opt_level\n","    #For distributed training: local_rank\n","    self.local_rank = local_rank\n","    #For distant debugging.\n","    self.server_ip = server_ip\n","    #For distant debugging.\n","    self.server_port = server_port\n","\n","#args = parser.parse_args()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I99OAm9i2llC","colab_type":"text"},"source":["**creating the object for training**"]},{"cell_type":"code","metadata":{"id":"bCftw30m3LGD","colab_type":"code","colab":{}},"source":["args=sequence_labelling_data_input(data_dir='data',model_type = \"layoutlm\",model_name_or_path = \"bert-base-uncased\",do_lower_case = True,max_seq_length = 512,do_train = True,num_train_epochs = 2,overwrite_output_dir = True,logging_steps = 1,save_steps = -1,output_dir = 'output_dir2',labels = 'data/labels.txt' ,per_gpu_train_batch_size = 1,per_gpu_eval_batch_size = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"em8XAsSBwT8V","colab_type":"code","colab":{}},"source":["def main(args):  # noqa C901\n","    if (\n","        os.path.exists(args.output_dir)\n","        and os.listdir(args.output_dir)\n","        and args.do_train\n","    ):\n","        if not args.overwrite_output_dir:\n","            raise ValueError(\n","                \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n","                    args.output_dir\n","                )\n","            )\n","        else:\n","            if args.local_rank in [-1, 0]:\n","                shutil.rmtree(args.output_dir)\n","\n","    if not os.path.exists(args.output_dir) and (args.do_eval or args.do_predict):\n","        raise ValueError(\n","            \"Output directory ({}) does not exist. Please train and save the model before inference stage.\".format(\n","                args.output_dir\n","            )\n","        )\n","\n","    if (\n","        not os.path.exists(args.output_dir)\n","        and args.do_train\n","        and args.local_rank in [-1, 0]\n","    ):\n","        os.makedirs(args.output_dir)\n","\n","    # Setup distant debugging if needed\n","    if args.server_ip and args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","        print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(\n","            address=(args.server_ip, args.server_port), redirect_output=True\n","        )\n","        ptvsd.wait_for_attach()\n","\n","    # Setup CUDA, GPU & distributed training\n","    if args.local_rank == -1 or args.no_cuda:\n","        device = torch.device(\n","            \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n","        )\n","        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n","    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","        torch.cuda.set_device(args.local_rank)\n","        device = torch.device(\"cuda\", args.local_rank)\n","        torch.distributed.init_process_group(backend=\"nccl\")\n","        args.n_gpu = 1\n","    args.device = device\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        filename=os.path.join(args.output_dir, \"train.log\")\n","        if args.local_rank in [-1, 0]\n","        else None,\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.warning(\n","        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","        args.local_rank,\n","        device,\n","        args.n_gpu,\n","        bool(args.local_rank != -1),\n","        args.fp16,\n","    )\n","\n","    # Set seed\n","    set_seed(args)\n","\n","    labels = get_labels(args.labels)\n","    num_labels = len(labels)\n","    # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n","    pad_token_label_id = CrossEntropyLoss().ignore_index\n","\n","    # Load pretrained model and tokenizer\n","    if args.local_rank not in [-1, 0]:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n","\n","    args.model_type = args.model_type.lower()\n","    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","    config = config_class.from_pretrained(\n","        args.config_name if args.config_name else args.model_name_or_path,\n","        num_labels=num_labels,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    tokenizer = tokenizer_class.from_pretrained(\n","        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n","        do_lower_case=args.do_lower_case,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    model = model_class.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","\n","    if args.local_rank == 0:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n","\n","    model.to(args.device)\n","\n","    logger.info(\"Training/evaluation parameters %s\", args)\n","\n","    \n","    \n","    # Training\n","    if args.do_train:\n","        train_dataset = FunsdDataset(\n","            args, tokenizer, labels, pad_token_label_id, mode=\"train\"\n","        )\n","        global_step, tr_loss = train(\n","            args, train_dataset, model, tokenizer, labels, pad_token_label_id\n","        )\n","        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n","\n","    \n","    \n","    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n","        # Create output directory if needed\n","        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n","            os.makedirs(args.output_dir)\n","\n","        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n","        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","        # They can then be reloaded using `from_pretrained()`\n","        model_to_save = (\n","            model.module if hasattr(model, \"module\") else model\n","        )  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(args.output_dir)\n","        tokenizer.save_pretrained(args.output_dir)\n","\n","        # Good practice: save your training arguments together with the trained model\n","        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n","\n","    \n","    \n","    \n","    # Evaluation\n","    results = {}\n","    if args.do_eval and args.local_rank in [-1, 0]:\n","        tokenizer = tokenizer_class.from_pretrained(\n","            args.output_dir, do_lower_case=args.do_lower_case\n","        )\n","        checkpoints = [args.output_dir]\n","        if args.eval_all_checkpoints:\n","            checkpoints = list(\n","                os.path.dirname(c)\n","                for c in sorted(\n","                    glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True)\n","                )\n","            )\n","            logging.getLogger(\"pytorch_transformers.modeling_utils\").setLevel(\n","                logging.WARN\n","            )  # Reduce logging\n","        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","        for checkpoint in checkpoints:\n","            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n","            model = model_class.from_pretrained(checkpoint)\n","            model.to(args.device)\n","            result, _ = evaluate(\n","                args,\n","                model,\n","                tokenizer,\n","                labels,\n","                pad_token_label_id,\n","                mode=\"test\",\n","                prefix=global_step,\n","            )\n","            if global_step:\n","                result = {\"{}_{}\".format(global_step, k): v for k, v in result.items()}\n","            results.update(result)\n","        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n","        with open(output_eval_file, \"w\") as writer:\n","            for key in sorted(results.keys()):\n","                writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n","\n","    \n","    \n","    # do predict part\n","    if args.do_predict and args.local_rank in [-1, 0]:\n","        tokenizer = tokenizer_class.from_pretrained(\n","            args.model_name_or_path, do_lower_case=args.do_lower_case\n","        )\n","        model = model_class.from_pretrained(args.output_dir)\n","        model.to(args.device)\n","        result, predictions = evaluate(\n","            args, model, tokenizer, labels, pad_token_label_id, mode=\"test\"\n","        )\n","        # Save results\n","        output_test_results_file = os.path.join(args.output_dir, \"test_results.txt\")\n","        with open(output_test_results_file, \"w\") as writer:\n","            for key in sorted(result.keys()):\n","                writer.write(\"{} = {}\\n\".format(key, str(result[key])))\n","        # Save predictions\n","        output_test_predictions_file = os.path.join(\n","            args.output_dir, \"test_predictions.txt\"\n","        )\n","        with open(output_test_predictions_file, \"w\", encoding=\"utf8\") as writer:\n","            with open(\n","                os.path.join(args.data_dir, \"test.txt\"), \"r\", encoding=\"utf8\"\n","            ) as f:\n","                example_id = 0\n","                for line in f:\n","                    if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n","                        writer.write(line)\n","                        if not predictions[example_id]:\n","                            example_id += 1\n","                    elif predictions[example_id]:\n","                        output_line = (\n","                            line.split()[0]\n","                            + \" \"\n","                            + predictions[example_id].pop(0)\n","                            + \"\\n\"\n","                        )\n","                        writer.write(output_line)\n","                    else:\n","                        logger.warning(\n","                            \"Maximum sequence length exceeded: No prediction for '%s'.\",\n","                            line.split()[0],\n","                        )\n","\n","    return results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKnlp0Emwdyj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596799768893,"user_tz":-330,"elapsed":2019431,"user":{"displayName":"Rohan Kakarlapudi","photoUrl":"","userId":"09959240400628694972"}},"outputId":"e8fc7e72-87df-4156-ef39-abaeaafdd7d5"},"source":["if __name__ == \"__main__\":\n","    main(args)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n","Iteration:   0%|          | 0/150 [00:00<?, ?it/s]\u001b[A/usr/local/lib/python3.6/dist-packages/transformers/optimization.py:155: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n","  exp_avg.mul_(beta1).add_(1.0 - beta1, grad)\n","/usr/local/lib/python3.6/dist-packages/torch/optim/lr_scheduler.py:231: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n","  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n","\n","Iteration:   1%|          | 1/150 [00:07<17:27,  7.03s/it]\u001b[A\n","Iteration:   1%|▏         | 2/150 [00:14<17:21,  7.04s/it]\u001b[A\n","Iteration:   2%|▏         | 3/150 [00:20<16:57,  6.92s/it]\u001b[A\n","Iteration:   3%|▎         | 4/150 [00:27<16:37,  6.83s/it]\u001b[A\n","Iteration:   3%|▎         | 5/150 [00:34<16:24,  6.79s/it]\u001b[A\n","Iteration:   4%|▍         | 6/150 [00:40<16:12,  6.75s/it]\u001b[A\n","Iteration:   5%|▍         | 7/150 [00:47<16:04,  6.75s/it]\u001b[A\n","Iteration:   5%|▌         | 8/150 [00:54<15:55,  6.73s/it]\u001b[A\n","Iteration:   6%|▌         | 9/150 [01:00<15:45,  6.71s/it]\u001b[A\n","Iteration:   7%|▋         | 10/150 [01:07<15:36,  6.69s/it]\u001b[A\n","Iteration:   7%|▋         | 11/150 [01:14<15:28,  6.68s/it]\u001b[A\n","Iteration:   8%|▊         | 12/150 [01:20<15:20,  6.67s/it]\u001b[A\n","Iteration:   9%|▊         | 13/150 [01:27<15:13,  6.67s/it]\u001b[A\n","Iteration:   9%|▉         | 14/150 [01:34<15:07,  6.67s/it]\u001b[A\n","Iteration:  10%|█         | 15/150 [01:40<15:01,  6.67s/it]\u001b[A\n","Iteration:  11%|█         | 16/150 [01:47<14:58,  6.71s/it]\u001b[A\n","Iteration:  11%|█▏        | 17/150 [01:54<14:51,  6.70s/it]\u001b[A\n","Iteration:  12%|█▏        | 18/150 [02:00<14:42,  6.68s/it]\u001b[A\n","Iteration:  13%|█▎        | 19/150 [02:07<14:35,  6.68s/it]\u001b[A\n","Iteration:  13%|█▎        | 20/150 [02:14<14:26,  6.67s/it]\u001b[A\n","Iteration:  14%|█▍        | 21/150 [02:20<14:18,  6.66s/it]\u001b[A\n","Iteration:  15%|█▍        | 22/150 [02:27<14:12,  6.66s/it]\u001b[A\n","Iteration:  15%|█▌        | 23/150 [02:34<14:05,  6.66s/it]\u001b[A\n","Iteration:  16%|█▌        | 24/150 [02:40<13:57,  6.65s/it]\u001b[A\n","Iteration:  17%|█▋        | 25/150 [02:47<13:52,  6.66s/it]\u001b[A\n","Iteration:  17%|█▋        | 26/150 [02:54<13:55,  6.74s/it]\u001b[A\n","Iteration:  18%|█▊        | 27/150 [03:02<14:45,  7.20s/it]\u001b[A\n","Iteration:  19%|█▊        | 28/150 [03:09<14:18,  7.04s/it]\u001b[A\n","Iteration:  19%|█▉        | 29/150 [03:15<13:57,  6.92s/it]\u001b[A\n","Iteration:  20%|██        | 30/150 [03:22<13:41,  6.85s/it]\u001b[A\n","Iteration:  21%|██        | 31/150 [03:29<13:27,  6.78s/it]\u001b[A\n","Iteration:  21%|██▏       | 32/150 [03:36<13:18,  6.77s/it]\u001b[A\n","Iteration:  22%|██▏       | 33/150 [03:42<13:07,  6.73s/it]\u001b[A\n","Iteration:  23%|██▎       | 34/150 [03:49<13:03,  6.75s/it]\u001b[A\n","Iteration:  23%|██▎       | 35/150 [03:56<12:53,  6.73s/it]\u001b[A\n","Iteration:  24%|██▍       | 36/150 [04:02<12:44,  6.70s/it]\u001b[A\n","Iteration:  25%|██▍       | 37/150 [04:09<12:37,  6.70s/it]\u001b[A\n","Iteration:  25%|██▌       | 38/150 [04:16<12:27,  6.67s/it]\u001b[A\n","Iteration:  26%|██▌       | 39/150 [04:22<12:20,  6.67s/it]\u001b[A\n","Iteration:  27%|██▋       | 40/150 [04:29<12:13,  6.66s/it]\u001b[A\n","Iteration:  27%|██▋       | 41/150 [04:36<12:06,  6.66s/it]\u001b[A\n","Iteration:  28%|██▊       | 42/150 [04:42<11:58,  6.65s/it]\u001b[A\n","Iteration:  29%|██▊       | 43/150 [04:49<11:50,  6.64s/it]\u001b[A\n","Iteration:  29%|██▉       | 44/150 [04:55<11:44,  6.65s/it]\u001b[A\n","Iteration:  30%|███       | 45/150 [05:02<11:38,  6.65s/it]\u001b[A\n","Iteration:  31%|███       | 46/150 [05:09<11:29,  6.63s/it]\u001b[A\n","Iteration:  31%|███▏      | 47/150 [05:15<11:22,  6.63s/it]\u001b[A\n","Iteration:  32%|███▏      | 48/150 [05:22<11:15,  6.62s/it]\u001b[A\n","Iteration:  33%|███▎      | 49/150 [05:29<11:09,  6.63s/it]\u001b[A\n","Iteration:  33%|███▎      | 50/150 [05:35<11:04,  6.64s/it]\u001b[A\n","Iteration:  34%|███▍      | 51/150 [05:42<10:57,  6.64s/it]\u001b[A\n","Iteration:  35%|███▍      | 52/150 [05:49<10:51,  6.64s/it]\u001b[A\n","Iteration:  35%|███▌      | 53/150 [05:55<10:45,  6.65s/it]\u001b[A\n","Iteration:  36%|███▌      | 54/150 [06:02<10:38,  6.65s/it]\u001b[A\n","Iteration:  37%|███▋      | 55/150 [06:08<10:30,  6.64s/it]\u001b[A\n","Iteration:  37%|███▋      | 56/150 [06:15<10:23,  6.63s/it]\u001b[A\n","Iteration:  38%|███▊      | 57/150 [06:22<10:16,  6.63s/it]\u001b[A\n","Iteration:  39%|███▊      | 58/150 [06:28<10:09,  6.62s/it]\u001b[A\n","Iteration:  39%|███▉      | 59/150 [06:35<10:03,  6.63s/it]\u001b[A\n","Iteration:  40%|████      | 60/150 [06:42<09:57,  6.64s/it]\u001b[A\n","Iteration:  41%|████      | 61/150 [06:48<09:50,  6.64s/it]\u001b[A\n","Iteration:  41%|████▏     | 62/150 [06:55<09:45,  6.65s/it]\u001b[A\n","Iteration:  42%|████▏     | 63/150 [07:02<09:40,  6.67s/it]\u001b[A\n","Iteration:  43%|████▎     | 64/150 [07:08<09:34,  6.68s/it]\u001b[A\n","Iteration:  43%|████▎     | 65/150 [07:15<09:30,  6.71s/it]\u001b[A\n","Iteration:  44%|████▍     | 66/150 [07:22<09:25,  6.73s/it]\u001b[A\n","Iteration:  45%|████▍     | 67/150 [07:29<09:18,  6.73s/it]\u001b[A\n","Iteration:  45%|████▌     | 68/150 [07:35<09:14,  6.76s/it]\u001b[A\n","Iteration:  46%|████▌     | 69/150 [07:42<09:04,  6.73s/it]\u001b[A\n","Iteration:  47%|████▋     | 70/150 [07:49<08:56,  6.71s/it]\u001b[A\n","Iteration:  47%|████▋     | 71/150 [07:55<08:49,  6.70s/it]\u001b[A\n","Iteration:  48%|████▊     | 72/150 [08:02<08:42,  6.70s/it]\u001b[A\n","Iteration:  49%|████▊     | 73/150 [08:09<08:35,  6.70s/it]\u001b[A\n","Iteration:  49%|████▉     | 74/150 [08:16<08:28,  6.69s/it]\u001b[A\n","Iteration:  50%|█████     | 75/150 [08:22<08:21,  6.69s/it]\u001b[A\n","Iteration:  51%|█████     | 76/150 [08:29<08:14,  6.69s/it]\u001b[A\n","Iteration:  51%|█████▏    | 77/150 [08:36<08:08,  6.69s/it]\u001b[A\n","Iteration:  52%|█████▏    | 78/150 [08:42<08:01,  6.69s/it]\u001b[A\n","Iteration:  53%|█████▎    | 79/150 [08:49<07:54,  6.68s/it]\u001b[A\n","Iteration:  53%|█████▎    | 80/150 [08:56<07:47,  6.68s/it]\u001b[A\n","Iteration:  54%|█████▍    | 81/150 [09:02<07:42,  6.70s/it]\u001b[A\n","Iteration:  55%|█████▍    | 82/150 [09:09<07:35,  6.70s/it]\u001b[A\n","Iteration:  55%|█████▌    | 83/150 [09:16<07:28,  6.69s/it]\u001b[A\n","Iteration:  56%|█████▌    | 84/150 [09:22<07:22,  6.70s/it]\u001b[A\n","Iteration:  57%|█████▋    | 85/150 [09:29<07:15,  6.70s/it]\u001b[A\n","Iteration:  57%|█████▋    | 86/150 [09:36<07:09,  6.71s/it]\u001b[A\n","Iteration:  58%|█████▊    | 87/150 [09:43<07:02,  6.70s/it]\u001b[A\n","Iteration:  59%|█████▊    | 88/150 [09:49<06:55,  6.70s/it]\u001b[A\n","Iteration:  59%|█████▉    | 89/150 [09:56<06:48,  6.70s/it]\u001b[A\n","Iteration:  60%|██████    | 90/150 [10:03<06:41,  6.70s/it]\u001b[A\n","Iteration:  61%|██████    | 91/150 [10:09<06:34,  6.69s/it]\u001b[A\n","Iteration:  61%|██████▏   | 92/150 [10:16<06:28,  6.69s/it]\u001b[A\n","Iteration:  62%|██████▏   | 93/150 [10:23<06:21,  6.69s/it]\u001b[A\n","Iteration:  63%|██████▎   | 94/150 [10:29<06:14,  6.69s/it]\u001b[A\n","Iteration:  63%|██████▎   | 95/150 [10:36<06:08,  6.70s/it]\u001b[A\n","Iteration:  64%|██████▍   | 96/150 [10:43<06:00,  6.68s/it]\u001b[A\n","Iteration:  65%|██████▍   | 97/150 [10:49<05:53,  6.67s/it]\u001b[A\n","Iteration:  65%|██████▌   | 98/150 [10:56<05:45,  6.65s/it]\u001b[A\n","Iteration:  66%|██████▌   | 99/150 [11:03<05:40,  6.67s/it]\u001b[A\n","Iteration:  67%|██████▋   | 100/150 [11:09<05:33,  6.66s/it]\u001b[A\n","Iteration:  67%|██████▋   | 101/150 [11:16<05:25,  6.65s/it]\u001b[A\n","Iteration:  68%|██████▊   | 102/150 [11:23<05:18,  6.64s/it]\u001b[A\n","Iteration:  69%|██████▊   | 103/150 [11:29<05:12,  6.65s/it]\u001b[A\n","Iteration:  69%|██████▉   | 104/150 [11:36<05:06,  6.66s/it]\u001b[A\n","Iteration:  70%|███████   | 105/150 [11:43<04:59,  6.65s/it]\u001b[A\n","Iteration:  71%|███████   | 106/150 [11:49<04:51,  6.63s/it]\u001b[A\n","Iteration:  71%|███████▏  | 107/150 [11:56<04:45,  6.64s/it]\u001b[A\n","Iteration:  72%|███████▏  | 108/150 [12:03<04:39,  6.65s/it]\u001b[A\n","Iteration:  73%|███████▎  | 109/150 [12:09<04:32,  6.66s/it]\u001b[A\n","Iteration:  73%|███████▎  | 110/150 [12:16<04:25,  6.64s/it]\u001b[A\n","Iteration:  74%|███████▍  | 111/150 [12:22<04:19,  6.65s/it]\u001b[A\n","Iteration:  75%|███████▍  | 112/150 [12:29<04:12,  6.64s/it]\u001b[A\n","Iteration:  75%|███████▌  | 113/150 [12:36<04:06,  6.65s/it]\u001b[A\n","Iteration:  76%|███████▌  | 114/150 [12:42<03:59,  6.64s/it]\u001b[A\n","Iteration:  77%|███████▋  | 115/150 [12:49<03:52,  6.64s/it]\u001b[A\n","Iteration:  77%|███████▋  | 116/150 [12:56<03:45,  6.63s/it]\u001b[A\n","Iteration:  78%|███████▊  | 117/150 [13:02<03:39,  6.64s/it]\u001b[A\n","Iteration:  79%|███████▊  | 118/150 [13:11<03:50,  7.19s/it]\u001b[A\n","Iteration:  79%|███████▉  | 119/150 [13:17<03:38,  7.04s/it]\u001b[A\n","Iteration:  80%|████████  | 120/150 [13:24<03:28,  6.94s/it]\u001b[A\n","Iteration:  81%|████████  | 121/150 [13:31<03:19,  6.88s/it]\u001b[A\n","Iteration:  81%|████████▏ | 122/150 [13:38<03:11,  6.85s/it]\u001b[A\n","Iteration:  82%|████████▏ | 123/150 [13:44<03:03,  6.80s/it]\u001b[A\n","Iteration:  83%|████████▎ | 124/150 [13:51<02:56,  6.80s/it]\u001b[A\n","Iteration:  83%|████████▎ | 125/150 [13:58<02:48,  6.75s/it]\u001b[A\n","Iteration:  84%|████████▍ | 126/150 [14:04<02:41,  6.73s/it]\u001b[A\n","Iteration:  85%|████████▍ | 127/150 [14:11<02:34,  6.72s/it]\u001b[A\n","Iteration:  85%|████████▌ | 128/150 [14:18<02:27,  6.71s/it]\u001b[A\n","Iteration:  86%|████████▌ | 129/150 [14:25<02:20,  6.70s/it]\u001b[A\n","Iteration:  87%|████████▋ | 130/150 [14:31<02:13,  6.70s/it]\u001b[A\n","Iteration:  87%|████████▋ | 131/150 [14:38<02:06,  6.68s/it]\u001b[A\n","Iteration:  88%|████████▊ | 132/150 [14:45<02:00,  6.67s/it]\u001b[A\n","Iteration:  89%|████████▊ | 133/150 [14:51<01:53,  6.67s/it]\u001b[A\n","Iteration:  89%|████████▉ | 134/150 [14:58<01:46,  6.67s/it]\u001b[A\n","Iteration:  90%|█████████ | 135/150 [15:05<01:40,  6.68s/it]\u001b[A\n","Iteration:  91%|█████████ | 136/150 [15:11<01:33,  6.68s/it]\u001b[A\n","Iteration:  91%|█████████▏| 137/150 [15:18<01:26,  6.67s/it]\u001b[A\n","Iteration:  92%|█████████▏| 138/150 [15:25<01:20,  6.68s/it]\u001b[A\n","Iteration:  93%|█████████▎| 139/150 [15:31<01:13,  6.69s/it]\u001b[A\n","Iteration:  93%|█████████▎| 140/150 [15:38<01:06,  6.69s/it]\u001b[A\n","Iteration:  94%|█████████▍| 141/150 [15:45<01:00,  6.69s/it]\u001b[A\n","Iteration:  95%|█████████▍| 142/150 [15:51<00:53,  6.69s/it]\u001b[A\n","Iteration:  95%|█████████▌| 143/150 [15:58<00:46,  6.68s/it]\u001b[A\n","Iteration:  96%|█████████▌| 144/150 [16:05<00:40,  6.68s/it]\u001b[A\n","Iteration:  97%|█████████▋| 145/150 [16:11<00:33,  6.67s/it]\u001b[A\n","Iteration:  97%|█████████▋| 146/150 [16:18<00:26,  6.66s/it]\u001b[A\n","Iteration:  98%|█████████▊| 147/150 [16:25<00:19,  6.65s/it]\u001b[A\n","Iteration:  99%|█████████▊| 148/150 [16:31<00:13,  6.65s/it]\u001b[A\n","Iteration:  99%|█████████▉| 149/150 [16:38<00:06,  6.65s/it]\u001b[A\n","Iteration: 100%|██████████| 150/150 [16:45<00:00,  6.70s/it]\n","Epoch:  50%|█████     | 1/2 [16:45<16:45, 1005.10s/it]\n","Iteration:   0%|          | 0/150 [00:00<?, ?it/s]\u001b[A\n","Iteration:   1%|          | 1/150 [00:06<16:30,  6.64s/it]\u001b[A\n","Iteration:   1%|▏         | 2/150 [00:13<16:23,  6.64s/it]\u001b[A\n","Iteration:   2%|▏         | 3/150 [00:19<16:17,  6.65s/it]\u001b[A\n","Iteration:   3%|▎         | 4/150 [00:26<16:11,  6.65s/it]\u001b[A\n","Iteration:   3%|▎         | 5/150 [00:33<16:07,  6.67s/it]\u001b[A\n","Iteration:   4%|▍         | 6/150 [00:39<16:00,  6.67s/it]\u001b[A\n","Iteration:   5%|▍         | 7/150 [00:46<15:53,  6.67s/it]\u001b[A\n","Iteration:   5%|▌         | 8/150 [00:53<15:47,  6.67s/it]\u001b[A\n","Iteration:   6%|▌         | 9/150 [00:59<15:39,  6.67s/it]\u001b[A\n","Iteration:   7%|▋         | 10/150 [01:06<15:30,  6.65s/it]\u001b[A\n","Iteration:   7%|▋         | 11/150 [01:13<15:23,  6.65s/it]\u001b[A\n","Iteration:   8%|▊         | 12/150 [01:19<15:16,  6.64s/it]\u001b[A\n","Iteration:   9%|▊         | 13/150 [01:26<15:09,  6.64s/it]\u001b[A\n","Iteration:   9%|▉         | 14/150 [01:33<15:02,  6.64s/it]\u001b[A\n","Iteration:  10%|█         | 15/150 [01:39<14:55,  6.63s/it]\u001b[A\n","Iteration:  11%|█         | 16/150 [01:46<14:48,  6.63s/it]\u001b[A\n","Iteration:  11%|█▏        | 17/150 [01:52<14:41,  6.63s/it]\u001b[A\n","Iteration:  12%|█▏        | 18/150 [01:59<14:33,  6.62s/it]\u001b[A\n","Iteration:  13%|█▎        | 19/150 [02:06<14:27,  6.62s/it]\u001b[A\n","Iteration:  13%|█▎        | 20/150 [02:12<14:20,  6.62s/it]\u001b[A\n","Iteration:  14%|█▍        | 21/150 [02:19<14:14,  6.62s/it]\u001b[A\n","Iteration:  15%|█▍        | 22/150 [02:26<14:09,  6.64s/it]\u001b[A\n","Iteration:  15%|█▌        | 23/150 [02:32<14:06,  6.66s/it]\u001b[A\n","Iteration:  16%|█▌        | 24/150 [02:39<14:01,  6.68s/it]\u001b[A\n","Iteration:  17%|█▋        | 25/150 [02:46<13:57,  6.70s/it]\u001b[A\n","Iteration:  17%|█▋        | 26/150 [02:53<13:54,  6.73s/it]\u001b[A\n","Iteration:  18%|█▊        | 27/150 [02:59<13:44,  6.70s/it]\u001b[A\n","Iteration:  19%|█▊        | 28/150 [03:06<13:36,  6.69s/it]\u001b[A\n","Iteration:  19%|█▉        | 29/150 [03:13<13:29,  6.69s/it]\u001b[A\n","Iteration:  20%|██        | 30/150 [03:19<13:21,  6.68s/it]\u001b[A\n","Iteration:  21%|██        | 31/150 [03:26<13:16,  6.70s/it]\u001b[A\n","Iteration:  21%|██▏       | 32/150 [03:33<13:09,  6.69s/it]\u001b[A\n","Iteration:  22%|██▏       | 33/150 [03:39<13:02,  6.69s/it]\u001b[A\n","Iteration:  23%|██▎       | 34/150 [03:46<12:56,  6.69s/it]\u001b[A\n","Iteration:  23%|██▎       | 35/150 [03:53<12:49,  6.69s/it]\u001b[A\n","Iteration:  24%|██▍       | 36/150 [03:59<12:41,  6.68s/it]\u001b[A\n","Iteration:  25%|██▍       | 37/150 [04:06<12:35,  6.69s/it]\u001b[A\n","Iteration:  25%|██▌       | 38/150 [04:13<12:28,  6.69s/it]\u001b[A\n","Iteration:  26%|██▌       | 39/150 [04:19<12:21,  6.68s/it]\u001b[A\n","Iteration:  27%|██▋       | 40/150 [04:26<12:20,  6.73s/it]\u001b[A\n","Iteration:  27%|██▋       | 41/150 [04:33<12:10,  6.70s/it]\u001b[A\n","Iteration:  28%|██▊       | 42/150 [04:40<12:01,  6.68s/it]\u001b[A\n","Iteration:  29%|██▊       | 43/150 [04:46<11:55,  6.68s/it]\u001b[A\n","Iteration:  29%|██▉       | 44/150 [04:53<11:48,  6.68s/it]\u001b[A\n","Iteration:  30%|███       | 45/150 [05:00<11:40,  6.67s/it]\u001b[A\n","Iteration:  31%|███       | 46/150 [05:06<11:33,  6.67s/it]\u001b[A\n","Iteration:  31%|███▏      | 47/150 [05:13<11:26,  6.66s/it]\u001b[A\n","Iteration:  32%|███▏      | 48/150 [05:20<11:18,  6.65s/it]\u001b[A\n","Iteration:  33%|███▎      | 49/150 [05:26<11:12,  6.65s/it]\u001b[A\n","Iteration:  33%|███▎      | 50/150 [05:33<11:05,  6.65s/it]\u001b[A\n","Iteration:  34%|███▍      | 51/150 [05:39<10:58,  6.65s/it]\u001b[A\n","Iteration:  35%|███▍      | 52/150 [05:46<10:54,  6.68s/it]\u001b[A\n","Iteration:  35%|███▌      | 53/150 [05:53<10:46,  6.67s/it]\u001b[A\n","Iteration:  36%|███▌      | 54/150 [05:59<10:38,  6.66s/it]\u001b[A\n","Iteration:  37%|███▋      | 55/150 [06:06<10:31,  6.65s/it]\u001b[A\n","Iteration:  37%|███▋      | 56/150 [06:13<10:24,  6.64s/it]\u001b[A\n","Iteration:  38%|███▊      | 57/150 [06:19<10:17,  6.63s/it]\u001b[A\n","Iteration:  39%|███▊      | 58/150 [06:26<10:10,  6.64s/it]\u001b[A\n","Iteration:  39%|███▉      | 59/150 [06:35<10:56,  7.21s/it]\u001b[A\n","Iteration:  40%|████      | 60/150 [06:41<10:36,  7.07s/it]\u001b[A\n","Iteration:  41%|████      | 61/150 [06:48<10:21,  6.98s/it]\u001b[A\n","Iteration:  41%|████▏     | 62/150 [06:55<10:08,  6.92s/it]\u001b[A\n","Iteration:  42%|████▏     | 63/150 [07:02<09:56,  6.86s/it]\u001b[A\n","Iteration:  43%|████▎     | 64/150 [07:08<09:45,  6.80s/it]\u001b[A\n","Iteration:  43%|████▎     | 65/150 [07:15<09:36,  6.78s/it]\u001b[A\n","Iteration:  44%|████▍     | 66/150 [07:22<09:30,  6.79s/it]\u001b[A\n","Iteration:  45%|████▍     | 67/150 [07:28<09:21,  6.76s/it]\u001b[A\n","Iteration:  45%|████▌     | 68/150 [07:35<09:11,  6.72s/it]\u001b[A\n","Iteration:  46%|████▌     | 69/150 [07:42<09:02,  6.70s/it]\u001b[A\n","Iteration:  47%|████▋     | 70/150 [07:48<08:56,  6.70s/it]\u001b[A\n","Iteration:  47%|████▋     | 71/150 [07:55<08:48,  6.69s/it]\u001b[A\n","Iteration:  48%|████▊     | 72/150 [08:02<08:39,  6.67s/it]\u001b[A\n","Iteration:  49%|████▊     | 73/150 [08:08<08:32,  6.66s/it]\u001b[A\n","Iteration:  49%|████▉     | 74/150 [08:15<08:25,  6.65s/it]\u001b[A\n","Iteration:  50%|█████     | 75/150 [08:22<08:18,  6.65s/it]\u001b[A\n","Iteration:  51%|█████     | 76/150 [08:28<08:11,  6.65s/it]\u001b[A\n","Iteration:  51%|█████▏    | 77/150 [08:35<08:05,  6.65s/it]\u001b[A\n","Iteration:  52%|█████▏    | 78/150 [08:42<07:58,  6.65s/it]\u001b[A\n","Iteration:  53%|█████▎    | 79/150 [08:48<07:52,  6.66s/it]\u001b[A\n","Iteration:  53%|█████▎    | 80/150 [08:55<07:45,  6.65s/it]\u001b[A\n","Iteration:  54%|█████▍    | 81/150 [09:02<07:38,  6.65s/it]\u001b[A\n","Iteration:  55%|█████▍    | 82/150 [09:08<07:31,  6.64s/it]\u001b[A\n","Iteration:  55%|█████▌    | 83/150 [09:15<07:24,  6.64s/it]\u001b[A\n","Iteration:  56%|█████▌    | 84/150 [09:21<07:17,  6.63s/it]\u001b[A\n","Iteration:  57%|█████▋    | 85/150 [09:28<07:11,  6.64s/it]\u001b[A\n","Iteration:  57%|█████▋    | 86/150 [09:35<07:05,  6.64s/it]\u001b[A\n","Iteration:  58%|█████▊    | 87/150 [09:41<06:58,  6.64s/it]\u001b[A\n","Iteration:  59%|█████▊    | 88/150 [09:48<06:52,  6.65s/it]\u001b[A\n","Iteration:  59%|█████▉    | 89/150 [09:55<06:45,  6.65s/it]\u001b[A\n","Iteration:  60%|██████    | 90/150 [10:01<06:38,  6.64s/it]\u001b[A\n","Iteration:  61%|██████    | 91/150 [10:08<06:32,  6.65s/it]\u001b[A\n","Iteration:  61%|██████▏   | 92/150 [10:15<06:25,  6.65s/it]\u001b[A\n","Iteration:  62%|██████▏   | 93/150 [10:21<06:19,  6.66s/it]\u001b[A\n","Iteration:  63%|██████▎   | 94/150 [10:28<06:13,  6.66s/it]\u001b[A\n","Iteration:  63%|██████▎   | 95/150 [10:35<06:07,  6.67s/it]\u001b[A\n","Iteration:  64%|██████▍   | 96/150 [10:41<06:00,  6.67s/it]\u001b[A\n","Iteration:  65%|██████▍   | 97/150 [10:48<05:54,  6.70s/it]\u001b[A\n","Iteration:  65%|██████▌   | 98/150 [10:55<05:48,  6.71s/it]\u001b[A\n","Iteration:  66%|██████▌   | 99/150 [11:02<05:42,  6.71s/it]\u001b[A\n","Iteration:  67%|██████▋   | 100/150 [11:08<05:34,  6.69s/it]\u001b[A\n","Iteration:  67%|██████▋   | 101/150 [11:15<05:27,  6.69s/it]\u001b[A\n","Iteration:  68%|██████▊   | 102/150 [11:22<05:20,  6.69s/it]\u001b[A\n","Iteration:  69%|██████▊   | 103/150 [11:28<05:13,  6.68s/it]\u001b[A\n","Iteration:  69%|██████▉   | 104/150 [11:35<05:07,  6.68s/it]\u001b[A\n","Iteration:  70%|███████   | 105/150 [11:42<05:00,  6.67s/it]\u001b[A\n","Iteration:  71%|███████   | 106/150 [11:48<04:53,  6.67s/it]\u001b[A\n","Iteration:  71%|███████▏  | 107/150 [11:55<04:47,  6.68s/it]\u001b[A\n","Iteration:  72%|███████▏  | 108/150 [12:02<04:40,  6.67s/it]\u001b[A\n","Iteration:  73%|███████▎  | 109/150 [12:08<04:33,  6.67s/it]\u001b[A\n","Iteration:  73%|███████▎  | 110/150 [12:15<04:26,  6.67s/it]\u001b[A\n","Iteration:  74%|███████▍  | 111/150 [12:22<04:20,  6.67s/it]\u001b[A\n","Iteration:  75%|███████▍  | 112/150 [12:28<04:13,  6.67s/it]\u001b[A\n","Iteration:  75%|███████▌  | 113/150 [12:35<04:06,  6.67s/it]\u001b[A\n","Iteration:  76%|███████▌  | 114/150 [12:42<04:00,  6.67s/it]\u001b[A\n","Iteration:  77%|███████▋  | 115/150 [12:48<03:53,  6.69s/it]\u001b[A\n","Iteration:  77%|███████▋  | 116/150 [12:55<03:47,  6.69s/it]\u001b[A\n","Iteration:  78%|███████▊  | 117/150 [13:02<03:40,  6.68s/it]\u001b[A\n","Iteration:  79%|███████▊  | 118/150 [13:08<03:33,  6.68s/it]\u001b[A\n","Iteration:  79%|███████▉  | 119/150 [13:15<03:27,  6.68s/it]\u001b[A\n","Iteration:  80%|████████  | 120/150 [13:22<03:20,  6.68s/it]\u001b[A\n","Iteration:  81%|████████  | 121/150 [13:28<03:13,  6.67s/it]\u001b[A\n","Iteration:  81%|████████▏ | 122/150 [13:35<03:06,  6.67s/it]\u001b[A\n","Iteration:  82%|████████▏ | 123/150 [13:42<02:59,  6.66s/it]\u001b[A\n","Iteration:  83%|████████▎ | 124/150 [13:48<02:53,  6.67s/it]\u001b[A\n","Iteration:  83%|████████▎ | 125/150 [13:55<02:46,  6.67s/it]\u001b[A\n","Iteration:  84%|████████▍ | 126/150 [14:02<02:40,  6.67s/it]\u001b[A\n","Iteration:  85%|████████▍ | 127/150 [14:08<02:33,  6.68s/it]\u001b[A\n","Iteration:  85%|████████▌ | 128/150 [14:15<02:26,  6.68s/it]\u001b[A\n","Iteration:  86%|████████▌ | 129/150 [14:22<02:20,  6.68s/it]\u001b[A\n","Iteration:  87%|████████▋ | 130/150 [14:28<02:13,  6.68s/it]\u001b[A\n","Iteration:  87%|████████▋ | 131/150 [14:35<02:06,  6.68s/it]\u001b[A\n","Iteration:  88%|████████▊ | 132/150 [14:42<02:00,  6.69s/it]\u001b[A\n","Iteration:  89%|████████▊ | 133/150 [14:49<01:53,  6.69s/it]\u001b[A\n","Iteration:  89%|████████▉ | 134/150 [14:55<01:47,  6.69s/it]\u001b[A\n","Iteration:  90%|█████████ | 135/150 [15:02<01:40,  6.68s/it]\u001b[A\n","Iteration:  91%|█████████ | 136/150 [15:09<01:33,  6.68s/it]\u001b[A\n","Iteration:  91%|█████████▏| 137/150 [15:15<01:26,  6.68s/it]\u001b[A\n","Iteration:  92%|█████████▏| 138/150 [15:22<01:20,  6.68s/it]\u001b[A\n","Iteration:  93%|█████████▎| 139/150 [15:29<01:13,  6.68s/it]\u001b[A\n","Iteration:  93%|█████████▎| 140/150 [15:35<01:06,  6.68s/it]\u001b[A\n","Iteration:  94%|█████████▍| 141/150 [15:42<00:59,  6.67s/it]\u001b[A\n","Iteration:  95%|█████████▍| 142/150 [15:49<00:53,  6.67s/it]\u001b[A\n","Iteration:  95%|█████████▌| 143/150 [15:55<00:46,  6.67s/it]\u001b[A\n","Iteration:  96%|█████████▌| 144/150 [16:02<00:40,  6.69s/it]\u001b[A\n","Iteration:  97%|█████████▋| 145/150 [16:09<00:33,  6.70s/it]\u001b[A\n","Iteration:  97%|█████████▋| 146/150 [16:15<00:26,  6.69s/it]\u001b[A\n","Iteration:  98%|█████████▊| 147/150 [16:22<00:20,  6.72s/it]\u001b[A\n","Iteration:  99%|█████████▊| 148/150 [16:29<00:13,  6.76s/it]\u001b[A\n","Iteration:  99%|█████████▉| 149/150 [16:36<00:06,  6.75s/it]\u001b[A\n","Iteration: 100%|██████████| 150/150 [16:44<00:00,  6.70s/it]\n","Epoch: 100%|██████████| 2/2 [33:29<00:00, 1004.92s/it]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"Jh-W6R0iIswf","colab_type":"text"},"source":["**predicting using the trained model**"]},{"cell_type":"code","metadata":{"id":"nRhMCpgnLQlh","colab_type":"code","colab":{}},"source":["args = sequence_labelling_data_input(data_dir = \"custom_data\",model_type = \"layoutlm\",output_dir = \"output_dir2\",model_name_or_path = 'bert-base-uncased',do_lower_case = True,max_seq_length = 512,do_predict = True,labels = \"data/labels.txt\",fp16 = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1wj-xDtxMHdf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1597046422259,"user_tz":-330,"elapsed":203140,"user":{"displayName":"Rohan Kakarlapudi","photoUrl":"","userId":"09959240400628694972"}},"outputId":"384fc2cc-3084-4b96-986f-694eca7cb45a"},"source":["if __name__ == \"__main__\":\n","    main(args)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\rEvaluating:   0%|          | 0/103 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n"," 5\n","\n","\n"," dict_keys(['input_ids', 'attention_mask', 'labels', 'bbox', 'token_type_ids'])\n","{'input_ids': tensor([[  101,  2012,  2102,  ...,     0,     0,     0],\n","        [  101, 11703,  2184,  ...,     0,     0,     0],\n","        [  101,  1052,  1012,  ...,     0,     0,     0],\n","        ...,\n","        [  101, 18777,  6904,  ...,     0,     0,     0],\n","        [  101,  6726,  3104,  ...,     0,     0,     0],\n","        [  101,  6904,  2595,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[-100,    9, -100,  ..., -100, -100, -100],\n","        [-100,    9,    9,  ..., -100, -100, -100],\n","        [-100,    9, -100,  ..., -100, -100, -100],\n","        ...,\n","        [-100,    1,    4,  ..., -100, -100, -100],\n","        [-100,    1,    7,  ..., -100, -100, -100],\n","        [-100,    9, -100,  ..., -100, -100, -100]]), 'bbox': tensor([[[  0,   0,   0,   0],\n","         [136,  87, 173, 100],\n","         [136,  87, 173, 100],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        [[  0,   0,   0,   0],\n","         [557,  85, 583,  99],\n","         [588,  85, 608,  99],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        [[  0,   0,   0,   0],\n","         [745,  84, 761,  96],\n","         [745,  84, 761,  96],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        ...,\n","\n","        [[  0,   0,   0,   0],\n","         [366, 252, 502, 266],\n","         [506, 252, 604, 265],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        [[  0,   0,   0,   0],\n","         [352, 268, 485, 281],\n","         [493, 268, 554, 281],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        [[  0,   0,   0,   0],\n","         [391, 298, 428, 313],\n","         [391, 298, 428, 313],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]])}\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:   1%|          | 1/103 [00:47<1:20:19, 47.25s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n"," 5\n","\n","\n"," dict_keys(['input_ids', 'attention_mask', 'labels', 'bbox', 'token_type_ids'])\n","{'input_ids': tensor([[ 101, 2000, 1024,  ...,    0,    0,    0],\n","        [ 101, 2577, 3347,  ...,    0,    0,    0],\n","        [ 101, 6904, 2595,  ...,    0,    0,    0],\n","        ...,\n","        [ 101, 3058, 1024,  ...,    0,    0,    0],\n","        [ 101, 2260, 1013,  ...,    0,    0,    0],\n","        [ 101, 2193, 1997,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[-100,   12, -100,  ..., -100, -100, -100],\n","        [-100,    0,    3,  ..., -100, -100, -100],\n","        [-100,    0, -100,  ..., -100, -100, -100],\n","        ...,\n","        [-100,   12, -100,  ..., -100, -100, -100],\n","        [-100,   10, -100,  ..., -100, -100, -100],\n","        [-100,    2,    8,  ..., -100, -100, -100]]), 'bbox': tensor([[[  0,   0,   0,   0],\n","         [139, 346, 176, 359],\n","         [139, 346, 176, 359],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        [[  0,   0,   0,   0],\n","         [193, 346, 245, 360],\n","         [249, 346, 311, 361],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        [[  0,   0,   0,   0],\n","         [137, 377, 173, 391],\n","         [137, 377, 173, 391],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        ...,\n","\n","        [[  0,   0,   0,   0],\n","         [137, 409, 196, 423],\n","         [137, 409, 196, 423],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        [[  0,   0,   0,   0],\n","         [245, 410, 311, 423],\n","         [245, 410, 311, 423],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        [[  0,   0,   0,   0],\n","         [137, 442, 216, 455],\n","         [224, 442, 248, 455],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]])}\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:   2%|▏         | 2/103 [01:34<1:19:23, 47.16s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n"," 5\n","\n","\n"," dict_keys(['input_ids', 'attention_mask', 'labels', 'bbox', 'token_type_ids'])\n","{'input_ids': tensor([[ 101, 1017,  102,  ...,    0,    0,    0],\n","        [ 101, 4604, 2121,  ...,    0,    0,    0],\n","        [ 101, 2569, 8128,  ...,    0,    0,    0],\n","        ...,\n","        [ 101, 2004, 2574,  ...,    0,    0,    0],\n","        [ 101, 3602, 1024,  ...,    0,    0,    0],\n","        [ 101, 2023, 4471,  ...,    0,    0,    0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[-100,   10, -100,  ..., -100, -100, -100],\n","        [-100,    0, -100,  ..., -100, -100, -100],\n","        [-100,    9,    9,  ..., -100, -100, -100],\n","        ...,\n","        [-100,    9,    9,  ..., -100, -100, -100],\n","        [-100,    9, -100,  ..., -100, -100, -100],\n","        [-100,    9,    9,  ..., -100, -100, -100]]), 'bbox': tensor([[[   0,    0,    0,    0],\n","         [ 615,  442,  624,  454],\n","         [1000, 1000, 1000, 1000],\n","         ...,\n","         [   0,    0,    0,    0],\n","         [   0,    0,    0,    0],\n","         [   0,    0,    0,    0]],\n","\n","        [[   0,    0,    0,    0],\n","         [ 136,  474,  278,  487],\n","         [ 136,  474,  278,  487],\n","         ...,\n","         [   0,    0,    0,    0],\n","         [   0,    0,    0,    0],\n","         [   0,    0,    0,    0]],\n","\n","        [[   0,    0,    0,    0],\n","         [ 136,  505,  214,  519],\n","         [ 220,  505,  364,  519],\n","         ...,\n","         [   0,    0,    0,    0],\n","         [   0,    0,    0,    0],\n","         [   0,    0,    0,    0]],\n","\n","        ...,\n","\n","        [[   0,    0,    0,    0],\n","         [ 387,  633,  409,  646],\n","         [ 413,  632,  461,  646],\n","         ...,\n","         [   0,    0,    0,    0],\n","         [   0,    0,    0,    0],\n","         [   0,    0,    0,    0]],\n","\n","        [[   0,    0,    0,    0],\n","         [ 136,  693,  185,  706],\n","         [ 136,  693,  185,  706],\n","         ...,\n","         [   0,    0,    0,    0],\n","         [   0,    0,    0,    0],\n","         [   0,    0,    0,    0]],\n","\n","        [[   0,    0,    0,    0],\n","         [ 218,  693,  251,  706],\n","         [ 255,  693,  326,  706],\n","         ...,\n","         [   0,    0,    0,    0],\n","         [   0,    0,    0,    0],\n","         [   0,    0,    0,    0]]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]])}\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:   3%|▎         | 3/103 [02:21<1:18:27, 47.08s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n"," 5\n","\n","\n"," dict_keys(['input_ids', 'attention_mask', 'labels', 'bbox', 'token_type_ids'])\n","{'input_ids': tensor([[  101,  3183,  2009,  ...,     0,     0,     0],\n","        [  101, 18777,  1010,  ...,     0,     0,     0],\n","        [  101,  8068,  1997,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  2363,  2023,  ...,     0,     0,     0],\n","        [  101,  2434,  4471,  ...,     0,     0,     0],\n","        [  101,  6792,  1012,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[-100,    9,    9,  ..., -100, -100, -100],\n","        [-100,    9, -100,  ..., -100, -100, -100],\n","        [-100,    9,    9,  ..., -100, -100, -100],\n","        ...,\n","        [-100,    9,    9,  ..., -100, -100, -100],\n","        [-100,    9,    9,  ..., -100, -100, -100],\n","        [-100,    9, -100,  ..., -100, -100, -100]]), 'bbox': tensor([[[  0,   0,   0,   0],\n","         [218, 707, 263, 719],\n","         [273, 707, 290, 719],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        [[  0,   0,   0,   0],\n","         [218, 721, 334, 733],\n","         [218, 721, 334, 733],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        [[  0,   0,   0,   0],\n","         [217, 735, 255, 746],\n","         [258, 735, 271, 746],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        ...,\n","\n","        [[  0,   0,   0,   0],\n","         [217, 774, 270, 786],\n","         [274, 774, 299, 786],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        [[  0,   0,   0,   0],\n","         [217, 789, 267, 800],\n","         [270, 788, 324, 800],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        [[  0,   0,   0,   0],\n","         [217, 802, 293, 813],\n","         [217, 802, 293, 813],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]])}\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:   4%|▍         | 4/103 [03:08<1:17:38, 47.05s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n"," 5\n","\n","\n"," dict_keys(['input_ids', 'attention_mask', 'labels', 'bbox', 'token_type_ids'])\n","{'input_ids': tensor([[  101, 26667,  2683,  ...,     0,     0,     0],\n","        [  101,  2110,  2436,  ...,     0,     0,     0],\n","        [  101,  7479,  1012,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  8356,  3319,  ...,     0,     0,     0],\n","        [  101,  9098,  2671,  ...,     0,     0,     0],\n","        [  101,  8819,  2053,  ...,     0,     0,     0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        ...,\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0],\n","        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([[-100,    9, -100,  ..., -100, -100, -100],\n","        [-100,    9,    9,  ..., -100, -100, -100],\n","        [-100,    9, -100,  ..., -100, -100, -100],\n","        ...,\n","        [-100,    9,    9,  ..., -100, -100, -100],\n","        [-100,    1,    4,  ..., -100, -100, -100],\n","        [-100,    2,    5,  ..., -100, -100, -100]]), 'bbox': tensor([[[  0,   0,   0,   0],\n","         [866, 774, 838, 875],\n","         [866, 774, 838, 875],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        [[  0,   0,   0,   0],\n","         [242, 889, 275, 902],\n","         [279, 889, 319, 902],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        [[  0,   0,   0,   0],\n","         [413, 904, 541, 914],\n","         [413, 904, 541, 914],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        ...,\n","\n","        [[  0,   0,   0,   0],\n","         [125,  67, 218,  84],\n","         [220,  70, 278,  84],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        [[  0,   0,   0,   0],\n","         [439, 128, 518, 142],\n","         [527, 129, 597, 143],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]],\n","\n","        [[  0,   0,   0,   0],\n","         [122, 170, 220, 184],\n","         [224, 170, 258, 184],\n","         ...,\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0],\n","         [  0,   0,   0,   0]]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]])}\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-ed4bd21d196e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-45-59a4e90f3aed>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         result, predictions = evaluate(\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token_label_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         )\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# Save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-42-80f93ff1f3ba>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(args, model, tokenizer, labels, pad_token_label_id, mode, prefix)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mtmp_eval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-43c3122275ef>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, bbox, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         )\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-43c3122275ef>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, bbox, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         embedding_output = self.embeddings(\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m         )\n\u001b[1;32m    173\u001b[0m         encoder_outputs = self.encoder(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-40-43c3122275ef>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, bbox, token_type_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[1;32m     80\u001b[0m         )\n\u001b[1;32m     81\u001b[0m         w_position_embeddings = self.w_position_embeddings(\n\u001b[0;32m---> 82\u001b[0;31m             \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m         )\n\u001b[1;32m     84\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index out of range in self"]}]},{"cell_type":"code","metadata":{"id":"2X6Y5pcRuD0H","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["e93b24f7a0b54667ab2f55cab884a217","eac4981f5c9448cf8ea335eb6bf2a3cd","c72a5c3430554796a51dacc357aa4774","8ae47b774f3549b997e53a0a779a0bc7","3616b4ae16494628b8d6f427f7731c3c","7b7087c4752c406c98fc826059da2fd4","e71e4a9f79b244b0a2b20b169f0a7a24","92861b059e224ef1ad9efb283bf18e64"]},"executionInfo":{"status":"error","timestamp":1597210567715,"user_tz":-330,"elapsed":106936,"user":{"displayName":"Rohan Kakarlapudi","photoUrl":"","userId":"09959240400628694972"}},"outputId":"300c2f82-2e30-4591-a048-85960a26a948"},"source":["if __name__ == \"__main__\":\n","    main(args)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e93b24f7a0b54667ab2f55cab884a217","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:   0%|          | 0/103 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n"," lenght :-  5\n","\n","\n"," keys :-  dict_keys(['input_ids', 'attention_mask', 'labels', 'bbox', 'token_type_ids'])\n","\n","\n","\n","\n","\n","\n"," bbox shape :-  torch.Size([8, 512, 4])\n","\n","bbox[:,:,2] :-  tensor([[  0, 173, 173,  ...,   0,   0,   0],\n","        [  0, 583, 608,  ...,   0,   0,   0],\n","        [  0, 761, 761,  ...,   0,   0,   0],\n","        ...,\n","        [  0, 502, 604,  ...,   0,   0,   0],\n","        [  0, 485, 554,  ...,   0,   0,   0],\n","        [  0, 428, 428,  ...,   0,   0,   0]])\n","\n","bbox[:,:,2] shape :-  torch.Size([8, 512])\n","\n","bbox[:, :, 0] :-  tensor([[  0, 136, 136,  ...,   0,   0,   0],\n","        [  0, 557, 588,  ...,   0,   0,   0],\n","        [  0, 745, 745,  ...,   0,   0,   0],\n","        ...,\n","        [  0, 366, 506,  ...,   0,   0,   0],\n","        [  0, 352, 493,  ...,   0,   0,   0],\n","        [  0, 391, 391,  ...,   0,   0,   0]])\n","\n","bbox[:, :, 0] shape :-  torch.Size([8, 512])\n","\n"," sub of two above matrix :-  tensor([[  0,  37,  37,  ...,   0,   0,   0],\n","        [  0,  26,  20,  ...,   0,   0,   0],\n","        [  0,  16,  16,  ...,   0,   0,   0],\n","        ...,\n","        [  0, 136,  98,  ...,   0,   0,   0],\n","        [  0, 133,  61,  ...,   0,   0,   0],\n","        [  0,  37,  37,  ...,   0,   0,   0]])\n","\n"," printing the w_position_embeddings:-  Embedding(1024, 768)\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:   1%|          | 1/103 [00:16<28:13, 16.61s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n"," lenght :-  5\n","\n","\n"," keys :-  dict_keys(['input_ids', 'attention_mask', 'labels', 'bbox', 'token_type_ids'])\n","\n","\n","\n","\n","\n","\n"," bbox shape :-  torch.Size([8, 512, 4])\n","\n","bbox[:,:,2] :-  tensor([[  0, 176, 176,  ...,   0,   0,   0],\n","        [  0, 245, 311,  ...,   0,   0,   0],\n","        [  0, 173, 173,  ...,   0,   0,   0],\n","        ...,\n","        [  0, 196, 196,  ...,   0,   0,   0],\n","        [  0, 311, 311,  ...,   0,   0,   0],\n","        [  0, 216, 248,  ...,   0,   0,   0]])\n","\n","bbox[:,:,2] shape :-  torch.Size([8, 512])\n","\n","bbox[:, :, 0] :-  tensor([[  0, 139, 139,  ...,   0,   0,   0],\n","        [  0, 193, 249,  ...,   0,   0,   0],\n","        [  0, 137, 137,  ...,   0,   0,   0],\n","        ...,\n","        [  0, 137, 137,  ...,   0,   0,   0],\n","        [  0, 245, 245,  ...,   0,   0,   0],\n","        [  0, 137, 224,  ...,   0,   0,   0]])\n","\n","bbox[:, :, 0] shape :-  torch.Size([8, 512])\n","\n"," sub of two above matrix :-  tensor([[ 0, 37, 37,  ...,  0,  0,  0],\n","        [ 0, 52, 62,  ...,  0,  0,  0],\n","        [ 0, 36, 36,  ...,  0,  0,  0],\n","        ...,\n","        [ 0, 59, 59,  ...,  0,  0,  0],\n","        [ 0, 66, 66,  ...,  0,  0,  0],\n","        [ 0, 79, 24,  ...,  0,  0,  0]])\n","\n"," printing the w_position_embeddings:-  Embedding(1024, 768)\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:   2%|▏         | 2/103 [00:30<26:47, 15.92s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n"," lenght :-  5\n","\n","\n"," keys :-  dict_keys(['input_ids', 'attention_mask', 'labels', 'bbox', 'token_type_ids'])\n","\n","\n","\n","\n","\n","\n"," bbox shape :-  torch.Size([8, 512, 4])\n","\n","bbox[:,:,2] :-  tensor([[   0,  624, 1000,  ...,    0,    0,    0],\n","        [   0,  278,  278,  ...,    0,    0,    0],\n","        [   0,  214,  364,  ...,    0,    0,    0],\n","        ...,\n","        [   0,  409,  461,  ...,    0,    0,    0],\n","        [   0,  185,  185,  ...,    0,    0,    0],\n","        [   0,  251,  326,  ...,    0,    0,    0]])\n","\n","bbox[:,:,2] shape :-  torch.Size([8, 512])\n","\n","bbox[:, :, 0] :-  tensor([[   0,  615, 1000,  ...,    0,    0,    0],\n","        [   0,  136,  136,  ...,    0,    0,    0],\n","        [   0,  136,  220,  ...,    0,    0,    0],\n","        ...,\n","        [   0,  387,  413,  ...,    0,    0,    0],\n","        [   0,  136,  136,  ...,    0,    0,    0],\n","        [   0,  218,  255,  ...,    0,    0,    0]])\n","\n","bbox[:, :, 0] shape :-  torch.Size([8, 512])\n","\n"," sub of two above matrix :-  tensor([[  0,   9,   0,  ...,   0,   0,   0],\n","        [  0, 142, 142,  ...,   0,   0,   0],\n","        [  0,  78, 144,  ...,   0,   0,   0],\n","        ...,\n","        [  0,  22,  48,  ...,   0,   0,   0],\n","        [  0,  49,  49,  ...,   0,   0,   0],\n","        [  0,  33,  71,  ...,   0,   0,   0]])\n","\n"," printing the w_position_embeddings:-  Embedding(1024, 768)\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:   3%|▎         | 3/103 [00:45<25:41, 15.42s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n"," lenght :-  5\n","\n","\n"," keys :-  dict_keys(['input_ids', 'attention_mask', 'labels', 'bbox', 'token_type_ids'])\n","\n","\n","\n","\n","\n","\n"," bbox shape :-  torch.Size([8, 512, 4])\n","\n","bbox[:,:,2] :-  tensor([[  0, 263, 290,  ...,   0,   0,   0],\n","        [  0, 334, 334,  ...,   0,   0,   0],\n","        [  0, 255, 271,  ...,   0,   0,   0],\n","        ...,\n","        [  0, 270, 299,  ...,   0,   0,   0],\n","        [  0, 267, 324,  ...,   0,   0,   0],\n","        [  0, 293, 293,  ...,   0,   0,   0]])\n","\n","bbox[:,:,2] shape :-  torch.Size([8, 512])\n","\n","bbox[:, :, 0] :-  tensor([[  0, 218, 273,  ...,   0,   0,   0],\n","        [  0, 218, 218,  ...,   0,   0,   0],\n","        [  0, 217, 258,  ...,   0,   0,   0],\n","        ...,\n","        [  0, 217, 274,  ...,   0,   0,   0],\n","        [  0, 217, 270,  ...,   0,   0,   0],\n","        [  0, 217, 217,  ...,   0,   0,   0]])\n","\n","bbox[:, :, 0] shape :-  torch.Size([8, 512])\n","\n"," sub of two above matrix :-  tensor([[  0,  45,  17,  ...,   0,   0,   0],\n","        [  0, 116, 116,  ...,   0,   0,   0],\n","        [  0,  38,  13,  ...,   0,   0,   0],\n","        ...,\n","        [  0,  53,  25,  ...,   0,   0,   0],\n","        [  0,  50,  54,  ...,   0,   0,   0],\n","        [  0,  76,  76,  ...,   0,   0,   0]])\n","\n"," printing the w_position_embeddings:-  Embedding(1024, 768)\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:   4%|▍         | 4/103 [00:59<24:53, 15.09s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n"," lenght :-  5\n","\n","\n"," keys :-  dict_keys(['input_ids', 'attention_mask', 'labels', 'bbox', 'token_type_ids'])\n","\n","\n","\n","\n","\n","\n"," bbox shape :-  torch.Size([8, 512, 4])\n","\n","bbox[:,:,2] :-  tensor([[  0, 838, 838,  ...,   0,   0,   0],\n","        [  0, 275, 319,  ...,   0,   0,   0],\n","        [  0, 541, 541,  ...,   0,   0,   0],\n","        ...,\n","        [  0, 218, 278,  ...,   0,   0,   0],\n","        [  0, 518, 597,  ...,   0,   0,   0],\n","        [  0, 220, 258,  ...,   0,   0,   0]])\n","\n","bbox[:,:,2] shape :-  torch.Size([8, 512])\n","\n","bbox[:, :, 0] :-  tensor([[  0, 866, 866,  ...,   0,   0,   0],\n","        [  0, 242, 279,  ...,   0,   0,   0],\n","        [  0, 413, 413,  ...,   0,   0,   0],\n","        ...,\n","        [  0, 125, 220,  ...,   0,   0,   0],\n","        [  0, 439, 527,  ...,   0,   0,   0],\n","        [  0, 122, 224,  ...,   0,   0,   0]])\n","\n","bbox[:, :, 0] shape :-  torch.Size([8, 512])\n","\n"," sub of two above matrix :-  tensor([[  0, -28, -28,  ...,   0,   0,   0],\n","        [  0,  33,  40,  ...,   0,   0,   0],\n","        [  0, 128, 128,  ...,   0,   0,   0],\n","        ...,\n","        [  0,  93,  58,  ...,   0,   0,   0],\n","        [  0,  79,  70,  ...,   0,   0,   0],\n","        [  0,  98,  34,  ...,   0,   0,   0]])\n","\n"," printing the w_position_embeddings:-  Embedding(1024, 768)\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-ed4bd21d196e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-19-59a4e90f3aed>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         result, predictions = evaluate(\n\u001b[0;32m--> 195\u001b[0;31m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token_label_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m         )\n\u001b[1;32m    197\u001b[0m         \u001b[0;31m# Save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-17-2a557158c6a6>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(args, model, tokenizer, labels, pad_token_label_id, mode, prefix)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n keys :- \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\\n\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mtmp_eval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-48dc0d2aa00e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, bbox, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         )\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-48dc0d2aa00e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, bbox, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m         embedding_output = self.embeddings(\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         )\n\u001b[1;32m    180\u001b[0m         encoder_outputs = self.encoder(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-48dc0d2aa00e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, bbox, token_type_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n printing the w_position_embeddings:- \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_position_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         w_position_embeddings = self.w_position_embeddings(\n\u001b[0;32m---> 89\u001b[0;31m             \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         )\n\u001b[1;32m     91\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index out of range in self"]}]}]}