{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CHK-sEYI5pie"
   },
   "outputs": [],
   "source": [
    "import contextlib, glob, os, pickle, platform, random, sys, wave\n",
    "import numpy as np\n",
    "from scipy.io.wavfile import read\n",
    "import soundfile as sf\n",
    "from scipy.io import loadmat, savemat\n",
    "\n",
    "\n",
    "def save_wav(path, wav, f_s):\n",
    "    wav = np.squeeze(wav) \n",
    "    if isinstance(wav[0], np.float32): wav = np.asarray(np.multiply(wav, 32768.0), dtype=np.int16)\n",
    "    sf.write(path, wav, f_s)\n",
    "\n",
    "def read_wav(path):\n",
    "    wav, f_s = sf.read(path, dtype='int16')\n",
    "    return wav, f_s\n",
    "\n",
    "# CAN BE REMOVED\n",
    "# def save_mat(path, data, name):\n",
    "#     if not path.endswith('.mat'): path = path + '.mat'\n",
    "#     savemat(path, {name: data})\n",
    "\n",
    "def Batch(fdir, snr_l=[]):\n",
    "    fname_l = [] # list of file names.\n",
    "    wav_l = [] # list for waveforms.\n",
    "    snr_test_l = [] # list of SNR levels for the test set.\n",
    "    fnames = ['*.wav', '*.flac', '*.mp3']\n",
    "\n",
    "    for fname in fnames:\n",
    "        for fpath in glob.glob(os.path.join(fdir, fname)):\n",
    "            for snr in snr_l:\n",
    "                if fpath.find('_' + str(snr) + 'dB') != -1:\n",
    "                    snr_test_l.append(snr) \n",
    "            (wav, _) = read_wav(fpath) \n",
    "            if np.isnan(wav).any() or np.isinf(wav).any():\n",
    "                raise ValueError('Error: NaN or Inf value. File path: %s.' % (file_path))\n",
    "            wav_l.append(wav)\n",
    "            fname_l.append(os.path.basename(os.path.splitext(fpath)[0]))\n",
    "\n",
    "    len_l = [] \n",
    "    maxlen = max(len(wav) for wav in wav_l)\n",
    "    wav_np = np.zeros([len(wav_l), maxlen], np.int16) \n",
    "\n",
    "    for (i, wav) in zip(range(len(wav_l)), wav_l):\n",
    "        wav_np[i,:len(wav)] = wav\n",
    "        len_l.append(len(wav)) \n",
    "    return wav_np, np.array(len_l, np.int32), np.array(snr_test_l, np.int32), fname_l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PcC3QVnc7w9v"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.ops.signal import window_ops\n",
    "import functools\n",
    "import numpy as np\n",
    "import scipy.special as spsp\n",
    "import tensorflow as tf\n",
    "\n",
    "class STFT:\n",
    "    def __init__(self, N_d, N_s, NFFT, f_s):\n",
    "        self.N_d = N_d\n",
    "        self.N_s = N_s\n",
    "        self.NFFT = NFFT\n",
    "        self.f_s = f_s\n",
    "        self.W = functools.partial(window_ops.hamming_window, periodic=False)\n",
    "        self.ten = tf.cast(10.0, tf.float32)\n",
    "\n",
    "    def polar_analysis(self, x):\n",
    "        STFT = tf.signal.stft(x, self.N_d, self.N_s, self.NFFT, window_fn=self.W, pad_end=True)\n",
    "        return tf.abs(STFT), tf.math.angle(STFT)\n",
    "\n",
    "    def polar_synthesis(self, STMS, STPS):\n",
    "        STFT = tf.cast(STMS, tf.complex64)*tf.exp(1j*tf.cast(STPS, tf.complex64))\n",
    "        return tf.signal.inverse_stft(STFT, self.N_d, self.N_s, self.NFFT, tf.signal.inverse_stft_window_fn(self.N_s, self.W))\n",
    "\n",
    "class DeepXiInput(STFT):\n",
    "    def __init__(self, N_d, N_s, NFFT, f_s, mu=None, sigma=None):\n",
    "        super().__init__(N_d, N_s, NFFT, f_s)\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "\n",
    "    def observation(self, x):\n",
    "        x = self.normalise(x)\n",
    "        x_STMS, x_STPS = self.polar_analysis(x)\n",
    "        return x_STMS, x_STPS\n",
    "\n",
    "#     def example(self, s, d, s_len, d_len, snr):\n",
    "#         s_STMS, d_STMS, x_STMS, n_frames = self.mix(s, d, s_len, d_len, snr)\n",
    "#         mask = tf.expand_dims(tf.cast(tf.sequence_mask(n_frames), tf.float32), 2)\n",
    "#         xi_bar = tf.multiply(self.xi_bar(s_STMS, d_STMS), mask)\n",
    "#         return x_STMS, xi_bar, n_frames\n",
    "\n",
    "#     def mix(self, s, d, s_len, d_len, snr):\n",
    "#         s, d = self.normalise(s), self.normalise(d)\n",
    "#         n_frames = self.n_frames(s_len)\n",
    "#         (x, s, d) = self.add_noise_batch(s, d, s_len, d_len, snr)\n",
    "#         s_STMS, _ = self.polar_analysis(s)\n",
    "#         d_STMS, _ = self.polar_analysis(d)\n",
    "#         x_STMS, _ = self.polar_analysis(x)\n",
    "#         return s_STMS, d_STMS, x_STMS, n_frames\n",
    "\n",
    "    def normalise(self, x):\n",
    "        return tf.truediv(tf.cast(x, tf.float32), 32768.0)\n",
    "\n",
    "    def n_frames(self, N):\n",
    "        return tf.cast(tf.math.ceil(tf.truediv(tf.cast(N, tf.float32), tf.cast(self.N_s, tf.float32))), tf.int32)\n",
    "\n",
    "#     def add_noise_batch(self, s, d, s_len, d_len, snr):\n",
    "#         return tf.map_fn(lambda z: self.add_noise_pad(z[0], z[1], z[2], z[3], z[4],\n",
    "#             tf.reduce_max(s_len)), (s, d, s_len, d_len, snr), dtype=(tf.float32, tf.float32,\n",
    "#             tf.float32), back_prop=False)\n",
    "\n",
    "#     def add_noise_pad(self, s, d, s_len, d_len, snr, pad_len):\n",
    "#         s, d = s[:s_len], d[:d_len]\n",
    "#         (x, d) = self.add_noise(s, d, s_len, d_len, snr)\n",
    "#         total_zeros = tf.subtract(pad_len, s_len)\n",
    "#         x = tf.pad(x, [[0, total_zeros]], \"CONSTANT\")\n",
    "#         s = tf.pad(s, [[0, total_zeros]], \"CONSTANT\")\n",
    "#         d = tf.pad(d, [[0, total_zeros]], \"CONSTANT\")\n",
    "#         return (x, s, d)\n",
    "\n",
    "#     def add_noise(self, s, d, s_len, d_len, snr):\n",
    "#         snr = tf.cast(snr, tf.float32)\n",
    "#         snr = tf.pow(self.ten, tf.truediv(snr, self.ten)) # inverse of dB.\n",
    "#         i = tf.random.uniform([1], 0, tf.add(1, tf.subtract(d_len, s_len)), tf.int32)\n",
    "#         d = tf.slice(d, [i[0]], [s_len])\n",
    "#         P_s = tf.reduce_mean(tf.math.square(s), 0) # average power of clean speech.\n",
    "#         P_d = tf.reduce_mean(tf.math.square(d), 0) # average power of noise.\n",
    "#         alpha = tf.math.sqrt(tf.truediv(P_s,\n",
    "#             tf.maximum(tf.multiply(P_d, snr), 1e-12))) # scaling factor.\n",
    "#         d =\ttf.multiply(d, alpha)\n",
    "#         x = tf.add(s, d)\n",
    "#         return (x, d)\n",
    "\n",
    "#     def snr_db(self, s, d):\n",
    "#         P_s = tf.reduce_mean(tf.math.square(s), 0) # average power of clean speech.\n",
    "#         P_d = tf.reduce_mean(tf.math.square(d), 0) # average power of noise.\n",
    "#         return tf.multiply(self.ten, self.log_10(tf.truediv(P_s, P_d)))\n",
    "\n",
    "    def log_10(self, x):\n",
    "        return tf.truediv(tf.math.log(x), tf.math.log(self.ten))\n",
    "\n",
    "    def xi(self, s_STMS, d_STMS):\n",
    "        return tf.truediv(tf.square(s_STMS), tf.maximum(tf.square(d_STMS), 1e-12))\n",
    "\n",
    "    def xi_db(self, s_STMS, d_STMS):\n",
    "        return tf.multiply(10.0, self.log_10(tf.maximum(self.xi(s_STMS, d_STMS), 1e-12)))\n",
    "\n",
    "    def xi_bar(self, s_STMS, d_STMS):\n",
    "        return tf.multiply(0.5, tf.add(1.0, tf.math.erf(tf.truediv(tf.subtract(self.xi_db(s_STMS, d_STMS), self.mu),\n",
    "            tf.multiply(self.sigma, tf.sqrt(2.0))))))\n",
    "\n",
    "    def xi_hat(self, xi_bar_hat):\n",
    "        xi_db_hat = np.add(np.multiply(np.multiply(self.sigma, np.sqrt(2.0)),\n",
    "            spsp.erfinv(np.subtract(np.multiply(2.0, xi_bar_hat), 1))), self.mu)\n",
    "        return np.power(10.0, np.divide(xi_db_hat, 10.0))\n",
    "\n",
    "#     def mel_filter_bank(self, M):\n",
    "#         f_l = 0 # lowest frequency (Hz).\n",
    "#         f_h = self.f_s/2 # highest frequency (Hz).\n",
    "#         K = self.NFFT//2 + 1 # number of frequency bins.\n",
    "#         H = np.zeros([M, K], dtype=np.float32) # mel filter bank.\n",
    "#         for m in range(1, M + 1):\n",
    "#             bl = self.bpoint(m - 1, M, f_l, f_h) # lower boundary point, f(m - 1) for m-th filterbank.\n",
    "#             c = self.bpoint(m, M, f_l, f_h) # m-th filterbank centre point, f(m).\n",
    "#             bh = self.bpoint(m + 1, M, f_l, f_h) # higher boundary point f(m + 1) for m-th filterbank.\n",
    "#             for k in range(K):\n",
    "#                 if k >= bl and k <= c:\n",
    "#                     H[m-1,k] = (2*(k - bl))/((bh - bl)*(c - bl)) # m-th filterbank up-slope.\n",
    "#                 if k >= c and k <= bh:\n",
    "#                     H[m-1,k] = (2*(bh - k))/((bh - bl)*(bh - c)) # m-th filterbank down-slope.\n",
    "#         return H\n",
    "\n",
    "#     def bpoint(self, m, M, f_l, f_h):\n",
    "#         K = self.NFFT//2 + 1 # number of frequency bins.\n",
    "#         return ((2*K)/self.f_s)*self.mel2hz(self.hz2mel(f_l) + \\\n",
    "#             m*((self.hz2mel(f_h) - self.hz2mel(f_l))/(M + 1))) # boundary point.\n",
    "\n",
    "#     def hz2mel(self, f):\n",
    "#         return 2595*np.log10(1 + (f/700))\n",
    "\n",
    "#     def mel2hz(self, m):\n",
    "#         return 700*((10**(m/2595)) - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qFxVREOa8D9x"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Activation, Add, Conv1D, Conv2D, Dense, Dropout,Flatten, LayerNormalization, MaxPooling2D, ReLU\n",
    "import numpy as np\n",
    "\n",
    "class ResNet:\n",
    "    def __init__(self,inp,n_outp,n_blocks,d_model,d_f,k,max_d_rate,padding,):\n",
    "        self.d_model = d_model\n",
    "        self.d_f = d_f\n",
    "        self.k = k\n",
    "        self.n_outp = n_outp\n",
    "        self.padding = padding\n",
    "        self.first_layer = self.feedforward(inp)\n",
    "        self.layer_list = [self.first_layer]\n",
    "        for i in range(n_blocks): self.layer_list.append(self.block(self.layer_list[-1], int(2**(i%(np.log2(max_d_rate)+1)))))\n",
    "        self.logits = Conv1D(self.n_outp, 1, dilation_rate=1, use_bias=True)(self.layer_list[-1])\n",
    "        self.outp = Activation('sigmoid')(self.logits)\n",
    "\n",
    "    def feedforward(self, inp):\n",
    "        ff = Conv1D(self.d_model, 1, dilation_rate=1, use_bias=False)(inp)\n",
    "        norm = LayerNormalization(axis=2, epsilon=1e-6)(ff)\n",
    "        act = ReLU()(norm)\n",
    "        return act\n",
    "\n",
    "    def block(self, inp, d_rate):\n",
    "        self.conv_1 = self.unit(inp, self.d_f, 1, 1, False)\n",
    "        self.conv_2 = self.unit(self.conv_1, self.d_f, self.k, d_rate,\n",
    "            False)\n",
    "        self.conv_3 = self.unit(self.conv_2, self.d_model, 1, 1, True)\n",
    "        residual = Add()([inp, self.conv_3])\n",
    "        return residual\n",
    "\n",
    "    def unit(self, inp, n_filt, k, d_rate, use_bias):\n",
    "        norm = LayerNormalization(axis=2, epsilon=1e-6)(inp)\n",
    "        act = ReLU()(norm)\n",
    "        conv = Conv1D(n_filt, k, padding=self.padding, dilation_rate=d_rate,\n",
    "            use_bias=use_bias)(act)\n",
    "        return conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JSNW4_Zg8thn"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.special import exp1, i0, i1\n",
    "\n",
    "def mmse_lsa(xi, gamma):\n",
    "    nu = np.multiply(np.divide(xi, np.add(1, xi)), gamma)\n",
    "    return np.multiply(np.divide(xi, np.add(1, xi)), np.exp(np.multiply(0.5, exp1(nu)))) # MMSE-LSA gain function.\n",
    "\n",
    "def gfunc(xi, gamma=None, gtype='mmse-lsa'):\n",
    "    G = mmse_lsa(xi, gamma)\n",
    "    return G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-orlwtdU5N0E"
   },
   "outputs": [],
   "source": [
    "from pesq import pesq\n",
    "from pystoi import stoi\n",
    "from tensorflow.keras.callbacks import Callback, CSVLogger, ModelCheckpoint\n",
    "from tensorflow.keras.layers import Input, Masking\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.schedules import LearningRateSchedule\n",
    "from tensorflow.python.lib.io import file_io\n",
    "from tqdm import tqdm\n",
    "import csv, math, os, random \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "class DeepXi(DeepXiInput):\n",
    "    def __init__(self,N_d,N_s,NFFT,f_s,min_snr,max_snr,snr_inter,ver=\"resnet-1.0c\",**kwargs):\n",
    "        super().__init__(N_d, N_s, NFFT, f_s)\n",
    "        self.min_snr = min_snr\n",
    "        self.max_snr = max_snr\n",
    "        self.ver=ver\n",
    "        self.snr_levels = list(range(self.min_snr, self.max_snr + 1, snr_inter))\n",
    "        self.n_feat = math.ceil(self.NFFT/2 + 1)\n",
    "        self.n_outp = self.n_feat\n",
    "        self.inp = Input(name='inp', shape=[None, self.n_feat], dtype='float32')\n",
    "        self.mask = Masking(mask_value=0.0)(self.inp)\n",
    "        self.network = ResNet(\n",
    "            inp=self.mask,\n",
    "            n_outp=self.n_outp,\n",
    "            n_blocks=kwargs['n_blocks'],\n",
    "            d_model=kwargs['d_model'],\n",
    "            d_f=kwargs['d_f'],\n",
    "            k=kwargs['k'],\n",
    "            max_d_rate=kwargs['max_d_rate'],\n",
    "            padding=kwargs['padding'],\n",
    "            )\n",
    "        self.model = Model(inputs=self.inp, outputs=self.network.outp)\n",
    "        self.model.summary()\n",
    "        if not os.path.exists(\"log/summary\"):\n",
    "            os.makedirs(\"log/summary\")\n",
    "        with open(\"log/summary/\"+ self.ver + \".txt\", \"w\") as f:\n",
    "            self.model.summary(print_fn=lambda x: f.write(x + '\\n'))\n",
    "\n",
    "\n",
    "    def infer(self,test_x,test_x_len,test_x_base_names,model_path='model/saved_model/',out_type='y',gain='mmse-lsa',out_path='out/denoised/',stats_path=None,n_filters=40,):\n",
    "#         if out_type == 'xi_hat': out_path = out_path + '/xi_hat'\n",
    "#         elif out_type == 'y': \n",
    "        out_path = out_path + 'y/' + gain\n",
    "#         elif out_type == 'deepmmse': out_path = out_path + '/deepmmse'\n",
    "#         elif out_type == 'ibm_hat': out_path = out_path + '/ibm_hat'\n",
    "#         elif out_type == 'subband_ibm_hat': out_path = out_path + '/subband_ibm_hat'\n",
    "#         else: raise ValueError('Invalid output type.')\n",
    "#         if not os.path.exists(out_path): os.makedirs(out_path)\n",
    "\n",
    "#         if out_type == 'subband_ibm_hat':\n",
    "#             mel_filter_bank = self.mel_filter_bank(n_filters)\n",
    "\n",
    "        self.sample_stats(stats_path)\n",
    "        self.model.load_weights(model_path +\n",
    "            'variables/variables' )\n",
    "\n",
    "        print(\"Processing observations...\")\n",
    "        x_STMS_batch, x_STPS_batch, n_frames = self.observation_batch(test_x, test_x_len)\n",
    "        print(\"Performing inference...\")\n",
    "        xi_bar_hat_batch = self.model.predict(x_STMS_batch, batch_size=1, verbose=1)\n",
    "\n",
    "        print(\"Performing synthesis...\")\n",
    "        batch_size = len(test_x_len)\n",
    "        for i in tqdm(range(batch_size)):\n",
    "            base_name = test_x_base_names[i]\n",
    "            x_STMS = x_STMS_batch[i,:n_frames[i],:]\n",
    "            x_STPS = x_STPS_batch[i,:n_frames[i],:]\n",
    "            xi_bar_hat = xi_bar_hat_batch[i,:n_frames[i],:]\n",
    "            xi_hat = self.xi_hat(xi_bar_hat)\n",
    "#             if out_type == 'xi_hat': save_mat(args.out_path + '/' + base_name + '.mat',\n",
    "#                 xi_hat, 'xi_hat')\n",
    "#             elif out_type == 'y':\n",
    "            y_STMS = np.multiply(x_STMS, gfunc(xi_hat, xi_hat+1, gtype=gain))\n",
    "            y = self.polar_synthesis(y_STMS, x_STPS).numpy()\n",
    "            save_wav(out_path + '/'+ base_name + '.wav', y, self.f_s)\n",
    "#             elif out_type == 'deepmmse':\n",
    "#                 d_PSD_hat = np.multiply(np.square(x_STMS), gfunc(xi_hat, xi_hat+1,\n",
    "#                     gtype='deepmmse'))\n",
    "#                 save_mat(out_path + '/' + base_name + '.mat', d_PSD_hat, 'd_psd_hat')\n",
    "#             elif out_type == 'ibm_hat':\n",
    "#                 ibm_hat = np.greater(xi_hat, 1.0).astype(bool)\n",
    "#                 save_mat(out_path + '/' + base_name + '.mat', ibm_hat, 'ibm_hat')\n",
    "#             elif out_type == 'subband_ibm_hat':\n",
    "#                 xi_hat_subband = np.matmul(xi_hat, mel_filter_bank.transpose())\n",
    "#                 subband_ibm_hat = np.greater(xi_hat_subband, 1.0).astype(bool)\n",
    "#                 save_mat(out_path + '/' + base_name + '.mat', subband_ibm_hat,\n",
    "#                     'subband_ibm_hat')\n",
    "#             else: raise ValueError('Invalid output type.')\n",
    "\n",
    "    def sample_stats(self,stats_path='data',sample_size=1000,train_s_list=None,train_d_list=None):\n",
    "        if os.path.exists(stats_path + '/stats.npz'):\n",
    "            print('Loading sample statistics...')\n",
    "            with np.load(stats_path + '/stats.npz') as stats:\n",
    "                self.mu = stats['mu_hat']\n",
    "                self.sigma = stats['sigma_hat']\n",
    "        elif train_s_list == None:\n",
    "            raise ValueError('No stats.npz file exists. data/stats.p is available here: https://github.com/anicolson/DeepXi/blob/master/data/stats.npz.')\n",
    "        else:\n",
    "            print('Finding sample statistics...')\n",
    "            s_sample_list = random.sample(self.train_s_list, sample_size)\n",
    "            d_sample_list = random.sample(self.train_d_list, sample_size)\n",
    "            s_sample, d_sample, s_sample_len, d_sample_len, snr_sample = self.wav_batch(s_sample_list, d_sample_list)\n",
    "            snr_sample = np.array(random.choices(self.snr_levels, k=sample_size))\n",
    "            samples = []\n",
    "            for i in tqdm(range(s_sample.shape[0])):\n",
    "                s_STMS, d_STMS, _, _ = self.mix(s_sample[i:i+1], d_sample[i:i+1], s_sample_len[i:i+1],\n",
    "                    d_sample_len[i:i+1], snr_sample[i:i+1])\n",
    "                xi_db = self.xi_db(s_STMS, d_STMS) # instantaneous a priori SNR (dB).\n",
    "                samples.append(np.squeeze(xi_db.numpy()))\n",
    "            samples = np.vstack(samples)\n",
    "            if len(samples.shape) != 2: raise ValueError('Incorrect shape for sample.')\n",
    "            stats = {'mu_hat': np.mean(samples, axis=0), 'sigma_hat': np.std(samples, axis=0)}\n",
    "            self.mu, self.sigma = stats['mu_hat'], stats['sigma_hat']\n",
    "            if not os.path.exists(stats_path): os.makedirs(stats_path)\n",
    "            np.savez(stats_path + '/stats.npz', mu_hat=stats['mu_hat'], sigma_hat=stats['sigma_hat'])\n",
    "            save_mat(stats_path + '/stats.mat', stats, 'stats')\n",
    "            print('Sample statistics saved.')\n",
    "\n",
    "\n",
    "    def observation_batch(self, x_batch, x_batch_len):\n",
    "        batch_size = len(x_batch)\n",
    "        max_n_frames = self.n_frames(max(x_batch_len))\n",
    "        x_STMS_batch = np.zeros([batch_size, max_n_frames, self.n_feat], np.float32)\n",
    "        x_STPS_batch = np.zeros([batch_size, max_n_frames, self.n_feat], np.float32)\n",
    "        n_frames_batch = [self.n_frames(i) for i in x_batch_len]\n",
    "        for i in tqdm(range(batch_size)):\n",
    "            x_STMS, x_STPS = self.observation(x_batch[i,:x_batch_len[i]])\n",
    "            x_STMS_batch[i,:n_frames_batch[i],:] = x_STMS.numpy()\n",
    "            x_STPS_batch[i,:n_frames_batch[i],:] = x_STPS.numpy()\n",
    "        return x_STMS_batch, x_STPS_batch, n_frames_batch\n",
    "\n",
    "#     def wav_batch(self, s_list, d_list):\n",
    "#         batch_size = len(s_list)\n",
    "#         max_len = max([dic['wav_len'] for dic in s_list])\n",
    "#         s_batch = np.zeros([batch_size, max_len], np.int16)\n",
    "#         d_batch = np.zeros([batch_size, max_len], np.int16)\n",
    "#         s_batch_len = np.zeros(batch_size, np.int32)\n",
    "#         for i in range(batch_size):\n",
    "#             (wav, _) = read_wav(s_list[i]['file_path'])\n",
    "#             s_batch[i,:s_list[i]['wav_len']] = wav\n",
    "#             s_batch_len[i] = s_list[i]['wav_len']\n",
    "#             flag = True\n",
    "#             while flag:\n",
    "#                 if d_list[i]['wav_len'] < s_batch_len[i]: d_list[i] = random.choice(self.train_d_list)\n",
    "#                 else: flag = False\n",
    "#             (wav, _) = read_wav(d_list[i]['file_path'])\n",
    "#             rand_idx = np.random.randint(0, 1+d_list[i]['wav_len']-s_batch_len[i])\n",
    "#             d_batch[i,:s_batch_len[i]] = wav[rand_idx:rand_idx+s_batch_len[i]]\n",
    "#         d_batch_len = s_batch_len\n",
    "#         # snr_batch = np.random.randint(self.min_snr, self.max_snr+1, batch_size)\n",
    "#         snr_batch = np.array(random.choices(self.snr_levels, k=batch_size))\n",
    "#         return s_batch, d_batch, s_batch_len, d_batch_len, snr_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Qzl2t8lGlHr"
   },
   "outputs": [],
   "source": [
    "# from tensorflow.keras.layers import Input\n",
    "# from tensorflow.keras.models import Model\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# import math\n",
    "# import numpy as np\n",
    "# import tensorflow as tf\n",
    "\n",
    "# class Prelim():\n",
    "#     def __init__(self,n_feat,network):\n",
    "#         self.n_feat = n_feat\n",
    "#         self.n_outp = self.n_feat\n",
    "#         if self.n_feat < 5: raise ValueError('More input features are required for this exampple.')\n",
    "#         self.inp = Input(name='inp', shape=[None, self.n_feat], dtype='float32')\n",
    "#         self.mask = tf.keras.layers.Masking(mask_value=0.0)(self.inp)\n",
    "#         if network == 'ResNet': self.network = ResNet(self.mask, self.n_outp, B=40, d_model=256, d_f=64, k=3, max_d_rate=16)\n",
    "#         elif network == 'ResLSTM': self.network = ResLSTM(self.mask, self.n_outp, n_blocks=3, d_model=256)\n",
    "#         else: raise ValueError('Invalid network type.')\n",
    "#         self.model = Model(inputs=self.inp, outputs=self.network.outp)\n",
    "#         self.model.summary()\n",
    "\n",
    "#     def dataset(self, buffer_size=16):\n",
    "#         dataset = tf.data.Dataset.from_generator(\n",
    "#             self.mbatch_gen,\n",
    "#             (tf.float32, tf.float32, tf.float32),\n",
    "#             (tf.TensorShape([None, None, self.n_feat]),\n",
    "#                 tf.TensorShape([None, None, self.n_outp]),\n",
    "#                 tf.TensorShape([None, None]))\n",
    "#             )\n",
    "#         dataset = dataset.prefetch(buffer_size)\n",
    "#         return dataset\n",
    "\n",
    "#     def mbatch_gen(self):\n",
    "#         for _ in range(self.max_epochs):\n",
    "#             for _ in range(math.ceil(self.batch_size/self.mbatch_size)):\n",
    "#                 max_seq_len = 75\n",
    "#                 min_seq_len = 45\n",
    "#                 x_train = np.random.rand(self.mbatch_size, max_seq_len, self.n_feat)\n",
    "#                 y_frame = np.zeros(self.n_feat)\n",
    "#                 y_frame[0] = 0.05\n",
    "#                 y_frame[1] = 0.99\n",
    "#                 y_frame[2] = 0.5\n",
    "#                 y_frame[3] = 0.01\n",
    "#                 y_frame[4] = 0.75\n",
    "#                 y_train = np.tile(y_frame, (self.mbatch_size, max_seq_len, 1))\n",
    "#                 seq_len = np.random.randint(min_seq_len, max_seq_len+1, self.mbatch_size)\n",
    "#                 seq_mask = tf.cast(tf.sequence_mask(seq_len, maxlen=max_seq_len), tf.float32)\n",
    "#                 x_train = tf.multiply(x_train, tf.expand_dims(seq_mask, 2))\n",
    "#                 y_train = tf.multiply(y_train, tf.expand_dims(seq_mask, 2))\n",
    "#                 yield x_train, y_train, seq_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "MJu6zOOJ-Wwf",
    "outputId": "0e400343-ff08-4138-a9e1-b7b1f05baaf6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "inp (InputLayer)                [(None, None, 257)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "masking (Masking)               (None, None, 257)    0           inp[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, None, 256)    65792       masking[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (None, None, 256)    512         conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu (ReLU)                    (None, None, 256)    0           layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_1 (LayerNor (None, None, 256)    512         re_lu[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_1 (ReLU)                  (None, None, 256)    0           layer_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, None, 64)     16384       re_lu_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_2 (LayerNor (None, None, 64)     128         conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_2 (ReLU)                  (None, None, 64)     0           layer_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, None, 64)     12288       re_lu_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, None, 64)     128         conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_3 (ReLU)                  (None, None, 64)     0           layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, None, 256)    16640       re_lu_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, None, 256)    0           re_lu[0][0]                      \n",
      "                                                                 conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, None, 256)    512         add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_4 (ReLU)                  (None, None, 256)    0           layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, None, 64)     16384       re_lu_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, None, 64)     128         conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_5 (ReLU)                  (None, None, 64)     0           layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, None, 64)     12288       re_lu_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, None, 64)     128         conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_6 (ReLU)                  (None, None, 64)     0           layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, None, 256)    16640       re_lu_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, None, 256)    0           add[0][0]                        \n",
      "                                                                 conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, None, 256)    512         add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_7 (ReLU)                  (None, None, 256)    0           layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, None, 64)     16384       re_lu_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (None, None, 64)     128         conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_8 (ReLU)                  (None, None, 64)     0           layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, None, 64)     12288       re_lu_8[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (None, None, 64)     128         conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_9 (ReLU)                  (None, None, 64)     0           layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, None, 256)    16640       re_lu_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, None, 256)    0           add_1[0][0]                      \n",
      "                                                                 conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, None, 256)    512         add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_10 (ReLU)                 (None, None, 256)    0           layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, None, 64)     16384       re_lu_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_11 (LayerNo (None, None, 64)     128         conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_11 (ReLU)                 (None, None, 64)     0           layer_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, None, 64)     12288       re_lu_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_12 (LayerNo (None, None, 64)     128         conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_12 (ReLU)                 (None, None, 64)     0           layer_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, None, 256)    16640       re_lu_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, None, 256)    0           add_2[0][0]                      \n",
      "                                                                 conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_13 (LayerNo (None, None, 256)    512         add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_13 (ReLU)                 (None, None, 256)    0           layer_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, None, 64)     16384       re_lu_13[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_14 (LayerNo (None, None, 64)     128         conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_14 (ReLU)                 (None, None, 64)     0           layer_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, None, 64)     12288       re_lu_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_15 (LayerNo (None, None, 64)     128         conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_15 (ReLU)                 (None, None, 64)     0           layer_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, None, 256)    16640       re_lu_15[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, None, 256)    0           add_3[0][0]                      \n",
      "                                                                 conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_16 (LayerNo (None, None, 256)    512         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_16 (ReLU)                 (None, None, 256)    0           layer_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, None, 64)     16384       re_lu_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_17 (LayerNo (None, None, 64)     128         conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_17 (ReLU)                 (None, None, 64)     0           layer_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, None, 64)     12288       re_lu_17[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_18 (LayerNo (None, None, 64)     128         conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_18 (ReLU)                 (None, None, 64)     0           layer_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_18 (Conv1D)              (None, None, 256)    16640       re_lu_18[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, None, 256)    0           add_4[0][0]                      \n",
      "                                                                 conv1d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_19 (LayerNo (None, None, 256)    512         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_19 (ReLU)                 (None, None, 256)    0           layer_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_19 (Conv1D)              (None, None, 64)     16384       re_lu_19[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_20 (LayerNo (None, None, 64)     128         conv1d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_20 (ReLU)                 (None, None, 64)     0           layer_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_20 (Conv1D)              (None, None, 64)     12288       re_lu_20[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_21 (LayerNo (None, None, 64)     128         conv1d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_21 (ReLU)                 (None, None, 64)     0           layer_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_21 (Conv1D)              (None, None, 256)    16640       re_lu_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, None, 256)    0           add_5[0][0]                      \n",
      "                                                                 conv1d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_22 (LayerNo (None, None, 256)    512         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_22 (ReLU)                 (None, None, 256)    0           layer_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_22 (Conv1D)              (None, None, 64)     16384       re_lu_22[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_23 (LayerNo (None, None, 64)     128         conv1d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_23 (ReLU)                 (None, None, 64)     0           layer_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_23 (Conv1D)              (None, None, 64)     12288       re_lu_23[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_24 (LayerNo (None, None, 64)     128         conv1d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_24 (ReLU)                 (None, None, 64)     0           layer_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_24 (Conv1D)              (None, None, 256)    16640       re_lu_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, None, 256)    0           add_6[0][0]                      \n",
      "                                                                 conv1d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_25 (LayerNo (None, None, 256)    512         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_25 (ReLU)                 (None, None, 256)    0           layer_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_25 (Conv1D)              (None, None, 64)     16384       re_lu_25[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_26 (LayerNo (None, None, 64)     128         conv1d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_26 (ReLU)                 (None, None, 64)     0           layer_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_26 (Conv1D)              (None, None, 64)     12288       re_lu_26[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_27 (LayerNo (None, None, 64)     128         conv1d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_27 (ReLU)                 (None, None, 64)     0           layer_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_27 (Conv1D)              (None, None, 256)    16640       re_lu_27[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, None, 256)    0           add_7[0][0]                      \n",
      "                                                                 conv1d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_28 (LayerNo (None, None, 256)    512         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_28 (ReLU)                 (None, None, 256)    0           layer_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_28 (Conv1D)              (None, None, 64)     16384       re_lu_28[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_29 (LayerNo (None, None, 64)     128         conv1d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_29 (ReLU)                 (None, None, 64)     0           layer_normalization_29[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_29 (Conv1D)              (None, None, 64)     12288       re_lu_29[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_30 (LayerNo (None, None, 64)     128         conv1d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_30 (ReLU)                 (None, None, 64)     0           layer_normalization_30[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_30 (Conv1D)              (None, None, 256)    16640       re_lu_30[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, None, 256)    0           add_8[0][0]                      \n",
      "                                                                 conv1d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_31 (LayerNo (None, None, 256)    512         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_31 (ReLU)                 (None, None, 256)    0           layer_normalization_31[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_31 (Conv1D)              (None, None, 64)     16384       re_lu_31[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_32 (LayerNo (None, None, 64)     128         conv1d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_32 (ReLU)                 (None, None, 64)     0           layer_normalization_32[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_32 (Conv1D)              (None, None, 64)     12288       re_lu_32[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_33 (LayerNo (None, None, 64)     128         conv1d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_33 (ReLU)                 (None, None, 64)     0           layer_normalization_33[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_33 (Conv1D)              (None, None, 256)    16640       re_lu_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, None, 256)    0           add_9[0][0]                      \n",
      "                                                                 conv1d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_34 (LayerNo (None, None, 256)    512         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_34 (ReLU)                 (None, None, 256)    0           layer_normalization_34[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_34 (Conv1D)              (None, None, 64)     16384       re_lu_34[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_35 (LayerNo (None, None, 64)     128         conv1d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_35 (ReLU)                 (None, None, 64)     0           layer_normalization_35[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_35 (Conv1D)              (None, None, 64)     12288       re_lu_35[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_36 (LayerNo (None, None, 64)     128         conv1d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_36 (ReLU)                 (None, None, 64)     0           layer_normalization_36[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_36 (Conv1D)              (None, None, 256)    16640       re_lu_36[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, None, 256)    0           add_10[0][0]                     \n",
      "                                                                 conv1d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_37 (LayerNo (None, None, 256)    512         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_37 (ReLU)                 (None, None, 256)    0           layer_normalization_37[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_37 (Conv1D)              (None, None, 64)     16384       re_lu_37[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_38 (LayerNo (None, None, 64)     128         conv1d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_38 (ReLU)                 (None, None, 64)     0           layer_normalization_38[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_38 (Conv1D)              (None, None, 64)     12288       re_lu_38[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_39 (LayerNo (None, None, 64)     128         conv1d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_39 (ReLU)                 (None, None, 64)     0           layer_normalization_39[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_39 (Conv1D)              (None, None, 256)    16640       re_lu_39[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, None, 256)    0           add_11[0][0]                     \n",
      "                                                                 conv1d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_40 (LayerNo (None, None, 256)    512         add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_40 (ReLU)                 (None, None, 256)    0           layer_normalization_40[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_40 (Conv1D)              (None, None, 64)     16384       re_lu_40[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_41 (LayerNo (None, None, 64)     128         conv1d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_41 (ReLU)                 (None, None, 64)     0           layer_normalization_41[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_41 (Conv1D)              (None, None, 64)     12288       re_lu_41[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_42 (LayerNo (None, None, 64)     128         conv1d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_42 (ReLU)                 (None, None, 64)     0           layer_normalization_42[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_42 (Conv1D)              (None, None, 256)    16640       re_lu_42[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, None, 256)    0           add_12[0][0]                     \n",
      "                                                                 conv1d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_43 (LayerNo (None, None, 256)    512         add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_43 (ReLU)                 (None, None, 256)    0           layer_normalization_43[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_43 (Conv1D)              (None, None, 64)     16384       re_lu_43[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_44 (LayerNo (None, None, 64)     128         conv1d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_44 (ReLU)                 (None, None, 64)     0           layer_normalization_44[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_44 (Conv1D)              (None, None, 64)     12288       re_lu_44[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_45 (LayerNo (None, None, 64)     128         conv1d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_45 (ReLU)                 (None, None, 64)     0           layer_normalization_45[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_45 (Conv1D)              (None, None, 256)    16640       re_lu_45[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, None, 256)    0           add_13[0][0]                     \n",
      "                                                                 conv1d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_46 (LayerNo (None, None, 256)    512         add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_46 (ReLU)                 (None, None, 256)    0           layer_normalization_46[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_46 (Conv1D)              (None, None, 64)     16384       re_lu_46[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_47 (LayerNo (None, None, 64)     128         conv1d_46[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_47 (ReLU)                 (None, None, 64)     0           layer_normalization_47[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_47 (Conv1D)              (None, None, 64)     12288       re_lu_47[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_48 (LayerNo (None, None, 64)     128         conv1d_47[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_48 (ReLU)                 (None, None, 64)     0           layer_normalization_48[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_48 (Conv1D)              (None, None, 256)    16640       re_lu_48[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, None, 256)    0           add_14[0][0]                     \n",
      "                                                                 conv1d_48[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_49 (LayerNo (None, None, 256)    512         add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_49 (ReLU)                 (None, None, 256)    0           layer_normalization_49[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_49 (Conv1D)              (None, None, 64)     16384       re_lu_49[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_50 (LayerNo (None, None, 64)     128         conv1d_49[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_50 (ReLU)                 (None, None, 64)     0           layer_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_50 (Conv1D)              (None, None, 64)     12288       re_lu_50[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_51 (LayerNo (None, None, 64)     128         conv1d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_51 (ReLU)                 (None, None, 64)     0           layer_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_51 (Conv1D)              (None, None, 256)    16640       re_lu_51[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, None, 256)    0           add_15[0][0]                     \n",
      "                                                                 conv1d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_52 (LayerNo (None, None, 256)    512         add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_52 (ReLU)                 (None, None, 256)    0           layer_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_52 (Conv1D)              (None, None, 64)     16384       re_lu_52[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_53 (LayerNo (None, None, 64)     128         conv1d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_53 (ReLU)                 (None, None, 64)     0           layer_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_53 (Conv1D)              (None, None, 64)     12288       re_lu_53[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_54 (LayerNo (None, None, 64)     128         conv1d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_54 (ReLU)                 (None, None, 64)     0           layer_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_54 (Conv1D)              (None, None, 256)    16640       re_lu_54[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, None, 256)    0           add_16[0][0]                     \n",
      "                                                                 conv1d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_55 (LayerNo (None, None, 256)    512         add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_55 (ReLU)                 (None, None, 256)    0           layer_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_55 (Conv1D)              (None, None, 64)     16384       re_lu_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_56 (LayerNo (None, None, 64)     128         conv1d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_56 (ReLU)                 (None, None, 64)     0           layer_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_56 (Conv1D)              (None, None, 64)     12288       re_lu_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_57 (LayerNo (None, None, 64)     128         conv1d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_57 (ReLU)                 (None, None, 64)     0           layer_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_57 (Conv1D)              (None, None, 256)    16640       re_lu_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, None, 256)    0           add_17[0][0]                     \n",
      "                                                                 conv1d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_58 (LayerNo (None, None, 256)    512         add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_58 (ReLU)                 (None, None, 256)    0           layer_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_58 (Conv1D)              (None, None, 64)     16384       re_lu_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_59 (LayerNo (None, None, 64)     128         conv1d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_59 (ReLU)                 (None, None, 64)     0           layer_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_59 (Conv1D)              (None, None, 64)     12288       re_lu_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_60 (LayerNo (None, None, 64)     128         conv1d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_60 (ReLU)                 (None, None, 64)     0           layer_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, None, 256)    16640       re_lu_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, None, 256)    0           add_18[0][0]                     \n",
      "                                                                 conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_61 (LayerNo (None, None, 256)    512         add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_61 (ReLU)                 (None, None, 256)    0           layer_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, None, 64)     16384       re_lu_61[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_62 (LayerNo (None, None, 64)     128         conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_62 (ReLU)                 (None, None, 64)     0           layer_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, None, 64)     12288       re_lu_62[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_63 (LayerNo (None, None, 64)     128         conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_63 (ReLU)                 (None, None, 64)     0           layer_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_63 (Conv1D)              (None, None, 256)    16640       re_lu_63[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, None, 256)    0           add_19[0][0]                     \n",
      "                                                                 conv1d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_64 (LayerNo (None, None, 256)    512         add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_64 (ReLU)                 (None, None, 256)    0           layer_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_64 (Conv1D)              (None, None, 64)     16384       re_lu_64[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_65 (LayerNo (None, None, 64)     128         conv1d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_65 (ReLU)                 (None, None, 64)     0           layer_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_65 (Conv1D)              (None, None, 64)     12288       re_lu_65[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_66 (LayerNo (None, None, 64)     128         conv1d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_66 (ReLU)                 (None, None, 64)     0           layer_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_66 (Conv1D)              (None, None, 256)    16640       re_lu_66[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, None, 256)    0           add_20[0][0]                     \n",
      "                                                                 conv1d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_67 (LayerNo (None, None, 256)    512         add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_67 (ReLU)                 (None, None, 256)    0           layer_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_67 (Conv1D)              (None, None, 64)     16384       re_lu_67[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_68 (LayerNo (None, None, 64)     128         conv1d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_68 (ReLU)                 (None, None, 64)     0           layer_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_68 (Conv1D)              (None, None, 64)     12288       re_lu_68[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_69 (LayerNo (None, None, 64)     128         conv1d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_69 (ReLU)                 (None, None, 64)     0           layer_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_69 (Conv1D)              (None, None, 256)    16640       re_lu_69[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_22 (Add)                    (None, None, 256)    0           add_21[0][0]                     \n",
      "                                                                 conv1d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_70 (LayerNo (None, None, 256)    512         add_22[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_70 (ReLU)                 (None, None, 256)    0           layer_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_70 (Conv1D)              (None, None, 64)     16384       re_lu_70[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_71 (LayerNo (None, None, 64)     128         conv1d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_71 (ReLU)                 (None, None, 64)     0           layer_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_71 (Conv1D)              (None, None, 64)     12288       re_lu_71[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_72 (LayerNo (None, None, 64)     128         conv1d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_72 (ReLU)                 (None, None, 64)     0           layer_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_72 (Conv1D)              (None, None, 256)    16640       re_lu_72[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_23 (Add)                    (None, None, 256)    0           add_22[0][0]                     \n",
      "                                                                 conv1d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_73 (LayerNo (None, None, 256)    512         add_23[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_73 (ReLU)                 (None, None, 256)    0           layer_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_73 (Conv1D)              (None, None, 64)     16384       re_lu_73[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_74 (LayerNo (None, None, 64)     128         conv1d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_74 (ReLU)                 (None, None, 64)     0           layer_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_74 (Conv1D)              (None, None, 64)     12288       re_lu_74[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_75 (LayerNo (None, None, 64)     128         conv1d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_75 (ReLU)                 (None, None, 64)     0           layer_normalization_75[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_75 (Conv1D)              (None, None, 256)    16640       re_lu_75[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_24 (Add)                    (None, None, 256)    0           add_23[0][0]                     \n",
      "                                                                 conv1d_75[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_76 (LayerNo (None, None, 256)    512         add_24[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_76 (ReLU)                 (None, None, 256)    0           layer_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_76 (Conv1D)              (None, None, 64)     16384       re_lu_76[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_77 (LayerNo (None, None, 64)     128         conv1d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_77 (ReLU)                 (None, None, 64)     0           layer_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_77 (Conv1D)              (None, None, 64)     12288       re_lu_77[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_78 (LayerNo (None, None, 64)     128         conv1d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_78 (ReLU)                 (None, None, 64)     0           layer_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_78 (Conv1D)              (None, None, 256)    16640       re_lu_78[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_25 (Add)                    (None, None, 256)    0           add_24[0][0]                     \n",
      "                                                                 conv1d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_79 (LayerNo (None, None, 256)    512         add_25[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_79 (ReLU)                 (None, None, 256)    0           layer_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_79 (Conv1D)              (None, None, 64)     16384       re_lu_79[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_80 (LayerNo (None, None, 64)     128         conv1d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_80 (ReLU)                 (None, None, 64)     0           layer_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_80 (Conv1D)              (None, None, 64)     12288       re_lu_80[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_81 (LayerNo (None, None, 64)     128         conv1d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_81 (ReLU)                 (None, None, 64)     0           layer_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_81 (Conv1D)              (None, None, 256)    16640       re_lu_81[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_26 (Add)                    (None, None, 256)    0           add_25[0][0]                     \n",
      "                                                                 conv1d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_82 (LayerNo (None, None, 256)    512         add_26[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_82 (ReLU)                 (None, None, 256)    0           layer_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_82 (Conv1D)              (None, None, 64)     16384       re_lu_82[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_83 (LayerNo (None, None, 64)     128         conv1d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_83 (ReLU)                 (None, None, 64)     0           layer_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_83 (Conv1D)              (None, None, 64)     12288       re_lu_83[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_84 (LayerNo (None, None, 64)     128         conv1d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_84 (ReLU)                 (None, None, 64)     0           layer_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_84 (Conv1D)              (None, None, 256)    16640       re_lu_84[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_27 (Add)                    (None, None, 256)    0           add_26[0][0]                     \n",
      "                                                                 conv1d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_85 (LayerNo (None, None, 256)    512         add_27[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_85 (ReLU)                 (None, None, 256)    0           layer_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_85 (Conv1D)              (None, None, 64)     16384       re_lu_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_86 (LayerNo (None, None, 64)     128         conv1d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_86 (ReLU)                 (None, None, 64)     0           layer_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_86 (Conv1D)              (None, None, 64)     12288       re_lu_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_87 (LayerNo (None, None, 64)     128         conv1d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_87 (ReLU)                 (None, None, 64)     0           layer_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_87 (Conv1D)              (None, None, 256)    16640       re_lu_87[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_28 (Add)                    (None, None, 256)    0           add_27[0][0]                     \n",
      "                                                                 conv1d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_88 (LayerNo (None, None, 256)    512         add_28[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_88 (ReLU)                 (None, None, 256)    0           layer_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_88 (Conv1D)              (None, None, 64)     16384       re_lu_88[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_89 (LayerNo (None, None, 64)     128         conv1d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_89 (ReLU)                 (None, None, 64)     0           layer_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_89 (Conv1D)              (None, None, 64)     12288       re_lu_89[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_90 (LayerNo (None, None, 64)     128         conv1d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_90 (ReLU)                 (None, None, 64)     0           layer_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_90 (Conv1D)              (None, None, 256)    16640       re_lu_90[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_29 (Add)                    (None, None, 256)    0           add_28[0][0]                     \n",
      "                                                                 conv1d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_91 (LayerNo (None, None, 256)    512         add_29[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_91 (ReLU)                 (None, None, 256)    0           layer_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_91 (Conv1D)              (None, None, 64)     16384       re_lu_91[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_92 (LayerNo (None, None, 64)     128         conv1d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_92 (ReLU)                 (None, None, 64)     0           layer_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_92 (Conv1D)              (None, None, 64)     12288       re_lu_92[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_93 (LayerNo (None, None, 64)     128         conv1d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_93 (ReLU)                 (None, None, 64)     0           layer_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_93 (Conv1D)              (None, None, 256)    16640       re_lu_93[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_30 (Add)                    (None, None, 256)    0           add_29[0][0]                     \n",
      "                                                                 conv1d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_94 (LayerNo (None, None, 256)    512         add_30[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_94 (ReLU)                 (None, None, 256)    0           layer_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_94 (Conv1D)              (None, None, 64)     16384       re_lu_94[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_95 (LayerNo (None, None, 64)     128         conv1d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_95 (ReLU)                 (None, None, 64)     0           layer_normalization_95[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_95 (Conv1D)              (None, None, 64)     12288       re_lu_95[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_96 (LayerNo (None, None, 64)     128         conv1d_95[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_96 (ReLU)                 (None, None, 64)     0           layer_normalization_96[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_96 (Conv1D)              (None, None, 256)    16640       re_lu_96[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_31 (Add)                    (None, None, 256)    0           add_30[0][0]                     \n",
      "                                                                 conv1d_96[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_97 (LayerNo (None, None, 256)    512         add_31[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_97 (ReLU)                 (None, None, 256)    0           layer_normalization_97[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_97 (Conv1D)              (None, None, 64)     16384       re_lu_97[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_98 (LayerNo (None, None, 64)     128         conv1d_97[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_98 (ReLU)                 (None, None, 64)     0           layer_normalization_98[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_98 (Conv1D)              (None, None, 64)     12288       re_lu_98[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_99 (LayerNo (None, None, 64)     128         conv1d_98[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_99 (ReLU)                 (None, None, 64)     0           layer_normalization_99[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_99 (Conv1D)              (None, None, 256)    16640       re_lu_99[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_32 (Add)                    (None, None, 256)    0           add_31[0][0]                     \n",
      "                                                                 conv1d_99[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_100 (LayerN (None, None, 256)    512         add_32[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_100 (ReLU)                (None, None, 256)    0           layer_normalization_100[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_100 (Conv1D)             (None, None, 64)     16384       re_lu_100[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_101 (LayerN (None, None, 64)     128         conv1d_100[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_101 (ReLU)                (None, None, 64)     0           layer_normalization_101[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_101 (Conv1D)             (None, None, 64)     12288       re_lu_101[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_102 (LayerN (None, None, 64)     128         conv1d_101[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_102 (ReLU)                (None, None, 64)     0           layer_normalization_102[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_102 (Conv1D)             (None, None, 256)    16640       re_lu_102[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_33 (Add)                    (None, None, 256)    0           add_32[0][0]                     \n",
      "                                                                 conv1d_102[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_103 (LayerN (None, None, 256)    512         add_33[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_103 (ReLU)                (None, None, 256)    0           layer_normalization_103[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_103 (Conv1D)             (None, None, 64)     16384       re_lu_103[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_104 (LayerN (None, None, 64)     128         conv1d_103[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_104 (ReLU)                (None, None, 64)     0           layer_normalization_104[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_104 (Conv1D)             (None, None, 64)     12288       re_lu_104[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_105 (LayerN (None, None, 64)     128         conv1d_104[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_105 (ReLU)                (None, None, 64)     0           layer_normalization_105[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_105 (Conv1D)             (None, None, 256)    16640       re_lu_105[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_34 (Add)                    (None, None, 256)    0           add_33[0][0]                     \n",
      "                                                                 conv1d_105[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_106 (LayerN (None, None, 256)    512         add_34[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_106 (ReLU)                (None, None, 256)    0           layer_normalization_106[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_106 (Conv1D)             (None, None, 64)     16384       re_lu_106[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_107 (LayerN (None, None, 64)     128         conv1d_106[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_107 (ReLU)                (None, None, 64)     0           layer_normalization_107[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_107 (Conv1D)             (None, None, 64)     12288       re_lu_107[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_108 (LayerN (None, None, 64)     128         conv1d_107[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_108 (ReLU)                (None, None, 64)     0           layer_normalization_108[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_108 (Conv1D)             (None, None, 256)    16640       re_lu_108[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_35 (Add)                    (None, None, 256)    0           add_34[0][0]                     \n",
      "                                                                 conv1d_108[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_109 (LayerN (None, None, 256)    512         add_35[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_109 (ReLU)                (None, None, 256)    0           layer_normalization_109[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_109 (Conv1D)             (None, None, 64)     16384       re_lu_109[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_110 (LayerN (None, None, 64)     128         conv1d_109[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_110 (ReLU)                (None, None, 64)     0           layer_normalization_110[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_110 (Conv1D)             (None, None, 64)     12288       re_lu_110[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_111 (LayerN (None, None, 64)     128         conv1d_110[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_111 (ReLU)                (None, None, 64)     0           layer_normalization_111[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_111 (Conv1D)             (None, None, 256)    16640       re_lu_111[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_36 (Add)                    (None, None, 256)    0           add_35[0][0]                     \n",
      "                                                                 conv1d_111[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_112 (LayerN (None, None, 256)    512         add_36[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_112 (ReLU)                (None, None, 256)    0           layer_normalization_112[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_112 (Conv1D)             (None, None, 64)     16384       re_lu_112[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_113 (LayerN (None, None, 64)     128         conv1d_112[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_113 (ReLU)                (None, None, 64)     0           layer_normalization_113[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_113 (Conv1D)             (None, None, 64)     12288       re_lu_113[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_114 (LayerN (None, None, 64)     128         conv1d_113[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_114 (ReLU)                (None, None, 64)     0           layer_normalization_114[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_114 (Conv1D)             (None, None, 256)    16640       re_lu_114[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_37 (Add)                    (None, None, 256)    0           add_36[0][0]                     \n",
      "                                                                 conv1d_114[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_115 (LayerN (None, None, 256)    512         add_37[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_115 (ReLU)                (None, None, 256)    0           layer_normalization_115[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_115 (Conv1D)             (None, None, 64)     16384       re_lu_115[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_116 (LayerN (None, None, 64)     128         conv1d_115[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_116 (ReLU)                (None, None, 64)     0           layer_normalization_116[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_116 (Conv1D)             (None, None, 64)     12288       re_lu_116[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_117 (LayerN (None, None, 64)     128         conv1d_116[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_117 (ReLU)                (None, None, 64)     0           layer_normalization_117[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_117 (Conv1D)             (None, None, 256)    16640       re_lu_117[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_38 (Add)                    (None, None, 256)    0           add_37[0][0]                     \n",
      "                                                                 conv1d_117[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_118 (LayerN (None, None, 256)    512         add_38[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_118 (ReLU)                (None, None, 256)    0           layer_normalization_118[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_118 (Conv1D)             (None, None, 64)     16384       re_lu_118[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_119 (LayerN (None, None, 64)     128         conv1d_118[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_119 (ReLU)                (None, None, 64)     0           layer_normalization_119[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_119 (Conv1D)             (None, None, 64)     12288       re_lu_119[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_120 (LayerN (None, None, 64)     128         conv1d_119[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_120 (ReLU)                (None, None, 64)     0           layer_normalization_120[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_120 (Conv1D)             (None, None, 256)    16640       re_lu_120[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_39 (Add)                    (None, None, 256)    0           add_38[0][0]                     \n",
      "                                                                 conv1d_120[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_121 (Conv1D)             (None, None, 257)    66049       add_39[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, None, 257)    0           conv1d_121[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 1,975,553\n",
      "Trainable params: 1,975,553\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Loading sample statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:00<00:00, 22.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing observations...\n",
      "Performing inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 70ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing synthesis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 2/2 [00:00<00:00,  5.40it/s]\n"
     ]
    }
   ],
   "source": [
    "#VARIABLES FOR THE MODEL\n",
    "d_model  = 256\n",
    "n_blocks  = 40\n",
    "d_f = 64\n",
    "k =  3\n",
    "max_d_rate = 16 \n",
    "causal  =   1\n",
    "ver=\"resnet-1.0c\"\n",
    "f_s  = 16000 \n",
    "T_d  = 32 \n",
    "T_s  =  16 \n",
    "min_snr = -10 # CAN BE REMOVED\n",
    "max_snr =  20 # CAN BE REMOVED\n",
    "snr_inter = 1 # CAN BE REMOVED\n",
    "\n",
    "# VARIABLES FOR INFERENCE\n",
    "data_path='data'\n",
    "test_x_path='set/test_noisy_speech'\n",
    "out_path='out/denoised/'\n",
    "model_path='model/saved_model/'\n",
    "gain='mmse-lsa'\n",
    "\n",
    "\n",
    "\n",
    "if causal: padding = \"causal\"\n",
    "else: padding = \"same\"\n",
    "\n",
    "N_d = int(f_s*T_d*0.001) # window duration (samples).\n",
    "N_s = int(f_s*T_s*0.001) # window shift (samples).\n",
    "NFFT = int(pow(2, np.ceil(np.log2(N_d)))) # number of DFT components.\n",
    "\n",
    "test_x, test_x_len, _, test_x_base_names = Batch(test_x_path)\n",
    "\n",
    "deepxi = DeepXi(N_d=N_d,N_s=N_s,NFFT=NFFT,f_s=f_s,min_snr=min_snr,max_snr=max_snr,\n",
    "  snr_inter=snr_inter,d_model=d_model,n_blocks=n_blocks,d_f=d_f,k=k,max_d_rate =max_d_rate,padding=padding,\n",
    "  causal=causal,ver= ver)\n",
    "\n",
    "deepxi.infer(test_x=test_x,test_x_len=test_x_len,test_x_base_names=test_x_base_names,\n",
    "        model_path=model_path,gain=gain,out_path= out_path,stats_path= data_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removed in code:**\n",
    "\n",
    "- Removed the argument **'network_type'** from DeepXi class __init__ method. ( As we do not need ResNetLSTM network for out application)\n",
    "- Removed the class, **'class SaveWeights'** as it is redundant. (It was not used anywhere in our required code)\n",
    "- Removed the variables of the main cell, **'train_s_path','train_d_path','val_s_path','val_d_path'** as they are redundant.\n",
    "- Removed the variable of the main cell, **'set_path'** and correspinding if condition as they are redunant. ( used for above variables, which are also redundant)\n",
    "- Removed the variables of the main cell **'save_model','max_epochs','resume_epoch'** as they are redundant.\n",
    "- Removed the argument of deepxi.infer, **'test_epoch'**  and coresponding if condition as they are redundant.\n",
    "- Removed the methods of deepxi, **'dataset', 'mbatch_gen', 'val_batch','add_score', 'TransformerSchedular'** as they are redundant. ( used during training and validation of the model)\n",
    "- Removed the variables of main cell **'eval_example','log_iter','mbatch_size','sample_size'** as they are redundant.\n",
    "- The if statement : **'if infer or test:'** of main cell is removed as are anyway preparing the code for inference. So, the corresponding flag variable **'infer'** of the main cell is also removed.\n",
    "\n",
    "\n",
    "**Changed in code:**\n",
    "\n",
    "- Changed the path of output, **'out_path' as '/out/denoised/'**\n",
    "- Changed the value of argument **'model_path'** from DeepXi class __init__ method as 'model/saved_model/'. (As the older path had more recursive directories)\n",
    "\n",
    "**Can be Removed(Commented as of now):** [Can be removed after a discussion]\n",
    "\n",
    "- The variable **'out_type'** in deepxi.infer method. Can be removed as we are using only one out_type (**'y'**) for our inference currently.  \n",
    "- Commented (can be removed as well after discussion) the out_types : **deepmmse, xi_hat, ibm_hat, sub_band_ibm_hat** and corresponding if-else conditions from **deepxi.infer** method.   \n",
    "NOTE: ( We are using only one **out_type = 'y'** that outputs .wav file, other types are redundant as all the corresponding output files are matlab files, which i assume used for performance visulaization. I'm not sure if we need those for analysis. But, w.r.t inference, it is not required)  \n",
    "-The **'Class Prelim'** seems to be redundant.  \n",
    "\n",
    "- The deepxi method **'wav_batch'** can be removed if we are going to stick with only loading the **sample stats** file. Because it is only needed when we want to find **sample_stats** from scratch.\n",
    "- The variable **snr_levels,min_snr,max_snr,snr_inter** of deepxi class can be removed if we not going to perform **sample_stats**. \n",
    "- The gain - **'mmse-lsa'** is what we are using. So, we can remove the related variables.\n",
    "- The function **'save_mat'** can be removed if we not going to perform **sample_stats**. \n",
    "- In the **Class DeepXiInput**, The methods **'example','mix','add_noise_batch','add_noise_pad','add_noise','snr_db','mel_filter_bank','bpoint','hz2mel','mel2hz'** are redundant for the inference part.\n",
    "- In the main cell, **if condition** for **padding** can be removed, as there we it is sure that we are using **padding='causal'**.\n",
    "\n",
    "**No idea and need to clarify**\n",
    "\n",
    "- **'sample_stats'** method of deepxi.infer : I'm not sure of what it is. I could see it loads a **.npz file** from **data** directory. I beleive it is one of the file which we need as pre-requiste to infer the model. It is something that affects the variables depended on **'xi_hat'**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DeepXI_cleaned.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:deepxi]",
   "language": "python",
   "name": "conda-env-deepxi-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
