{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sroie_predict.ipynb","provenance":[],"authorship_tag":"ABX9TyPxjvGhSFyqY9xhv9o3jTWL"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"zumSftADoXyW","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1600775442270,"user_tz":-330,"elapsed":25965,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"c464841f-d860-4de0-86d5-405bed0b3255"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"hbN-4hQioet6","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":845},"executionInfo":{"status":"ok","timestamp":1600775449531,"user_tz":-330,"elapsed":8802,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"5ed90df2-0564-43b0-daa3-956c6cb01b12"},"source":["!pip install seqeval transformers==2.9.0 tensorboardX"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting seqeval\n","  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n","Collecting transformers==2.9.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n","\u001b[K     |████████████████████████████████| 645kB 11.7MB/s \n","\u001b[?25hCollecting tensorboardX\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n","\u001b[K     |████████████████████████████████| 317kB 21.1MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n","Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.4.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (2019.12.20)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 28.5MB/s \n","\u001b[?25hCollecting tokenizers==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 54.4MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (3.0.12)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 73.3MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (4.41.1)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (0.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (2.23.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.0) (0.16.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (2020.6.20)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.0)\n","Building wheels for collected packages: seqeval, sacremoses\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7423 sha256=aabd02838adde0934c07ecdd78f520cd391c9e3378b0549aa70bbcc7c3ab44c9\n","  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=66de107718624d454e9541ae5024b560c39c7b2ea6894c31f686f3012ffeff33\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built seqeval sacremoses\n","Installing collected packages: seqeval, sacremoses, tokenizers, sentencepiece, transformers, tensorboardX\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 seqeval-0.0.12 tensorboardX-2.1 tokenizers-0.7.0 transformers-2.9.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_tbogSyiogLa","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600775451780,"user_tz":-330,"elapsed":5239,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["import logging\n","\n","import torch\n","from torch import nn\n","from torch.nn import CrossEntropyLoss, MSELoss\n","from transformers import BertConfig, BertModel, BertPreTrainedModel\n","from transformers.modeling_bert import BertLayerNorm\n","\n","logger = logging.getLogger(__name__)\n","\n","LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP = {}\n","\n","LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP = {}\n","\n","\n","class LayoutlmConfig(BertConfig):\n","    pretrained_config_archive_map = LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP\n","    model_type = \"bert\"\n","\n","    def __init__(self, max_2d_position_embeddings=1024, **kwargs):\n","        super().__init__(**kwargs)\n","        self.max_2d_position_embeddings = max_2d_position_embeddings\n","\n","\n","\n","\n","class LayoutlmEmbeddings(nn.Module): \n","    def __init__(self, config):\n","        super(LayoutlmEmbeddings, self).__init__()\n","        self.word_embeddings = nn.Embedding(\n","            config.vocab_size, config.hidden_size, padding_idx=0\n","        )\n","        self.position_embeddings = nn.Embedding(\n","            config.max_position_embeddings, config.hidden_size\n","        )\n","        self.x_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.y_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.h_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.w_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.token_type_embeddings = nn.Embedding(\n","            config.type_vocab_size, config.hidden_size\n","        )\n","\n","        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n","        # any TensorFlow checkpoint file\n","        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        token_type_ids=None,\n","        position_ids=None,\n","        inputs_embeds=None,\n","    ):\n","        seq_length = input_ids.size(1)\n","        if position_ids is None:\n","            position_ids = torch.arange(\n","                seq_length, dtype=torch.long, device=input_ids.device\n","            )\n","            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        words_embeddings = self.word_embeddings(input_ids)\n","        position_embeddings = self.position_embeddings(position_ids)\n","        left_position_embeddings = self.x_position_embeddings(bbox[:, :, 0])\n","        upper_position_embeddings = self.y_position_embeddings(bbox[:, :, 1])\n","        right_position_embeddings = self.x_position_embeddings(bbox[:, :, 2])\n","        lower_position_embeddings = self.y_position_embeddings(bbox[:, :, 3])\n","        h_position_embeddings = self.h_position_embeddings(\n","            bbox[:, :, 3] - bbox[:, :, 1]\n","        )\n","       # print(\"\\n bbox shape :- \",bbox.size())\n","       # print(\"\\nbbox[:,:,2] :- \", bbox[:, :, 2])\n","       # print(\"\\nbbox[:,:,2] shape :- \", bbox[:, :, 2].shape)\n","       # print(\"\\nbbox[:, :, 0] :- \", bbox[:, :, 0])\n","       # print(\"\\nbbox[:, :, 0] shape :- \", bbox[:, :, 0].shape)\n","       # print(\"\\n sub of two above matrix :- \",bbox[:, :, 2] - bbox[:, :, 0])\n","       # print(\"\\n printing the w_position_embeddings:- \",self.w_position_embeddings)\n","        bboxshape=bbox[:, :, 2] - bbox[:, :, 0]\n","       # print(\"\\n shape of the resultant sub array :- \",bboxshape.size())\n","        w_position_embeddings = self.w_position_embeddings(\n","            bbox[:, :, 2] - bbox[:, :, 0]\n","        )\n","        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n","\n","        embeddings = (\n","            words_embeddings\n","            + position_embeddings\n","            + left_position_embeddings\n","            + upper_position_embeddings\n","            + right_position_embeddings\n","            + lower_position_embeddings\n","            + h_position_embeddings\n","            + w_position_embeddings\n","            + token_type_embeddings\n","        )\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","        return embeddings\n","\n","\n","class LayoutlmModel(BertModel):\n","\n","    config_class = LayoutlmConfig\n","    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, config):\n","        super(LayoutlmModel, self).__init__(config)\n","        self.embeddings = LayoutlmEmbeddings(config)\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","    ):\n","        if attention_mask is None:\n","            attention_mask = torch.ones_like(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        # We create a 3D attention mask from a 2D tensor mask.\n","        # Sizes are [batch_size, 1, 1, to_seq_length]\n","        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n","        # this attention mask is more simple than the triangular masking of causal attention\n","        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n","        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n","\n","        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n","        # masked positions, this operation will create a tensor which is 0.0 for\n","        # positions we want to attend and -10000.0 for masked positions.\n","        # Since we are adding it to the raw scores before the softmax, this is\n","        # effectively the same as removing these entirely.\n","        extended_attention_mask = extended_attention_mask.to(\n","            dtype=next(self.parameters()).dtype\n","        )  # fp16 compatibility\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","        # Prepare head mask if needed\n","        # 1.0 in head_mask indicate we keep the head\n","        # attention_probs has shape bsz x n_heads x N x N\n","        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n","        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n","        if head_mask is not None:\n","            if head_mask.dim() == 1:\n","                head_mask = (\n","                    head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n","                )\n","                head_mask = head_mask.expand(\n","                    self.config.num_hidden_layers, -1, -1, -1, -1\n","                )\n","            elif head_mask.dim() == 2:\n","                head_mask = (\n","                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n","                )  # We can specify head_mask for each layer\n","            head_mask = head_mask.to(\n","                dtype=next(self.parameters()).dtype\n","            )  # switch to fload if need + fp16 compatibility\n","        else:\n","            head_mask = [None] * self.config.num_hidden_layers\n","\n","        #print(\"\\n The arguments in the embedding layer :- \\n\\n\\n\")\n","        #print(\"\\n input_ids in embedding layer :- \",input_ids)\n","        #print(\"\\n position_ids in embedding layer :- \",position_ids)\n","        #print(\"\\n token_type_ids in embedding layer :- \",token_type_ids)\n","        #print(\"\\n input_ids in embedding layer :- \\n\\n\")\n","        embedding_output = self.embeddings(\n","            input_ids, bbox, position_ids=position_ids, token_type_ids=token_type_ids\n","        )\n","        encoder_outputs = self.encoder(\n","            embedding_output, extended_attention_mask, head_mask=head_mask\n","        )\n","        sequence_output = encoder_outputs[0]\n","        pooled_output = self.pooler(sequence_output)\n","\n","        outputs = (sequence_output, pooled_output) + encoder_outputs[\n","            1:\n","        ]  # add hidden_states and attentions if they are here\n","        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n","\n","\n","class LayoutlmForTokenClassification(BertPreTrainedModel):\n","    config_class = LayoutlmConfig\n","    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        #print('num_labels', 9)\n","        self.bert = LayoutlmModel(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        #print('model archi2', self.classifier())\n","\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","    ):\n","\n","        outputs = self.bert(\n","            input_ids=input_ids,\n","            bbox=bbox,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","        #print('model archi3', self.classifier())\n","        outputs = (logits,) + outputs[\n","            2:\n","        ]  # add hidden states and attention if they are here\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                print('check_label',self.num_labels)\n","                active_logits = logits.view(-1, self.num_labels)[active_loss]\n","                active_labels = labels.view(-1)[active_loss]\n","                loss = loss_fct(active_logits, active_labels)\n","            else:\n","                print('check_label2', self.num_labels)\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), scores, (hidden_states), (attentions)\n","\n","\n","class LayoutlmForSequenceClassification(BertPreTrainedModel):\n","    config_class = LayoutlmConfig\n","    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, config):\n","        super(LayoutlmForSequenceClassification, self).__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.bert = LayoutlmModel(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","      #\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels) #config.num_labels\n","        #print('model archi4', self.classifier())\n","\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","    ):\n","\n","        outputs = self.bert(\n","            input_ids=input_ids,\n","            bbox=bbox,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","        )\n","\n","        pooled_output = outputs[1]\n","\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        #print('model archi', self.classifier())\n","        outputs = (logits,) + outputs[\n","            2:\n","        ]  # add hidden states and attention if they are here\n","\n","        if labels is not None:\n","            if self.num_labels == 1:\n","                #  We are doing regression\n","                loss_fct = MSELoss()\n","                loss = loss_fct(logits.view(-1), labels.view(-1))\n","            else:\n","                loss_fct = CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"WVfBZEoAokm_","colab_type":"code","colab":{}},"source":["import logging\n","import os\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","class FunsdDataset(Dataset):\n","    def __init__(self, args, tokenizer, labels, pad_token_label_id, mode):\n","        if args.local_rank not in [-1, 0] and mode == \"train\":\n","            torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","\n","        # Load data features from cache or dataset file\n","        cached_features_file = os.path.join(\n","            args.data_dir,\n","            \"cached_{}_{}_{}\".format(\n","                mode,\n","                list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n","                str(args.max_seq_length),\n","            ),\n","        )\n","        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n","            logger.info(\"Loading features from cached file %s\", cached_features_file)\n","            features = torch.load(cached_features_file)\n","        else:\n","            logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n","            examples = read_examples_from_file(args.data_dir, mode)\n","            features = convert_examples_to_features(\n","                examples,\n","                labels,\n","                args.max_seq_length,\n","                tokenizer,\n","                cls_token_at_end=bool(args.model_type in [\"xlnet\"]),\n","                # xlnet has a cls token at the end\n","                cls_token=tokenizer.cls_token,\n","                cls_token_segment_id=2 if args.model_type in [\"xlnet\"] else 0,\n","                sep_token=tokenizer.sep_token,\n","                sep_token_extra=bool(args.model_type in [\"roberta\"]),\n","                # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n","                pad_on_left=bool(args.model_type in [\"xlnet\"]),\n","                # pad on the left for xlnet\n","                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n","                pad_token_segment_id=4 if args.model_type in [\"xlnet\"] else 0,\n","                pad_token_label_id=pad_token_label_id,\n","            )\n","            if args.local_rank in [-1, 0]:\n","                logger.info(\"Saving features into cached file %s\", cached_features_file)\n","                torch.save(features, cached_features_file)\n","\n","        if args.local_rank == 0 and mode == \"train\":\n","            torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","\n","        self.features = features\n","        # Convert to Tensors and build dataset\n","        self.all_input_ids = torch.tensor(\n","            [f.input_ids for f in features], dtype=torch.long\n","        )\n","        self.all_input_mask = torch.tensor(\n","            [f.input_mask for f in features], dtype=torch.long\n","        )\n","        self.all_segment_ids = torch.tensor(\n","            [f.segment_ids for f in features], dtype=torch.long\n","        )\n","        self.all_label_ids = torch.tensor(\n","            [f.label_ids for f in features], dtype=torch.long\n","        )\n","        self.all_bboxes = torch.tensor([f.boxes for f in features], dtype=torch.long)\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, index):\n","        return (\n","            self.all_input_ids[index],\n","            self.all_input_mask[index],\n","            self.all_segment_ids[index],\n","            self.all_label_ids[index],\n","            self.all_bboxes[index],\n","        )\n","\n","\n","class InputExample(object):\n","    \"\"\"A single training/test example for token classification.\"\"\"\n","\n","    def __init__(self, guid, words, labels, boxes, actual_bboxes, file_name, page_size):\n","        \"\"\"Constructs a InputExample.\n","\n","        Args:\n","            guid: Unique id for the example.\n","            words: list. The words of the sequence.\n","            labels: (Optional) list. The labels for each word of the sequence. This should be\n","            specified for train and dev examples, but not for test examples.\n","        \"\"\"\n","        self.guid = guid\n","        self.words = words\n","        self.labels = labels\n","        self.boxes = boxes\n","        self.actual_bboxes = actual_bboxes\n","        self.file_name = file_name\n","        self.page_size = page_size\n","\n","\n","class InputFeatures(object):\n","    \"\"\"A single set of features of data.\"\"\"\n","\n","    def __init__(\n","        self,\n","        input_ids,\n","        input_mask,\n","        segment_ids,\n","        label_ids,\n","        boxes,\n","        actual_bboxes,\n","        file_name,\n","        page_size,\n","    ):\n","        assert (\n","            0 <= all(boxes) <= 1000\n","        ), \"Error with input bbox ({}): the coordinate value is not between 0 and 1000\".format(\n","            boxes\n","        )\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.label_ids = label_ids\n","        self.boxes = boxes\n","        self.actual_bboxes = actual_bboxes\n","        self.file_name = file_name\n","        self.page_size = page_size\n","\n","\n","def read_examples_from_file(data_dir, mode):\n","    file_path = os.path.join(data_dir, \"{}.txt\".format(mode))\n","    box_file_path = os.path.join(data_dir, \"{}_box.txt\".format(mode))\n","    image_file_path = os.path.join(data_dir, \"{}_image.txt\".format(mode))\n","    guid_index = 1\n","    examples = []\n","    with open(file_path, encoding=\"utf-8\") as f, open(\n","        box_file_path, encoding=\"utf-8\"\n","    ) as fb, open(image_file_path, encoding=\"utf-8\") as fi:\n","        words = []\n","        boxes = []\n","        actual_bboxes = []\n","        file_name = None\n","        page_size = None\n","        labels = []\n","        for line, bline, iline in zip(f, fb, fi):\n","            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n","                if words:\n","                    examples.append(\n","                        InputExample(\n","                            guid=\"{}-{}\".format(mode, guid_index),\n","                            words=words,\n","                            labels=labels,\n","                            boxes=boxes,\n","                            actual_bboxes=actual_bboxes,\n","                            file_name=file_name,\n","                            page_size=page_size,\n","                        )\n","                    )\n","                    guid_index += 1\n","                    words = []\n","                    boxes = []\n","                    actual_bboxes = []\n","                    file_name = None\n","                    page_size = None\n","                    labels = []\n","            else:\n","                splits = line.split(\"\\t\")\n","                bsplits = bline.split(\"\\t\")\n","                isplits = iline.split(\"\\t\")\n","                assert len(splits) == 2\n","                assert len(bsplits) == 2\n","                assert len(isplits) == 4\n","                assert splits[0] == bsplits[0]\n","                words.append(splits[0])\n","                if len(splits) > 1:\n","                    labels.append(splits[-1].replace(\"\\n\", \"\"))\n","                    box = bsplits[-1].replace(\"\\n\", \"\")\n","                    box = [int(b) for b in box.split()]\n","                    boxes.append(box)\n","                    actual_bbox = [int(b) for b in isplits[1].split()]\n","                    actual_bboxes.append(actual_bbox)\n","                    page_size = [int(i) for i in isplits[2].split()]\n","                    file_name = isplits[3].strip()\n","                else:\n","                    # Examples could have no label for mode = \"test\"\n","                    labels.append(\"O\")\n","        if words:\n","            examples.append(\n","                InputExample(\n","                    guid=\"%s-%d\".format(mode, guid_index),\n","                    words=words,\n","                    labels=labels,\n","                    boxes=boxes,\n","                    actual_bboxes=actual_bboxes,\n","                    file_name=file_name,\n","                    page_size=page_size,\n","                )\n","            )\n","    return examples\n","\n","\n","def convert_examples_to_features(\n","    examples,\n","    label_list,\n","    max_seq_length,\n","    tokenizer,\n","    cls_token_at_end=False,\n","    cls_token=\"[CLS]\",\n","    cls_token_segment_id=1,\n","    sep_token=\"[SEP]\",\n","    sep_token_extra=False,\n","    pad_on_left=False,\n","    pad_token=0,\n","    cls_token_box=[0, 0, 0, 0],\n","    sep_token_box=[1000, 1000, 1000, 1000],\n","    pad_token_box=[0, 0, 0, 0],\n","    pad_token_segment_id=0,\n","    pad_token_label_id=-1,\n","    sequence_a_segment_id=0,\n","    mask_padding_with_zero=True,\n","):\n","    \"\"\" Loads a data file into a list of `InputBatch`s\n","        `cls_token_at_end` define the location of the CLS token:\n","            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n","            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n","        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n","    \"\"\"\n","\n","    label_map = {label: i for i, label in enumerate(label_list)}\n","\n","    features = []\n","    for (ex_index, example) in enumerate(examples):\n","        file_name = example.file_name\n","        page_size = example.page_size\n","        width, height = page_size\n","        if ex_index % 10000 == 0:\n","            logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n","\n","        tokens = []\n","        token_boxes = []\n","        actual_bboxes = []\n","        label_ids = []\n","        for word, label, box, actual_bbox in zip(\n","            example.words, example.labels, example.boxes, example.actual_bboxes\n","        ):\n","            word_tokens = tokenizer.tokenize(word)\n","            tokens.extend(word_tokens)\n","            token_boxes.extend([box] * len(word_tokens))\n","            actual_bboxes.extend([actual_bbox] * len(word_tokens))\n","            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n","            label_ids.extend(\n","                [label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1)\n","            )\n","\n","        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n","        special_tokens_count = 3 if sep_token_extra else 2\n","        if len(tokens) > max_seq_length - special_tokens_count:\n","            tokens = tokens[: (max_seq_length - special_tokens_count)]\n","            token_boxes = token_boxes[: (max_seq_length - special_tokens_count)]\n","            actual_bboxes = actual_bboxes[: (max_seq_length - special_tokens_count)]\n","            label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n","\n","        # The convention in BERT is:\n","        # (a) For sequence pairs:\n","        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n","        # (b) For single sequences:\n","        #  tokens:   [CLS] the dog is hairy . [SEP]\n","        #  type_ids:   0   0   0   0  0     0   0\n","        #\n","        # Where \"type_ids\" are used to indicate whether this is the first\n","        # sequence or the second sequence. The embedding vectors for `type=0` and\n","        # `type=1` were learned during pre-training and are added to the wordpiece\n","        # embedding vector (and position vector). This is not *strictly* necessary\n","        # since the [SEP] token unambiguously separates the sequences, but it makes\n","        # it easier for the model to learn the concept of sequences.\n","        #\n","        # For classification tasks, the first vector (corresponding to [CLS]) is\n","        # used as as the \"sentence vector\". Note that this only makes sense because\n","        # the entire model is fine-tuned.\n","        tokens += [sep_token]\n","        token_boxes += [sep_token_box]\n","        actual_bboxes += [[0, 0, width, height]]\n","        label_ids += [pad_token_label_id]\n","        if sep_token_extra:\n","            # roberta uses an extra separator b/w pairs of sentences\n","            tokens += [sep_token]\n","            token_boxes += [sep_token_box]\n","            actual_bboxes += [[0, 0, width, height]]\n","            label_ids += [pad_token_label_id]\n","        segment_ids = [sequence_a_segment_id] * len(tokens)\n","\n","        if cls_token_at_end:\n","            tokens += [cls_token]\n","            token_boxes += [cls_token_box]\n","            actual_bboxes += [[0, 0, width, height]]\n","            label_ids += [pad_token_label_id]\n","            segment_ids += [cls_token_segment_id]\n","        else:\n","            tokens = [cls_token] + tokens\n","            token_boxes = [cls_token_box] + token_boxes\n","            actual_bboxes = [[0, 0, width, height]] + actual_bboxes\n","            label_ids = [pad_token_label_id] + label_ids\n","            segment_ids = [cls_token_segment_id] + segment_ids\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","        # tokens are attended to.\n","        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n","\n","        # Zero-pad up to the sequence length.\n","        padding_length = max_seq_length - len(input_ids)\n","        if pad_on_left:\n","            input_ids = ([pad_token] * padding_length) + input_ids\n","            input_mask = (\n","                [0 if mask_padding_with_zero else 1] * padding_length\n","            ) + input_mask\n","            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n","            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n","            token_boxes = ([pad_token_box] * padding_length) + token_boxes\n","        else:\n","            input_ids += [pad_token] * padding_length\n","            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n","            segment_ids += [pad_token_segment_id] * padding_length\n","            label_ids += [pad_token_label_id] * padding_length\n","            token_boxes += [pad_token_box] * padding_length\n","\n","        assert len(input_ids) == max_seq_length\n","        assert len(input_mask) == max_seq_length\n","        assert len(segment_ids) == max_seq_length\n","        assert len(label_ids) == max_seq_length\n","        assert len(token_boxes) == max_seq_length\n","\n","        if ex_index < 5:\n","            logger.info(\"*** Example ***\")\n","            logger.info(\"guid: %s\", example.guid)\n","            logger.info(\"tokens: %s\", \" \".join([str(x) for x in tokens]))\n","            logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n","            logger.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n","            logger.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n","            logger.info(\"label_ids: %s\", \" \".join([str(x) for x in label_ids]))\n","            logger.info(\"boxes: %s\", \" \".join([str(x) for x in token_boxes]))\n","            logger.info(\"actual_bboxes: %s\", \" \".join([str(x) for x in actual_bboxes]))\n","\n","        features.append(\n","            InputFeatures(\n","                input_ids=input_ids,\n","                input_mask=input_mask,\n","                segment_ids=segment_ids,\n","                label_ids=label_ids,\n","                boxes=token_boxes,\n","                actual_bboxes=actual_bboxes,\n","                file_name=file_name,\n","                page_size=page_size,\n","            )\n","        )\n","    return features\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"woht1AgNopD1","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600775467763,"user_tz":-330,"elapsed":1299,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["\"\"\" Fine-tuning the library models for named entity recognition on CoNLL-2003 (Bert or Roberta). \"\"\"\n","\n","from __future__ import absolute_import, division, print_function\n","\n","import argparse\n","import glob\n","import logging\n","import os\n","import random\n","import shutil\n","\n","import numpy as np\n","import torch\n","from seqeval.metrics import (\n","    classification_report,\n","    f1_score,\n","    precision_score,\n","    recall_score,\n",")\n","from tensorboardX import SummaryWriter\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm import tqdm, trange\n","from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    BertConfig,\n","    BertForTokenClassification,\n","    BertTokenizer,\n","    RobertaConfig,\n","    RobertaForTokenClassification,\n","    RobertaTokenizer,\n","    get_linear_schedule_with_warmup,\n",")\n","'''\n","Other file imports {take only important files for deployment}\n","'''\n","#from layoutlm import FunsdDataset, LayoutlmConfig, LayoutlmForTokenClassification\n","\n","logger = logging.getLogger(__name__)\n","\n","ALL_MODELS = sum(\n","    (\n","        tuple(conf.pretrained_config_archive_map.keys())\n","        for conf in (BertConfig, RobertaConfig, LayoutlmConfig)\n","    ),\n","    (),\n",")\n","\n","MODEL_CLASSES = {\n","    \"bert\": (BertConfig, BertForTokenClassification, BertTokenizer),\n","    \"roberta\": (RobertaConfig, RobertaForTokenClassification, RobertaTokenizer),\n","    \"layoutlm\": (LayoutlmConfig, LayoutlmForTokenClassification, BertTokenizer),\n","}\n","\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if args.n_gpu > 0:\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","\n","def collate_fn(data):\n","    batch = [i for i in zip(*data)]\n","    for i in range(len(batch)):\n","        if i < len(batch) - 2:\n","            batch[i] = torch.stack(batch[i], 0)\n","    return tuple(batch)\n","\n","\n","def get_labels(path):\n","    with open(path, \"r\") as f:\n","        labels = f.read().splitlines()\n","    if \"O\" not in labels:\n","        labels = [\"O\"] + labels\n","    return labels"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"uZGxEw8NplFf","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600775525664,"user_tz":-330,"elapsed":1303,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["def evaluate(args, model, tokenizer, labels, pad_token_label_id, mode, prefix=\"\"):\n","    eval_dataset = FunsdDataset(args, tokenizer, labels, pad_token_label_id, mode=mode)\n","\n","    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n","    eval_sampler = SequentialSampler(eval_dataset)\n","    eval_dataloader = DataLoader(\n","        eval_dataset,\n","        sampler=eval_sampler,\n","        batch_size=args.eval_batch_size,\n","        collate_fn=None,\n","    )\n","\n","    # Eval!\n","    logger.info(\"***** Running evaluation %s *****\", prefix)\n","    logger.info(\"  Num examples = %d\", len(eval_dataset))\n","    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n","    eval_loss = 0.0\n","    nb_eval_steps = 0\n","    preds = None\n","    out_label_ids = None\n","    model.eval()\n","    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        with torch.no_grad():\n","            inputs = {\n","                \"input_ids\": batch[0].to(args.device),\n","                \"attention_mask\": batch[1].to(args.device),\n","                \"labels\": batch[3].to(args.device),\n","            }\n","            if args.model_type in [\"layoutlm\"]:\n","                inputs[\"bbox\"] = batch[4].to(args.device)\n","            inputs[\"token_type_ids\"] = (\n","                batch[2].to(args.device)\n","                if args.model_type in [\"bert\", \"layoutlm\"]\n","                else None\n","            )  # RoBERTa don\"t use segment_ids\n","           # print(\"\\n\\n lenght :- \",len(inputs))\n","           # print(\"\\n\\n keys :- \",inputs.keys())\n","           # print(\"\\n\\n\\n\\n\",)\n","            outputs = model(**inputs)\n","            tmp_eval_loss, logits = outputs[:2]\n","\n","            if args.n_gpu > 1:\n","                tmp_eval_loss = (\n","                    tmp_eval_loss.mean()\n","                )  # mean() to average on multi-gpu parallel evaluating\n","\n","            eval_loss += tmp_eval_loss.item()\n","        nb_eval_steps += 1\n","        if preds is None:\n","            preds = logits.detach().cpu().numpy()\n","            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n","        else:\n","            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n","            out_label_ids = np.append(\n","                out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n","            )\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    preds = np.argmax(preds, axis=2)\n","\n","    label_map = {i: label for i, label in enumerate(labels)}\n","\n","    out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n","    preds_list = [[] for _ in range(out_label_ids.shape[0])]\n","\n","    for i in range(out_label_ids.shape[0]):\n","        for j in range(out_label_ids.shape[1]):\n","            if out_label_ids[i, j] != pad_token_label_id:\n","                out_label_list[i].append(label_map[out_label_ids[i][j]])\n","                preds_list[i].append(label_map[preds[i][j]])\n","\n","    results = {\n","        \"loss\": eval_loss,\n","        \"precision\": precision_score(out_label_list, preds_list),\n","        \"recall\": recall_score(out_label_list, preds_list),\n","        \"f1\": f1_score(out_label_list, preds_list),\n","    }\n","\n","    report = classification_report(out_label_list, preds_list)\n","    logger.info(\"\\n\" + report)\n","\n","    logger.info(\"***** Eval results %s *****\", prefix)\n","    for key in sorted(results.keys()):\n","        logger.info(\"  %s = %s\", key, str(results[key]))\n","\n","    return results, preds_list"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"A5qt0kZFqA-L","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600775541382,"user_tz":-330,"elapsed":1309,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["class sequence_labelling_data_input:\n","  def __init__(self,data_dir = None,model_type = None,model_name_or_path = None,output_dir = None,labels = \"\",config_name = \"\",tokenizer_name=\"\",cache_dir = \"\",max_seq_length = 512,do_train = False,do_eval = False,do_predict = False,evaluate_during_training = False,do_lower_case = False,per_gpu_train_batch_size = 8,per_gpu_eval_batch_size=8,gradient_accumulation_steps = 1,learning_rate = 5e-5,weight_decay = 0.0,adam_epsilon = 1e-8,max_grad_norm = 1.0,num_train_epochs = 3.0,max_steps = -1,warmup_steps = 0,logging_steps = 50,eval_all_checkpoins = False,no_cuda = False,overwrite_output_dir = False,overwrite_cache = False,seed = 42,fp16 = False,fp16_opt_level = \"01\",local_rank = -1,server_ip = \"\",server_port = \"\",save_steps = -1):\n","    #The input data dir. Should contain the training files for the CoNLL-2003 NER task.\n","    self.data_dir = data_dir\n","    #Model type selected in the list: \n","    self.model_type = model_type\n","    #Path to pre-trained model or shortcut name selected in the list: \n","    self.model_name_or_path = model_name_or_path \n","    #The output directory where the model predictions and checkpoints will be written.\n","    self.output_dir = output_dir\n","    #Path to a file containing all labels. If not specified, CoNLL-2003 labels are used\n","    self.labels = labels\n","    #Pretrained config name or path if not the same as model_name\n","    self.config_name = config_name\n","    #Pretrained tokenizer name or path if not the same as model_name\n","    self.tokenizer_name = tokenizer_name\n","    #Where do you want to store the pre-trained models downloaded from s3\n","    self.cache_dir = cache_dir\n","    #The maximum total input sequence length after tokenization. Sequences longer \"than this will be truncated, sequences shorter will be padded.\"\n","    self.max_seq_length = max_seq_length\n","    #Whether to run training.\n","    self.do_train = do_train\n","    #Whether to run eval on the dev set.\n","    self.do_eval = do_eval\n","    #Whether to run predictions on the test set.\n","    self.do_predict = do_predict\n","    #Whether to run evaluation during training at each logging step.\n","    self.evaluate_during_training = evaluate_during_training\n","    #Set this flag if you are using an uncased model.\n","    self.do_lower_case = do_lower_case\n","    #Batch size per GPU/CPU for training.\n","    self.per_gpu_train_batch_size = per_gpu_train_batch_size\n","    #Batch size per GPU/CPU for evaluation.\n","    self.per_gpu_eval_batch_size = per_gpu_eval_batch_size\n","    #Number of updates steps to accumulate before performing a backward/update pass.\n","    self.gradient_accumulation_steps = gradient_accumulation_steps\n","    #The initial learning rate for Adam.\n","    self.learning_rate = learning_rate\n","    #Weight decay if we apply some.\n","    self.weight_decay = weight_decay\n","    #Epsilon for Adam optimizer.\n","    self.adam_epsilon = adam_epsilon\n","    #Max gradient norm.\n","    self.max_grad_norm = max_grad_norm\n","    #Total number of training epochs to perform.\n","    self.num_train_epochs = num_train_epochs\n","    #If > 0: set total number of training steps to perform. Override num_train_epochs.\n","    self.max_steps = max_steps\n","    #Linear warmup over warmup_steps.\n","    self.warmup_steps = warmup_steps\n","    #Log every X updates steps.\n","    self.logging_steps = logging_steps\n","    #Save checkpoint every X updates steps.\n","    self.save_steps = save_steps\n","    #Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\n","    self.eval_all_checkpoints = False\n","    #Avoid using CUDA when available\n","    self.no_cuda = False\n","    #Overwrite the content of the output directory\n","    self.overwrite_output_dir =False\n","    #Overwrite the cached training and evaluation sets\n","    self.overwrite_cache = overwrite_cache\n","    #random seed for initialization\n","    self.seed = seed\n","    #Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\n","    self.fp16 = fp16\n","    #For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\"See details at https://nvidia.github.io/apex/amp.html\"\n","    self.fp16_opt_level = fp16_opt_level\n","    #For distributed training: local_rank\n","    self.local_rank = local_rank\n","    #For distant debugging.\n","    self.server_ip = server_ip\n","    #For distant debugging.\n","    self.server_port = server_port\n","\n","#args = parser.parse_args()"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"TUUNOx_EqE0c","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600775556141,"user_tz":-330,"elapsed":1428,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["def main(args):  # noqa C901\n","    if (\n","        os.path.exists(args.output_dir)\n","        and os.listdir(args.output_dir)\n","        and args.do_train\n","    ):\n","        if not args.overwrite_output_dir:\n","            raise ValueError(\n","                \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n","                    args.output_dir\n","                )\n","            )\n","        else:\n","            if args.local_rank in [-1, 0]:\n","                shutil.rmtree(args.output_dir)\n","\n","    if not os.path.exists(args.output_dir) and (args.do_eval or args.do_predict):\n","        raise ValueError(\n","            \"Output directory ({}) does not exist. Please train and save the model before inference stage.\".format(\n","                args.output_dir\n","            )\n","        )\n","\n","    if (\n","        not os.path.exists(args.output_dir)\n","        and args.do_train\n","        and args.local_rank in [-1, 0]\n","    ):\n","        os.makedirs(args.output_dir)\n","\n","    # Setup distant debugging if needed\n","    if args.server_ip and args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","     #   print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(\n","            address=(args.server_ip, args.server_port), redirect_output=True\n","        )\n","        ptvsd.wait_for_attach()\n","\n","    # Setup CUDA, GPU & distributed training\n","    if args.local_rank == -1 or args.no_cuda:\n","        device = torch.device(\n","            \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n","        )\n","        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n","    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","        torch.cuda.set_device(args.local_rank)\n","        device = torch.device(\"cuda\", args.local_rank)\n","        torch.distributed.init_process_group(backend=\"nccl\")\n","        args.n_gpu = 1\n","    args.device = device\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        filename=os.path.join(args.output_dir, \"train.log\")\n","        if args.local_rank in [-1, 0]\n","        else None,\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.warning(\n","        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","        args.local_rank,\n","        device,\n","        args.n_gpu,\n","        bool(args.local_rank != -1),\n","        args.fp16,\n","    )\n","\n","    # Set seed\n","    set_seed(args)\n","\n","    labels = get_labels(args.labels)\n","    num_labels = len(labels)\n","    # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n","    pad_token_label_id = CrossEntropyLoss().ignore_index\n","\n","    # Load pretrained model and tokenizer\n","    if args.local_rank not in [-1, 0]:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n","\n","    args.model_type = args.model_type.lower()\n","    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","    config = config_class.from_pretrained(\n","        args.config_name if args.config_name else args.model_name_or_path,\n","        num_labels=num_labels,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    tokenizer = tokenizer_class.from_pretrained(\n","        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n","        do_lower_case=args.do_lower_case,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    model = model_class.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","\n","    if args.local_rank == 0:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n","\n","    model.to(args.device)\n","\n","    logger.info(\"Training/evaluation parameters %s\", args)\n","\n","    \n","    \n","    # Training\n","    if args.do_train:\n","        train_dataset = FunsdDataset(\n","            args, tokenizer, labels, pad_token_label_id, mode=\"train\"\n","        )\n","        #print('new statment', labels)\n","        #print('pad_id', pad_token_label_id)\n","        global_step, tr_loss = train(\n","            args, train_dataset, model, tokenizer, labels, pad_token_label_id\n","        )\n","        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n","\n","    \n","    \n","    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n","        # Create output directory if needed\n","        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n","            os.makedirs(args.output_dir)\n","\n","        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n","        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","        # They can then be reloaded using `from_pretrained()`\n","        model_to_save = (\n","            model.module if hasattr(model, \"module\") else model\n","        )  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(args.output_dir)\n","        tokenizer.save_pretrained(args.output_dir)\n","\n","        # Good practice: save your training arguments together with the trained model\n","        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n","\n","    \n","    \n","    \n","    # Evaluation\n","    results = {}\n","    if args.do_eval and args.local_rank in [-1, 0]:\n","        tokenizer = tokenizer_class.from_pretrained(\n","            args.output_dir, do_lower_case=args.do_lower_case\n","        )\n","        checkpoints = [args.output_dir]\n","        if args.eval_all_checkpoints:\n","            checkpoints = list(\n","                os.path.dirname(c)\n","                for c in sorted(\n","                    glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True)\n","                )\n","            )\n","            logging.getLogger(\"pytorch_transformers.modeling_utils\").setLevel(\n","                logging.WARN\n","            )  # Reduce logging\n","        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","        for checkpoint in checkpoints:\n","            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n","            model = model_class.from_pretrained(checkpoint)\n","            model.to(args.device)\n","            result, _ = evaluate(\n","                args,\n","                model,\n","                tokenizer,\n","                labels,\n","                pad_token_label_id,\n","                mode=\"test\",\n","                prefix=global_step,\n","            )\n","            if global_step:\n","                result = {\"{}_{}\".format(global_step, k): v for k, v in result.items()}\n","            results.update(result)\n","        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n","        with open(output_eval_file, \"w\") as writer:\n","            for key in sorted(results.keys()):\n","                writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n","\n","    \n","    \n","    # do predict part\n","    if args.do_predict and args.local_rank in [-1, 0]:\n","        tokenizer = tokenizer_class.from_pretrained(\n","            args.model_name_or_path, do_lower_case=args.do_lower_case\n","        )\n","        model = model_class.from_pretrained(args.output_dir)\n","        model.to(args.device)\n","       # print(\"\\n evaluate function parameter1 args: - \", args)\n","        #print(\"\\n evaluate function parameter2 model : - \", model)\n","        #print(\"\\n evaluate function parameters3 tokenizer :- \", tokenizer)\n","        #print(\"\\n evaluate function parameter4 labels : - \", labels)\n","        #print(\"\\n  evaluate fnctions parameter 5 pad_token_label\", pad_token_label_id)\n","        result, predictions = evaluate(\n","            args, model, tokenizer, labels, pad_token_label_id, mode=\"test\"\n","        )\n","        # Save results\n","        output_test_results_file = os.path.join(args.output_dir, \"test_results.txt\")\n","        with open(output_test_results_file, \"w\") as writer:\n","            for key in sorted(result.keys()):\n","                writer.write(\"{} = {}\\n\".format(key, str(result[key])))\n","        # Save predictions\n","        output_test_predictions_file = os.path.join(\n","            args.output_dir, \"test_predictions.txt\"\n","        )\n","        with open(output_test_predictions_file, \"w\", encoding=\"utf8\") as writer:\n","            with open(\n","                os.path.join(args.data_dir, \"test.txt\"), \"r\", encoding=\"utf8\"\n","            ) as f:\n","                example_id = 0\n","                for line in f:\n","                    if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n","                        writer.write(line)\n","                        if not predictions[example_id]:\n","                            example_id += 1\n","                    elif predictions[example_id]:\n","                        output_line = (\n","                            line.split()[0]\n","                            + \" \"\n","                            + predictions[example_id].pop(0)\n","                            + \"\\n\"\n","                        )\n","                        writer.write(output_line)\n","                    else:\n","                        logger.warning(\n","                            \"Maximum sequence length exceeded: No prediction for '%s'.\",\n","                            line.split()[0],\n","                        )\n","\n","    return results"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"OL3gwB9ZqIY4","colab_type":"code","colab":{}},"source":["args = sequence_labelling_data_input(data_dir = '/content/gdrive/My Drive/OCR_PROJECT/SROIE/pre-processing files',model_type = \"layoutlm\",output_dir = \"/content/gdrive/My Drive/OCR_PROJECT/SROIE/predictions/output\",model_name_or_path = \"bert-base-uncased\",do_lower_case = True,max_seq_length = 512,do_predict = True,labels = '/content/gdrive/My Drive/OCR_PROJECT/SROIE/pre-processing files/labels.txt',fp16 = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q9khDQEuqL-4","colab_type":"code","colab":{}},"source":["if __name__ == \"__main__\":\n","    main(args)"],"execution_count":null,"outputs":[]}]}