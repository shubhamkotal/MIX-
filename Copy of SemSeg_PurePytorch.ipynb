{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of SemSeg_PurePytorch.ipynb","provenance":[{"file_id":"1zmylQvjNt6KTXHs-K9v8E8rbGolZzFHm","timestamp":1619609984056},{"file_id":"1SuiOk8enCny3T96iumc0NxS2IcZCgeGm","timestamp":1585568374571}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"7016ab5f0d1246e89ef36ab5e68aabdd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0dcb98f1da6d4772b2cd416fdce0a251","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_3bb748ee4f5f47eea892d1e8c1bbef54","IPY_MODEL_cab7cf4bc35f47c0ba7036ef13ead06f"]}},"0dcb98f1da6d4772b2cd416fdce0a251":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3bb748ee4f5f47eea892d1e8c1bbef54":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_710407b10ab64a348843de665474d388","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":178728960,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":178728960,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_35a8d9dea55742e4bfe8f0534f2d5707"}},"cab7cf4bc35f47c0ba7036ef13ead06f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_6191b21519f0420fa37fdc5ef217a412","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 170M/170M [00:21&lt;00:00, 8.39MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d429124cd9b94faca69007c072d9e259"}},"710407b10ab64a348843de665474d388":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"35a8d9dea55742e4bfe8f0534f2d5707":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6191b21519f0420fa37fdc5ef217a412":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d429124cd9b94faca69007c072d9e259":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3b855cfb78f94884a77f51458885ef99":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_552e7cee84b040dfb81ee9ee53fd25d8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_a46a2a9f9bc24fb48096efdee9a09857","IPY_MODEL_0ac6be1db5c1440291af22e961181e9f"]}},"552e7cee84b040dfb81ee9ee53fd25d8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a46a2a9f9bc24fb48096efdee9a09857":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_cda263447b314ed5a7163df6d74c1d9c","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":217800805,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":217800805,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_bff1ae5bc42b45adbdf6878e3b747efa"}},"0ac6be1db5c1440291af22e961181e9f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a883c2c3af294b52afbf66a03e383203","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 208M/208M [00:01&lt;00:00, 205MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_088381b29901458688f86210e206aa06"}},"cda263447b314ed5a7163df6d74c1d9c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"bff1ae5bc42b45adbdf6878e3b747efa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a883c2c3af294b52afbf66a03e383203":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"088381b29901458688f86210e206aa06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"PhJHOb50O5fI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619610053829,"user_tz":-330,"elapsed":27350,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"18607e44-06a4-41d0-cdba-4e10854a8a4c"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"gLJo8f0iPYRs","executionInfo":{"status":"ok","timestamp":1619610074436,"user_tz":-330,"elapsed":4448,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["from pathlib import Path\n","import random\n","from PIL import Image\n","import cv2\n","import torch\n","import numpy as np\n","# from torch.utils.data import DataLoader\n","# from torch.utils.data import Dataset\n","from torchvision import models\n","import torchvision.transforms as T\n","\n","#import segmentation_models_pytorch as smp\n","#import albumentations as albu"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"fTZGafY1Qwpl","executionInfo":{"status":"ok","timestamp":1619610112611,"user_tz":-330,"elapsed":1206,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["# Data directory\n","loc = '/content/gdrive/MyDrive/Deeplobe GIT/deeplobe-ai-master/Datasets/semantic'\n","path = Path(loc)\n","path_lbl = path/'Masks'\n","path_img = path/'Images'\n","mask_definitions = path/'mask_definitions.json'"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"17ahUVjVwvQm","executionInfo":{"status":"ok","timestamp":1619610116516,"user_tz":-330,"elapsed":1442,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["def return_files(path):\n","  files = []\n","  for ext in extensions:\n","    files.extend(path.glob('*.'+ext))\n","  return files"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"HR_ofwpzxaQh","executionInfo":{"status":"ok","timestamp":1619610118943,"user_tz":-330,"elapsed":1078,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["#get filenames from directory\n","extensions = [\"jpg\",'jpeg','png','PNG']\n","img_files = return_files(path_img)\n","mask_files = return_files(path_lbl)"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"9p6KMdiix9_p","executionInfo":{"status":"ok","timestamp":1619610125156,"user_tz":-330,"elapsed":1300,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["#train val split\n","\n","dataset_size = len(img_files)\n","valid_size = int(dataset_size * 0.5)\n","train_size = dataset_size - valid_size\n","\n","#images\n","valid_imgs = random.sample(img_files,valid_size)\n","train_imgs = [img for img in img_files if img not in valid_imgs]\n","\n","#corresponding masks\n","valid_img_names = [img.stem for img in valid_imgs]\n","valid_masks = [mask for mask in mask_files if mask.stem in valid_img_names]\n","train_masks = [mask for mask in mask_files if mask not in valid_masks]"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"Slyruuln41vA","executionInfo":{"status":"ok","timestamp":1619610132225,"user_tz":-330,"elapsed":1419,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["#get label names and color-codes from mask_definitions json\n","with open(mask_definitions) as md:\n","    mask_def = eval(md.read())\n","\n","mask_info = {}\n","label_dict = {}\n","\n","for image in mask_def['masks']:\n","    mask=mask_def['masks'][image]['mask']\n","    mask = mask.lstrip('masks/')\n","    colors = mask_def['masks'][image]['color_categories']\n","    rgb = colors.keys()\n","    categories = [i['super_category'] for i in colors.values()]\n","    color_map = dict(zip(rgb,categories))\n","    mask_info.update({mask:color_map})\n","\n","for v in mask_info.values():\n","  label_dict.update(v.items())\n","\n","#create an rgb dict with colors as keys and labels as values\n","label_dict['(0, 0, 0)'] = 'background'\n","\n","name2id = {v:k for k,v in enumerate(sorted(label_dict.values()))}\n","class_dict = {eval(v):k for k,v in enumerate(sorted(label_dict.keys()))}"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQR5OF6z2PvH","executionInfo":{"status":"ok","timestamp":1619610143563,"user_tz":-330,"elapsed":1160,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, img_files, img_transform=None,mask_transform=None):\n","        self.img_files = img_files\n","        self.img_transform = img_transform\n","        self.mask_transform = mask_transform\n","\n","    #helper function to convert rbg mask to label encoded masks\n","    def mask_to_class(self,mask):\n","        target = torch.from_numpy(mask)\n","        #target = target.permute(1,2,0).contiguous()\n","        h,w = target.shape[0],target.shape[1]\n","        masks = torch.zeros(h, w, dtype=torch.long)\n","        colors = torch.unique(target.view(-1,target.size(2)),dim=0).numpy()\n","        target = target.permute(2, 0, 1).contiguous()\n","        mapping = class_dict #{tuple(c): t for c, t in zip(colors.tolist(), range(len(colors)))}\n","\n","        for k in mapping:\n","            idx = (target==torch.tensor(k, dtype=torch.uint8).unsqueeze(1).unsqueeze(2))\n","            validx = (idx.sum(0) == 3) \n","            masks[validx] = torch.tensor(mapping[k], dtype=torch.long)\n","        return masks\n","\n","    def __getitem__(self, idx):\n","        image_file = self.img_files[idx]\n","        mask_file = path_lbl/image_file.name      \n","        # mask_file = str(path_lbl/image_file.name)\n","        # image_file = str(image_file)\n","\n","        image = Image.open(image_file)\n","        mask = Image.open(mask_file)  \n","\n","        # image = cv2.imread(image_file)\n","        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","        # mask = cv2.imread(mask_file)\n","        # mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)\n","        \n","        #apply tranforms\n","        if self.img_transform:\n","          image = self.img_transform(image)\n","\n","        if self.mask_transform:\n","          mask = self.mask_transform(mask)\n","\n","        #encode RGB mask to class_labels\n","        mask = np.array(mask)\n","        mask = self.mask_to_class(mask)\n","\n","        return image, mask\n","\n","    def __len__(self):  # return count of sample we have\n","        return len(self.img_files)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"5TuZUwVC8ECP","executionInfo":{"status":"ok","timestamp":1619610146701,"user_tz":-330,"elapsed":1531,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["def get_img_transform(train=True):\n","    transforms = []\n","    # converts the image, a PIL image, into a PyTorch Tensor\n","    transforms.append(T.Resize(256))\n","    transforms.append(T.CenterCrop(224))\n","    transforms.append(T.ToTensor())\n","    transforms.append(T.Normalize(mean=[0.485,0.456,0.406],\n","                                std=[0.229,0.224,0.225]))\n","\n","    return T.Compose(transforms)\n","\n","def get_mask_transform(train=True):\n","    transforms = []\n","    # apply same resize and cropping on masks\n","    transforms.append(T.Resize(256))\n","    transforms.append(T.CenterCrop(224))    \n","    #transforms.append(T.ToTensor())\n","\n","    return T.Compose(transforms)\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"KAaC0tTpzvvy","executionInfo":{"status":"ok","timestamp":1619610149141,"user_tz":-330,"elapsed":707,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["# Datasets\n","train_dataset = CustomDataset(train_imgs,img_transform=get_img_transform(train=True),mask_transform=get_mask_transform(train=True))\n","valid_dataset = CustomDataset(valid_imgs,img_transform=get_img_transform(train=False),mask_transform=get_mask_transform(train=False))"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vAHynPIJS5yG","executionInfo":{"status":"ok","timestamp":1619610441642,"user_tz":-330,"elapsed":1098,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"c40bf2c2-611c-4a87-ef0a-013d43582314"},"source":["train_dataset[0][0].shape"],"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([3, 224, 224])"]},"metadata":{"tags":[]},"execution_count":29}]},{"cell_type":"code","metadata":{"id":"C1ybriKNUlQG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1619610152465,"user_tz":-330,"elapsed":1136,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"b25bbeb9-7bc3-4e97-e672-837ac36a8727"},"source":["# Dataloaders\n","bs = 1 #batch_size\n","train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=bs,shuffle=True,num_workers=4)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=bs,shuffle=False,num_workers=4)"],"execution_count":12,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wu7IQOrQSbfi","executionInfo":{"status":"ok","timestamp":1619610176821,"user_tz":-330,"elapsed":1406,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"4b6a1079-00a6-4b05-cf87-aeeec2f98c3e"},"source":["a=iter(train_loader)"],"execution_count":13,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"TKTGilseShF9","executionInfo":{"status":"ok","timestamp":1619610226732,"user_tz":-330,"elapsed":1114,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["b=next(a)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GfgTuYVHStDk","executionInfo":{"status":"ok","timestamp":1619610242287,"user_tz":-330,"elapsed":1236,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"822f00b4-3f19-4f4c-e585-55dbad291202"},"source":["b[0].shape"],"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1, 3, 224, 224])"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"p6PddN7PuTdC","colab":{"base_uri":"https://localhost:8080/","height":154,"referenced_widgets":["7016ab5f0d1246e89ef36ab5e68aabdd","0dcb98f1da6d4772b2cd416fdce0a251","3bb748ee4f5f47eea892d1e8c1bbef54","cab7cf4bc35f47c0ba7036ef13ead06f","710407b10ab64a348843de665474d388","35a8d9dea55742e4bfe8f0534f2d5707","6191b21519f0420fa37fdc5ef217a412","d429124cd9b94faca69007c072d9e259","3b855cfb78f94884a77f51458885ef99","552e7cee84b040dfb81ee9ee53fd25d8","a46a2a9f9bc24fb48096efdee9a09857","0ac6be1db5c1440291af22e961181e9f","cda263447b314ed5a7163df6d74c1d9c","bff1ae5bc42b45adbdf6878e3b747efa","a883c2c3af294b52afbf66a03e383203","088381b29901458688f86210e206aa06"]},"executionInfo":{"status":"ok","timestamp":1614148325269,"user_tz":-330,"elapsed":15109,"user":{"displayName":"Mohit Tuli","photoUrl":"","userId":"05250823286326681737"}},"outputId":"6a3ff9c9-22dc-4cc4-9a0f-e974629f9048"},"source":["#Model instantiation\n","\n","classes = len(name2id.keys())\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","#FCN Head\n","from torchvision.models.segmentation.fcn import FCNHead\n","\n","# create segmentation model with pretrained encoder\n","model = models.segmentation.fcn_resnet101(pretrained=True)\n","# change the output layer\n","model.classifier = FCNHead(2048,classes)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/resnet101-5d3b4d8f.pth\" to /root/.cache/torch/hub/checkpoints/resnet101-5d3b4d8f.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"7016ab5f0d1246e89ef36ab5e68aabdd","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=178728960.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/fcn_resnet101_coco-7ecb50ca.pth\" to /root/.cache/torch/hub/checkpoints/fcn_resnet101_coco-7ecb50ca.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3b855cfb78f94884a77f51458885ef99","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=217800805.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"bQ5IEeezXMIz"},"source":["# training the model:\n","\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.Adadelta(params, lr=1e-4)\n","criterion = torch.nn.CrossEntropyLoss()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7dq5eXdoYcaY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1614148696047,"user_tz":-330,"elapsed":243780,"user":{"displayName":"Mohit Tuli","photoUrl":"","userId":"05250823286326681737"}},"outputId":"2a90be7c-3bd8-4702-8955-86dff7fa00c3"},"source":["num_epochs = 2\n","total_batches = len(train_loader)\n","\n","for epoch in range(num_epochs):\n","  model.train()\n","  model.to(device)\n","  i = 0\n","  for imgs,masks in train_loader:\n","    imgs = imgs.to(device)\n","    masks = masks.to(device)\n","    outputs = model(imgs)\n","    loss = criterion(outputs['out'],masks)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","    \n","    print(f'Epoch no: {epoch}/{num_epochs-1}, Batch no: {i}/{total_batches}, Loss: {loss}')\n","    i+=1\n","  "],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch no: 0/1, Batch no: 0/300, Loss: 0.8002278804779053\n","Epoch no: 0/1, Batch no: 1/300, Loss: 1.0299780368804932\n","Epoch no: 0/1, Batch no: 2/300, Loss: 1.0450252294540405\n","Epoch no: 0/1, Batch no: 3/300, Loss: 0.9137486219406128\n","Epoch no: 0/1, Batch no: 4/300, Loss: 1.0510627031326294\n","Epoch no: 0/1, Batch no: 5/300, Loss: 1.0226765871047974\n","Epoch no: 0/1, Batch no: 6/300, Loss: 0.9292600154876709\n","Epoch no: 0/1, Batch no: 7/300, Loss: 1.0380001068115234\n","Epoch no: 0/1, Batch no: 8/300, Loss: 0.946729838848114\n","Epoch no: 0/1, Batch no: 9/300, Loss: 1.0359259843826294\n","Epoch no: 0/1, Batch no: 10/300, Loss: 1.0117186307907104\n","Epoch no: 0/1, Batch no: 11/300, Loss: 0.8990718126296997\n","Epoch no: 0/1, Batch no: 12/300, Loss: 0.9889218807220459\n","Epoch no: 0/1, Batch no: 13/300, Loss: 1.007154107093811\n","Epoch no: 0/1, Batch no: 14/300, Loss: 0.9647420644760132\n","Epoch no: 0/1, Batch no: 15/300, Loss: 1.0852463245391846\n","Epoch no: 0/1, Batch no: 16/300, Loss: 1.1243287324905396\n","Epoch no: 0/1, Batch no: 17/300, Loss: 1.0129146575927734\n","Epoch no: 0/1, Batch no: 18/300, Loss: 0.9873413443565369\n","Epoch no: 0/1, Batch no: 19/300, Loss: 1.0642200708389282\n","Epoch no: 0/1, Batch no: 20/300, Loss: 0.9253545999526978\n","Epoch no: 0/1, Batch no: 21/300, Loss: 1.0860741138458252\n","Epoch no: 0/1, Batch no: 22/300, Loss: 1.068686842918396\n","Epoch no: 0/1, Batch no: 23/300, Loss: 0.9423631429672241\n","Epoch no: 0/1, Batch no: 24/300, Loss: 1.0036964416503906\n","Epoch no: 0/1, Batch no: 25/300, Loss: 0.9602713584899902\n","Epoch no: 0/1, Batch no: 26/300, Loss: 0.9384676218032837\n","Epoch no: 0/1, Batch no: 27/300, Loss: 0.9869201183319092\n","Epoch no: 0/1, Batch no: 28/300, Loss: 0.9847052693367004\n","Epoch no: 0/1, Batch no: 29/300, Loss: 1.0423640012741089\n","Epoch no: 0/1, Batch no: 30/300, Loss: 0.9926791191101074\n","Epoch no: 0/1, Batch no: 31/300, Loss: 1.0412203073501587\n","Epoch no: 0/1, Batch no: 32/300, Loss: 1.0721662044525146\n","Epoch no: 0/1, Batch no: 33/300, Loss: 0.9802998304367065\n","Epoch no: 0/1, Batch no: 34/300, Loss: 1.0453932285308838\n","Epoch no: 0/1, Batch no: 35/300, Loss: 1.0274015665054321\n","Epoch no: 0/1, Batch no: 36/300, Loss: 1.1099135875701904\n","Epoch no: 0/1, Batch no: 37/300, Loss: 1.0406591892242432\n","Epoch no: 0/1, Batch no: 38/300, Loss: 0.9498603343963623\n","Epoch no: 0/1, Batch no: 39/300, Loss: 1.0138297080993652\n","Epoch no: 0/1, Batch no: 40/300, Loss: 1.027942419052124\n","Epoch no: 0/1, Batch no: 41/300, Loss: 1.0370255708694458\n","Epoch no: 0/1, Batch no: 42/300, Loss: 0.9617831707000732\n","Epoch no: 0/1, Batch no: 43/300, Loss: 1.0178191661834717\n","Epoch no: 0/1, Batch no: 44/300, Loss: 0.8634386658668518\n","Epoch no: 0/1, Batch no: 45/300, Loss: 1.0274217128753662\n","Epoch no: 0/1, Batch no: 46/300, Loss: 0.843614399433136\n","Epoch no: 0/1, Batch no: 47/300, Loss: 1.063601016998291\n","Epoch no: 0/1, Batch no: 48/300, Loss: 1.079769253730774\n","Epoch no: 0/1, Batch no: 49/300, Loss: 0.9079074263572693\n","Epoch no: 0/1, Batch no: 50/300, Loss: 1.0710676908493042\n","Epoch no: 0/1, Batch no: 51/300, Loss: 0.8841181397438049\n","Epoch no: 0/1, Batch no: 52/300, Loss: 1.0504939556121826\n","Epoch no: 0/1, Batch no: 53/300, Loss: 0.9965932369232178\n","Epoch no: 0/1, Batch no: 54/300, Loss: 1.086478352546692\n","Epoch no: 0/1, Batch no: 55/300, Loss: 1.005057692527771\n","Epoch no: 0/1, Batch no: 56/300, Loss: 0.9503464102745056\n","Epoch no: 0/1, Batch no: 57/300, Loss: 0.8249961733818054\n","Epoch no: 0/1, Batch no: 58/300, Loss: 0.9980150461196899\n","Epoch no: 0/1, Batch no: 59/300, Loss: 0.8818906545639038\n","Epoch no: 0/1, Batch no: 60/300, Loss: 0.896882176399231\n","Epoch no: 0/1, Batch no: 61/300, Loss: 1.0512912273406982\n","Epoch no: 0/1, Batch no: 62/300, Loss: 0.9828944802284241\n","Epoch no: 0/1, Batch no: 63/300, Loss: 0.9814974665641785\n","Epoch no: 0/1, Batch no: 64/300, Loss: 1.0048376321792603\n","Epoch no: 0/1, Batch no: 65/300, Loss: 1.072474718093872\n","Epoch no: 0/1, Batch no: 66/300, Loss: 1.06212317943573\n","Epoch no: 0/1, Batch no: 67/300, Loss: 0.9861019849777222\n","Epoch no: 0/1, Batch no: 68/300, Loss: 0.9852177500724792\n","Epoch no: 0/1, Batch no: 69/300, Loss: 1.0172879695892334\n","Epoch no: 0/1, Batch no: 70/300, Loss: 0.9889024496078491\n","Epoch no: 0/1, Batch no: 71/300, Loss: 1.0000733137130737\n","Epoch no: 0/1, Batch no: 72/300, Loss: 0.805029571056366\n","Epoch no: 0/1, Batch no: 73/300, Loss: 1.0203458070755005\n","Epoch no: 0/1, Batch no: 74/300, Loss: 0.8751892447471619\n","Epoch no: 0/1, Batch no: 75/300, Loss: 1.0768753290176392\n","Epoch no: 0/1, Batch no: 76/300, Loss: 0.9796820878982544\n","Epoch no: 0/1, Batch no: 77/300, Loss: 0.9053326845169067\n","Epoch no: 0/1, Batch no: 78/300, Loss: 0.9011884331703186\n","Epoch no: 0/1, Batch no: 79/300, Loss: 0.9726691842079163\n","Epoch no: 0/1, Batch no: 80/300, Loss: 0.964302659034729\n","Epoch no: 0/1, Batch no: 81/300, Loss: 0.9118950366973877\n","Epoch no: 0/1, Batch no: 82/300, Loss: 0.9892919063568115\n","Epoch no: 0/1, Batch no: 83/300, Loss: 1.0047125816345215\n","Epoch no: 0/1, Batch no: 84/300, Loss: 1.016422152519226\n","Epoch no: 0/1, Batch no: 85/300, Loss: 0.9949862360954285\n","Epoch no: 0/1, Batch no: 86/300, Loss: 1.0308103561401367\n","Epoch no: 0/1, Batch no: 87/300, Loss: 1.0111502408981323\n","Epoch no: 0/1, Batch no: 88/300, Loss: 0.8937344551086426\n","Epoch no: 0/1, Batch no: 89/300, Loss: 0.9482889771461487\n","Epoch no: 0/1, Batch no: 90/300, Loss: 1.0669960975646973\n","Epoch no: 0/1, Batch no: 91/300, Loss: 0.9807513356208801\n","Epoch no: 0/1, Batch no: 92/300, Loss: 0.9794119000434875\n","Epoch no: 0/1, Batch no: 93/300, Loss: 0.8306607007980347\n","Epoch no: 0/1, Batch no: 94/300, Loss: 0.8784813284873962\n","Epoch no: 0/1, Batch no: 95/300, Loss: 1.0691590309143066\n","Epoch no: 0/1, Batch no: 96/300, Loss: 0.8969938158988953\n","Epoch no: 0/1, Batch no: 97/300, Loss: 1.0546236038208008\n","Epoch no: 0/1, Batch no: 98/300, Loss: 0.996699333190918\n","Epoch no: 0/1, Batch no: 99/300, Loss: 0.9036691784858704\n","Epoch no: 0/1, Batch no: 100/300, Loss: 0.9523768424987793\n","Epoch no: 0/1, Batch no: 101/300, Loss: 0.9656659364700317\n","Epoch no: 0/1, Batch no: 102/300, Loss: 0.8631290793418884\n","Epoch no: 0/1, Batch no: 103/300, Loss: 1.0798773765563965\n","Epoch no: 0/1, Batch no: 104/300, Loss: 1.013053297996521\n","Epoch no: 0/1, Batch no: 105/300, Loss: 0.9882407784461975\n","Epoch no: 0/1, Batch no: 106/300, Loss: 0.9759179949760437\n","Epoch no: 0/1, Batch no: 107/300, Loss: 0.9998224377632141\n","Epoch no: 0/1, Batch no: 108/300, Loss: 0.7891947627067566\n","Epoch no: 0/1, Batch no: 109/300, Loss: 1.1060057878494263\n","Epoch no: 0/1, Batch no: 110/300, Loss: 0.9193296432495117\n","Epoch no: 0/1, Batch no: 111/300, Loss: 1.0275806188583374\n","Epoch no: 0/1, Batch no: 112/300, Loss: 0.7892428636550903\n","Epoch no: 0/1, Batch no: 113/300, Loss: 0.8993037343025208\n","Epoch no: 0/1, Batch no: 114/300, Loss: 1.0478227138519287\n","Epoch no: 0/1, Batch no: 115/300, Loss: 1.0334538221359253\n","Epoch no: 0/1, Batch no: 116/300, Loss: 1.0472902059555054\n","Epoch no: 0/1, Batch no: 117/300, Loss: 1.003717064857483\n","Epoch no: 0/1, Batch no: 118/300, Loss: 0.9298861026763916\n","Epoch no: 0/1, Batch no: 119/300, Loss: 1.0474313497543335\n","Epoch no: 0/1, Batch no: 120/300, Loss: 1.0614417791366577\n","Epoch no: 0/1, Batch no: 121/300, Loss: 0.9458794593811035\n","Epoch no: 0/1, Batch no: 122/300, Loss: 1.0336350202560425\n","Epoch no: 0/1, Batch no: 123/300, Loss: 0.860065758228302\n","Epoch no: 0/1, Batch no: 124/300, Loss: 0.8840335011482239\n","Epoch no: 0/1, Batch no: 125/300, Loss: 0.9765848517417908\n","Epoch no: 0/1, Batch no: 126/300, Loss: 1.0282917022705078\n","Epoch no: 0/1, Batch no: 127/300, Loss: 0.9172740578651428\n","Epoch no: 0/1, Batch no: 128/300, Loss: 1.0354092121124268\n","Epoch no: 0/1, Batch no: 129/300, Loss: 1.012343406677246\n","Epoch no: 0/1, Batch no: 130/300, Loss: 0.8741946220397949\n","Epoch no: 0/1, Batch no: 131/300, Loss: 0.9902068972587585\n","Epoch no: 0/1, Batch no: 132/300, Loss: 0.9438008069992065\n","Epoch no: 0/1, Batch no: 133/300, Loss: 1.0198450088500977\n","Epoch no: 0/1, Batch no: 134/300, Loss: 1.019827127456665\n","Epoch no: 0/1, Batch no: 135/300, Loss: 0.9252989888191223\n","Epoch no: 0/1, Batch no: 136/300, Loss: 0.9278513789176941\n","Epoch no: 0/1, Batch no: 137/300, Loss: 0.9461024403572083\n","Epoch no: 0/1, Batch no: 138/300, Loss: 0.906196653842926\n","Epoch no: 0/1, Batch no: 139/300, Loss: 0.85894376039505\n","Epoch no: 0/1, Batch no: 140/300, Loss: 0.9914641976356506\n","Epoch no: 0/1, Batch no: 141/300, Loss: 0.9546418190002441\n","Epoch no: 0/1, Batch no: 142/300, Loss: 1.0243679285049438\n","Epoch no: 0/1, Batch no: 143/300, Loss: 0.854417085647583\n","Epoch no: 0/1, Batch no: 144/300, Loss: 0.938080370426178\n","Epoch no: 0/1, Batch no: 145/300, Loss: 0.8501198291778564\n","Epoch no: 0/1, Batch no: 146/300, Loss: 0.9711795449256897\n","Epoch no: 0/1, Batch no: 147/300, Loss: 1.0492477416992188\n","Epoch no: 0/1, Batch no: 148/300, Loss: 1.0153898000717163\n","Epoch no: 0/1, Batch no: 149/300, Loss: 1.0433489084243774\n","Epoch no: 0/1, Batch no: 150/300, Loss: 1.021132230758667\n","Epoch no: 0/1, Batch no: 151/300, Loss: 0.8550981879234314\n","Epoch no: 0/1, Batch no: 152/300, Loss: 0.9986419677734375\n","Epoch no: 0/1, Batch no: 153/300, Loss: 0.9705119729042053\n","Epoch no: 0/1, Batch no: 154/300, Loss: 0.8367607593536377\n","Epoch no: 0/1, Batch no: 155/300, Loss: 0.9733434915542603\n","Epoch no: 0/1, Batch no: 156/300, Loss: 1.032589316368103\n","Epoch no: 0/1, Batch no: 157/300, Loss: 0.8926020860671997\n","Epoch no: 0/1, Batch no: 158/300, Loss: 1.0420832633972168\n","Epoch no: 0/1, Batch no: 159/300, Loss: 1.026910662651062\n","Epoch no: 0/1, Batch no: 160/300, Loss: 0.9830769300460815\n","Epoch no: 0/1, Batch no: 161/300, Loss: 1.0235308408737183\n","Epoch no: 0/1, Batch no: 162/300, Loss: 0.9415688514709473\n","Epoch no: 0/1, Batch no: 163/300, Loss: 0.9758842587471008\n","Epoch no: 0/1, Batch no: 164/300, Loss: 1.0226095914840698\n","Epoch no: 0/1, Batch no: 165/300, Loss: 0.8716506958007812\n","Epoch no: 0/1, Batch no: 166/300, Loss: 0.9534227848052979\n","Epoch no: 0/1, Batch no: 167/300, Loss: 0.9422572255134583\n","Epoch no: 0/1, Batch no: 168/300, Loss: 1.010434865951538\n","Epoch no: 0/1, Batch no: 169/300, Loss: 1.0532302856445312\n","Epoch no: 0/1, Batch no: 170/300, Loss: 0.892625093460083\n","Epoch no: 0/1, Batch no: 171/300, Loss: 1.0217405557632446\n","Epoch no: 0/1, Batch no: 172/300, Loss: 0.9183731079101562\n","Epoch no: 0/1, Batch no: 173/300, Loss: 0.8284134268760681\n","Epoch no: 0/1, Batch no: 174/300, Loss: 0.9965250492095947\n","Epoch no: 0/1, Batch no: 175/300, Loss: 1.0261635780334473\n","Epoch no: 0/1, Batch no: 176/300, Loss: 0.8667150139808655\n","Epoch no: 0/1, Batch no: 177/300, Loss: 1.0172309875488281\n","Epoch no: 0/1, Batch no: 178/300, Loss: 1.0350111722946167\n","Epoch no: 0/1, Batch no: 179/300, Loss: 1.0497242212295532\n","Epoch no: 0/1, Batch no: 180/300, Loss: 0.8479207754135132\n","Epoch no: 0/1, Batch no: 181/300, Loss: 0.9688006043434143\n","Epoch no: 0/1, Batch no: 182/300, Loss: 0.9557109475135803\n","Epoch no: 0/1, Batch no: 183/300, Loss: 1.0142197608947754\n","Epoch no: 0/1, Batch no: 184/300, Loss: 0.9991986155509949\n","Epoch no: 0/1, Batch no: 185/300, Loss: 0.828355073928833\n","Epoch no: 0/1, Batch no: 186/300, Loss: 1.070952296257019\n","Epoch no: 0/1, Batch no: 187/300, Loss: 1.0072094202041626\n","Epoch no: 0/1, Batch no: 188/300, Loss: 1.0617702007293701\n","Epoch no: 0/1, Batch no: 189/300, Loss: 0.9302952289581299\n","Epoch no: 0/1, Batch no: 190/300, Loss: 0.9224924445152283\n","Epoch no: 0/1, Batch no: 191/300, Loss: 0.8086186647415161\n","Epoch no: 0/1, Batch no: 192/300, Loss: 0.997499942779541\n","Epoch no: 0/1, Batch no: 193/300, Loss: 0.9491958618164062\n","Epoch no: 0/1, Batch no: 194/300, Loss: 0.9948183298110962\n","Epoch no: 0/1, Batch no: 195/300, Loss: 0.7922754883766174\n","Epoch no: 0/1, Batch no: 196/300, Loss: 0.7675509452819824\n","Epoch no: 0/1, Batch no: 197/300, Loss: 0.8262245059013367\n","Epoch no: 0/1, Batch no: 198/300, Loss: 0.9091435074806213\n","Epoch no: 0/1, Batch no: 199/300, Loss: 0.9953048825263977\n","Epoch no: 0/1, Batch no: 200/300, Loss: 1.0057048797607422\n","Epoch no: 0/1, Batch no: 201/300, Loss: 0.9268571734428406\n","Epoch no: 0/1, Batch no: 202/300, Loss: 1.0323911905288696\n","Epoch no: 0/1, Batch no: 203/300, Loss: 0.9800898432731628\n","Epoch no: 0/1, Batch no: 204/300, Loss: 1.033210277557373\n","Epoch no: 0/1, Batch no: 205/300, Loss: 0.8096550703048706\n","Epoch no: 0/1, Batch no: 206/300, Loss: 0.8660703301429749\n","Epoch no: 0/1, Batch no: 207/300, Loss: 0.7104898691177368\n","Epoch no: 0/1, Batch no: 208/300, Loss: 0.8611384630203247\n","Epoch no: 0/1, Batch no: 209/300, Loss: 1.078452706336975\n","Epoch no: 0/1, Batch no: 210/300, Loss: 0.9776568412780762\n","Epoch no: 0/1, Batch no: 211/300, Loss: 0.8101485371589661\n","Epoch no: 0/1, Batch no: 212/300, Loss: 1.0211783647537231\n","Epoch no: 0/1, Batch no: 213/300, Loss: 0.957139790058136\n","Epoch no: 0/1, Batch no: 214/300, Loss: 0.8314218521118164\n","Epoch no: 0/1, Batch no: 215/300, Loss: 0.9447682499885559\n","Epoch no: 0/1, Batch no: 216/300, Loss: 0.7928421497344971\n","Epoch no: 0/1, Batch no: 217/300, Loss: 0.7688171863555908\n","Epoch no: 0/1, Batch no: 218/300, Loss: 0.8822034001350403\n","Epoch no: 0/1, Batch no: 219/300, Loss: 0.8745349049568176\n","Epoch no: 0/1, Batch no: 220/300, Loss: 0.9269092082977295\n","Epoch no: 0/1, Batch no: 221/300, Loss: 0.8575002551078796\n","Epoch no: 0/1, Batch no: 222/300, Loss: 1.0412400960922241\n","Epoch no: 0/1, Batch no: 223/300, Loss: 1.0062646865844727\n","Epoch no: 0/1, Batch no: 224/300, Loss: 1.0041121244430542\n","Epoch no: 0/1, Batch no: 225/300, Loss: 0.9649640917778015\n","Epoch no: 0/1, Batch no: 226/300, Loss: 0.971454918384552\n","Epoch no: 0/1, Batch no: 227/300, Loss: 0.9004337787628174\n","Epoch no: 0/1, Batch no: 228/300, Loss: 0.8107757568359375\n","Epoch no: 0/1, Batch no: 229/300, Loss: 0.8101818561553955\n","Epoch no: 0/1, Batch no: 230/300, Loss: 0.865696907043457\n","Epoch no: 0/1, Batch no: 231/300, Loss: 0.9043222665786743\n","Epoch no: 0/1, Batch no: 232/300, Loss: 0.9108619689941406\n","Epoch no: 0/1, Batch no: 233/300, Loss: 0.9443318843841553\n","Epoch no: 0/1, Batch no: 234/300, Loss: 0.9776555299758911\n","Epoch no: 0/1, Batch no: 235/300, Loss: 0.862763524055481\n","Epoch no: 0/1, Batch no: 236/300, Loss: 0.8099839091300964\n","Epoch no: 0/1, Batch no: 237/300, Loss: 0.9919798970222473\n","Epoch no: 0/1, Batch no: 238/300, Loss: 1.0162899494171143\n","Epoch no: 0/1, Batch no: 239/300, Loss: 1.0646028518676758\n","Epoch no: 0/1, Batch no: 240/300, Loss: 1.000930905342102\n","Epoch no: 0/1, Batch no: 241/300, Loss: 0.9442795515060425\n","Epoch no: 0/1, Batch no: 242/300, Loss: 0.8873441219329834\n","Epoch no: 0/1, Batch no: 243/300, Loss: 1.0058027505874634\n","Epoch no: 0/1, Batch no: 244/300, Loss: 1.0413504838943481\n","Epoch no: 0/1, Batch no: 245/300, Loss: 0.8890634775161743\n","Epoch no: 0/1, Batch no: 246/300, Loss: 0.9060588479042053\n","Epoch no: 0/1, Batch no: 247/300, Loss: 0.9222376346588135\n","Epoch no: 0/1, Batch no: 248/300, Loss: 0.9830106496810913\n","Epoch no: 0/1, Batch no: 249/300, Loss: 0.8040977120399475\n","Epoch no: 0/1, Batch no: 250/300, Loss: 1.0042262077331543\n","Epoch no: 0/1, Batch no: 251/300, Loss: 0.8115590214729309\n","Epoch no: 0/1, Batch no: 252/300, Loss: 0.9470612406730652\n","Epoch no: 0/1, Batch no: 253/300, Loss: 0.9386111497879028\n","Epoch no: 0/1, Batch no: 254/300, Loss: 0.9529325366020203\n","Epoch no: 0/1, Batch no: 255/300, Loss: 0.9686267375946045\n","Epoch no: 0/1, Batch no: 256/300, Loss: 0.9195253849029541\n","Epoch no: 0/1, Batch no: 257/300, Loss: 0.8848316669464111\n","Epoch no: 0/1, Batch no: 258/300, Loss: 0.9325928092002869\n","Epoch no: 0/1, Batch no: 259/300, Loss: 0.925877034664154\n","Epoch no: 0/1, Batch no: 260/300, Loss: 0.7358593344688416\n","Epoch no: 0/1, Batch no: 261/300, Loss: 0.9983840584754944\n","Epoch no: 0/1, Batch no: 262/300, Loss: 0.9983317255973816\n","Epoch no: 0/1, Batch no: 263/300, Loss: 0.8810590505599976\n","Epoch no: 0/1, Batch no: 264/300, Loss: 0.7874747514724731\n","Epoch no: 0/1, Batch no: 265/300, Loss: 1.0203953981399536\n","Epoch no: 0/1, Batch no: 266/300, Loss: 0.9564574956893921\n","Epoch no: 0/1, Batch no: 267/300, Loss: 0.7257056832313538\n","Epoch no: 0/1, Batch no: 268/300, Loss: 0.720289409160614\n","Epoch no: 0/1, Batch no: 269/300, Loss: 0.7280313968658447\n","Epoch no: 0/1, Batch no: 270/300, Loss: 1.032639741897583\n","Epoch no: 0/1, Batch no: 271/300, Loss: 0.9831743836402893\n","Epoch no: 0/1, Batch no: 272/300, Loss: 0.9631165266036987\n","Epoch no: 0/1, Batch no: 273/300, Loss: 0.9075602293014526\n","Epoch no: 0/1, Batch no: 274/300, Loss: 0.9679427742958069\n","Epoch no: 0/1, Batch no: 275/300, Loss: 0.8748756051063538\n","Epoch no: 0/1, Batch no: 276/300, Loss: 0.9726361036300659\n","Epoch no: 0/1, Batch no: 277/300, Loss: 1.0199247598648071\n","Epoch no: 0/1, Batch no: 278/300, Loss: 0.8815242052078247\n","Epoch no: 0/1, Batch no: 279/300, Loss: 0.9733733534812927\n","Epoch no: 0/1, Batch no: 280/300, Loss: 0.875093400478363\n","Epoch no: 0/1, Batch no: 281/300, Loss: 0.8989462852478027\n","Epoch no: 0/1, Batch no: 282/300, Loss: 0.8513864874839783\n","Epoch no: 0/1, Batch no: 283/300, Loss: 0.9360126852989197\n","Epoch no: 0/1, Batch no: 284/300, Loss: 0.9348720908164978\n","Epoch no: 0/1, Batch no: 285/300, Loss: 0.9695091843605042\n","Epoch no: 0/1, Batch no: 286/300, Loss: 0.9098363518714905\n","Epoch no: 0/1, Batch no: 287/300, Loss: 1.0515422821044922\n","Epoch no: 0/1, Batch no: 288/300, Loss: 0.9818471074104309\n","Epoch no: 0/1, Batch no: 289/300, Loss: 0.8685785531997681\n","Epoch no: 0/1, Batch no: 290/300, Loss: 0.993208646774292\n","Epoch no: 0/1, Batch no: 291/300, Loss: 0.9957817792892456\n","Epoch no: 0/1, Batch no: 292/300, Loss: 1.0127620697021484\n","Epoch no: 0/1, Batch no: 293/300, Loss: 1.0115562677383423\n","Epoch no: 0/1, Batch no: 294/300, Loss: 0.9706565737724304\n","Epoch no: 0/1, Batch no: 295/300, Loss: 0.9787105917930603\n","Epoch no: 0/1, Batch no: 296/300, Loss: 0.9284233450889587\n","Epoch no: 0/1, Batch no: 297/300, Loss: 0.8957781195640564\n","Epoch no: 0/1, Batch no: 298/300, Loss: 1.0462065935134888\n","Epoch no: 0/1, Batch no: 299/300, Loss: 0.8574087023735046\n","Epoch no: 1/1, Batch no: 0/300, Loss: 1.0855783224105835\n","Epoch no: 1/1, Batch no: 1/300, Loss: 0.9131221175193787\n","Epoch no: 1/1, Batch no: 2/300, Loss: 0.7950915694236755\n","Epoch no: 1/1, Batch no: 3/300, Loss: 0.9367957711219788\n","Epoch no: 1/1, Batch no: 4/300, Loss: 0.828834056854248\n","Epoch no: 1/1, Batch no: 5/300, Loss: 1.0051381587982178\n","Epoch no: 1/1, Batch no: 6/300, Loss: 0.8608625531196594\n","Epoch no: 1/1, Batch no: 7/300, Loss: 0.803978443145752\n","Epoch no: 1/1, Batch no: 8/300, Loss: 0.9266754388809204\n","Epoch no: 1/1, Batch no: 9/300, Loss: 0.8278319835662842\n","Epoch no: 1/1, Batch no: 10/300, Loss: 0.8858620524406433\n","Epoch no: 1/1, Batch no: 11/300, Loss: 0.764515221118927\n","Epoch no: 1/1, Batch no: 12/300, Loss: 0.7676979303359985\n","Epoch no: 1/1, Batch no: 13/300, Loss: 0.9149813652038574\n","Epoch no: 1/1, Batch no: 14/300, Loss: 0.8122668862342834\n","Epoch no: 1/1, Batch no: 15/300, Loss: 0.9822704792022705\n","Epoch no: 1/1, Batch no: 16/300, Loss: 0.8613424301147461\n","Epoch no: 1/1, Batch no: 17/300, Loss: 0.6607636213302612\n","Epoch no: 1/1, Batch no: 18/300, Loss: 0.9382280707359314\n","Epoch no: 1/1, Batch no: 19/300, Loss: 0.9413378238677979\n","Epoch no: 1/1, Batch no: 20/300, Loss: 1.0157849788665771\n","Epoch no: 1/1, Batch no: 21/300, Loss: 0.9178764224052429\n","Epoch no: 1/1, Batch no: 22/300, Loss: 0.9845329523086548\n","Epoch no: 1/1, Batch no: 23/300, Loss: 0.9459494948387146\n","Epoch no: 1/1, Batch no: 24/300, Loss: 0.9751291275024414\n","Epoch no: 1/1, Batch no: 25/300, Loss: 0.8722610473632812\n","Epoch no: 1/1, Batch no: 26/300, Loss: 0.9958467483520508\n","Epoch no: 1/1, Batch no: 27/300, Loss: 0.8421623110771179\n","Epoch no: 1/1, Batch no: 28/300, Loss: 0.9640432596206665\n","Epoch no: 1/1, Batch no: 29/300, Loss: 0.9579586386680603\n","Epoch no: 1/1, Batch no: 30/300, Loss: 0.9138204455375671\n","Epoch no: 1/1, Batch no: 31/300, Loss: 0.9302915334701538\n","Epoch no: 1/1, Batch no: 32/300, Loss: 0.9701224565505981\n","Epoch no: 1/1, Batch no: 33/300, Loss: 0.9542928338050842\n","Epoch no: 1/1, Batch no: 34/300, Loss: 0.8626307249069214\n","Epoch no: 1/1, Batch no: 35/300, Loss: 1.027609944343567\n","Epoch no: 1/1, Batch no: 36/300, Loss: 0.7425867319107056\n","Epoch no: 1/1, Batch no: 37/300, Loss: 0.832172691822052\n","Epoch no: 1/1, Batch no: 38/300, Loss: 0.9983497262001038\n","Epoch no: 1/1, Batch no: 39/300, Loss: 1.012222170829773\n","Epoch no: 1/1, Batch no: 40/300, Loss: 0.9072314500808716\n","Epoch no: 1/1, Batch no: 41/300, Loss: 0.9928624629974365\n","Epoch no: 1/1, Batch no: 42/300, Loss: 0.8825584650039673\n","Epoch no: 1/1, Batch no: 43/300, Loss: 0.9620664715766907\n","Epoch no: 1/1, Batch no: 44/300, Loss: 1.0059552192687988\n","Epoch no: 1/1, Batch no: 45/300, Loss: 0.9247848987579346\n","Epoch no: 1/1, Batch no: 46/300, Loss: 0.9665318131446838\n","Epoch no: 1/1, Batch no: 47/300, Loss: 0.7687073349952698\n","Epoch no: 1/1, Batch no: 48/300, Loss: 0.8669577836990356\n","Epoch no: 1/1, Batch no: 49/300, Loss: 1.0107169151306152\n","Epoch no: 1/1, Batch no: 50/300, Loss: 0.8523114323616028\n","Epoch no: 1/1, Batch no: 51/300, Loss: 0.9883095622062683\n","Epoch no: 1/1, Batch no: 52/300, Loss: 0.8983216285705566\n","Epoch no: 1/1, Batch no: 53/300, Loss: 0.965583860874176\n","Epoch no: 1/1, Batch no: 54/300, Loss: 0.950930118560791\n","Epoch no: 1/1, Batch no: 55/300, Loss: 0.912440299987793\n","Epoch no: 1/1, Batch no: 56/300, Loss: 0.8683096170425415\n","Epoch no: 1/1, Batch no: 57/300, Loss: 0.80629962682724\n","Epoch no: 1/1, Batch no: 58/300, Loss: 0.9164677262306213\n","Epoch no: 1/1, Batch no: 59/300, Loss: 0.9558677077293396\n","Epoch no: 1/1, Batch no: 60/300, Loss: 0.9649561643600464\n","Epoch no: 1/1, Batch no: 61/300, Loss: 0.9674374461174011\n","Epoch no: 1/1, Batch no: 62/300, Loss: 0.7781248092651367\n","Epoch no: 1/1, Batch no: 63/300, Loss: 0.9459390640258789\n","Epoch no: 1/1, Batch no: 64/300, Loss: 0.9538753032684326\n","Epoch no: 1/1, Batch no: 65/300, Loss: 0.8546552658081055\n","Epoch no: 1/1, Batch no: 66/300, Loss: 0.7694497108459473\n","Epoch no: 1/1, Batch no: 67/300, Loss: 0.9509423971176147\n","Epoch no: 1/1, Batch no: 68/300, Loss: 0.8036565780639648\n","Epoch no: 1/1, Batch no: 69/300, Loss: 0.9491655230522156\n","Epoch no: 1/1, Batch no: 70/300, Loss: 0.9501520991325378\n","Epoch no: 1/1, Batch no: 71/300, Loss: 0.9463559985160828\n","Epoch no: 1/1, Batch no: 72/300, Loss: 0.9334354996681213\n","Epoch no: 1/1, Batch no: 73/300, Loss: 0.8936858773231506\n","Epoch no: 1/1, Batch no: 74/300, Loss: 0.9505606889724731\n","Epoch no: 1/1, Batch no: 75/300, Loss: 0.7083446979522705\n","Epoch no: 1/1, Batch no: 76/300, Loss: 0.8969711065292358\n","Epoch no: 1/1, Batch no: 77/300, Loss: 0.9092265367507935\n","Epoch no: 1/1, Batch no: 78/300, Loss: 0.9468592405319214\n","Epoch no: 1/1, Batch no: 79/300, Loss: 0.925690233707428\n","Epoch no: 1/1, Batch no: 80/300, Loss: 0.7867127060890198\n","Epoch no: 1/1, Batch no: 81/300, Loss: 0.9769907593727112\n","Epoch no: 1/1, Batch no: 82/300, Loss: 0.9809016585350037\n","Epoch no: 1/1, Batch no: 83/300, Loss: 0.8856648206710815\n","Epoch no: 1/1, Batch no: 84/300, Loss: 0.9397609233856201\n","Epoch no: 1/1, Batch no: 85/300, Loss: 0.8798312544822693\n","Epoch no: 1/1, Batch no: 86/300, Loss: 0.9524298906326294\n","Epoch no: 1/1, Batch no: 87/300, Loss: 0.9835258722305298\n","Epoch no: 1/1, Batch no: 88/300, Loss: 0.9319712519645691\n","Epoch no: 1/1, Batch no: 89/300, Loss: 1.0084559917449951\n","Epoch no: 1/1, Batch no: 90/300, Loss: 0.8768429160118103\n","Epoch no: 1/1, Batch no: 91/300, Loss: 0.7757426500320435\n","Epoch no: 1/1, Batch no: 92/300, Loss: 1.0341897010803223\n","Epoch no: 1/1, Batch no: 93/300, Loss: 0.7594242691993713\n","Epoch no: 1/1, Batch no: 94/300, Loss: 0.8909803628921509\n","Epoch no: 1/1, Batch no: 95/300, Loss: 0.7622406482696533\n","Epoch no: 1/1, Batch no: 96/300, Loss: 0.7614948749542236\n","Epoch no: 1/1, Batch no: 97/300, Loss: 1.0123478174209595\n","Epoch no: 1/1, Batch no: 98/300, Loss: 0.8033139109611511\n","Epoch no: 1/1, Batch no: 99/300, Loss: 0.871427595615387\n","Epoch no: 1/1, Batch no: 100/300, Loss: 0.9789930582046509\n","Epoch no: 1/1, Batch no: 101/300, Loss: 0.8166148066520691\n","Epoch no: 1/1, Batch no: 102/300, Loss: 0.7757435441017151\n","Epoch no: 1/1, Batch no: 103/300, Loss: 0.9050871133804321\n","Epoch no: 1/1, Batch no: 104/300, Loss: 0.9405890703201294\n","Epoch no: 1/1, Batch no: 105/300, Loss: 0.9237241744995117\n","Epoch no: 1/1, Batch no: 106/300, Loss: 0.9982190728187561\n","Epoch no: 1/1, Batch no: 107/300, Loss: 0.8123627305030823\n","Epoch no: 1/1, Batch no: 108/300, Loss: 0.9524414539337158\n","Epoch no: 1/1, Batch no: 109/300, Loss: 0.8866541385650635\n","Epoch no: 1/1, Batch no: 110/300, Loss: 0.9499176740646362\n","Epoch no: 1/1, Batch no: 111/300, Loss: 0.8554425239562988\n","Epoch no: 1/1, Batch no: 112/300, Loss: 1.0694020986557007\n","Epoch no: 1/1, Batch no: 113/300, Loss: 0.990348219871521\n","Epoch no: 1/1, Batch no: 114/300, Loss: 0.7246127128601074\n","Epoch no: 1/1, Batch no: 115/300, Loss: 0.920780599117279\n","Epoch no: 1/1, Batch no: 116/300, Loss: 1.0345776081085205\n","Epoch no: 1/1, Batch no: 117/300, Loss: 0.895659327507019\n","Epoch no: 1/1, Batch no: 118/300, Loss: 0.9065943956375122\n","Epoch no: 1/1, Batch no: 119/300, Loss: 0.7683606743812561\n","Epoch no: 1/1, Batch no: 120/300, Loss: 0.9378656148910522\n","Epoch no: 1/1, Batch no: 121/300, Loss: 0.7317986488342285\n","Epoch no: 1/1, Batch no: 122/300, Loss: 0.9457860589027405\n","Epoch no: 1/1, Batch no: 123/300, Loss: 0.8520339131355286\n","Epoch no: 1/1, Batch no: 124/300, Loss: 0.8958098888397217\n","Epoch no: 1/1, Batch no: 125/300, Loss: 0.9409967660903931\n","Epoch no: 1/1, Batch no: 126/300, Loss: 0.8946595191955566\n","Epoch no: 1/1, Batch no: 127/300, Loss: 0.8949003219604492\n","Epoch no: 1/1, Batch no: 128/300, Loss: 1.0298051834106445\n","Epoch no: 1/1, Batch no: 129/300, Loss: 0.8786280155181885\n","Epoch no: 1/1, Batch no: 130/300, Loss: 0.9427168965339661\n","Epoch no: 1/1, Batch no: 131/300, Loss: 0.9954292178153992\n","Epoch no: 1/1, Batch no: 132/300, Loss: 0.7836425304412842\n","Epoch no: 1/1, Batch no: 133/300, Loss: 0.7390334606170654\n","Epoch no: 1/1, Batch no: 134/300, Loss: 0.979927122592926\n","Epoch no: 1/1, Batch no: 135/300, Loss: 0.7503692507743835\n","Epoch no: 1/1, Batch no: 136/300, Loss: 0.8923963308334351\n","Epoch no: 1/1, Batch no: 137/300, Loss: 0.9652312397956848\n","Epoch no: 1/1, Batch no: 138/300, Loss: 0.9568678140640259\n","Epoch no: 1/1, Batch no: 139/300, Loss: 1.0265557765960693\n","Epoch no: 1/1, Batch no: 140/300, Loss: 0.8262913227081299\n","Epoch no: 1/1, Batch no: 141/300, Loss: 0.924505352973938\n","Epoch no: 1/1, Batch no: 142/300, Loss: 1.0111100673675537\n","Epoch no: 1/1, Batch no: 143/300, Loss: 0.7951538562774658\n","Epoch no: 1/1, Batch no: 144/300, Loss: 0.9158604741096497\n","Epoch no: 1/1, Batch no: 145/300, Loss: 0.8417300581932068\n","Epoch no: 1/1, Batch no: 146/300, Loss: 0.7383780479431152\n","Epoch no: 1/1, Batch no: 147/300, Loss: 0.8838976621627808\n","Epoch no: 1/1, Batch no: 148/300, Loss: 0.9629400372505188\n","Epoch no: 1/1, Batch no: 149/300, Loss: 0.9371331930160522\n","Epoch no: 1/1, Batch no: 150/300, Loss: 0.8261798024177551\n","Epoch no: 1/1, Batch no: 151/300, Loss: 0.7005469799041748\n","Epoch no: 1/1, Batch no: 152/300, Loss: 0.9752262830734253\n","Epoch no: 1/1, Batch no: 153/300, Loss: 0.9187649488449097\n","Epoch no: 1/1, Batch no: 154/300, Loss: 1.032107949256897\n","Epoch no: 1/1, Batch no: 155/300, Loss: 0.9692304730415344\n","Epoch no: 1/1, Batch no: 156/300, Loss: 0.9905393123626709\n","Epoch no: 1/1, Batch no: 157/300, Loss: 0.899355411529541\n","Epoch no: 1/1, Batch no: 158/300, Loss: 0.8638694286346436\n","Epoch no: 1/1, Batch no: 159/300, Loss: 0.8777758479118347\n","Epoch no: 1/1, Batch no: 160/300, Loss: 0.7550240159034729\n","Epoch no: 1/1, Batch no: 161/300, Loss: 0.8381381630897522\n","Epoch no: 1/1, Batch no: 162/300, Loss: 0.8969447016716003\n","Epoch no: 1/1, Batch no: 163/300, Loss: 0.8983263373374939\n","Epoch no: 1/1, Batch no: 164/300, Loss: 0.7138827443122864\n","Epoch no: 1/1, Batch no: 165/300, Loss: 0.8537196516990662\n","Epoch no: 1/1, Batch no: 166/300, Loss: 0.7669131755828857\n","Epoch no: 1/1, Batch no: 167/300, Loss: 0.8474303483963013\n","Epoch no: 1/1, Batch no: 168/300, Loss: 0.8836555480957031\n","Epoch no: 1/1, Batch no: 169/300, Loss: 0.8585271239280701\n","Epoch no: 1/1, Batch no: 170/300, Loss: 0.9388519525527954\n","Epoch no: 1/1, Batch no: 171/300, Loss: 0.7726666927337646\n","Epoch no: 1/1, Batch no: 172/300, Loss: 0.7852992415428162\n","Epoch no: 1/1, Batch no: 173/300, Loss: 0.9282903671264648\n","Epoch no: 1/1, Batch no: 174/300, Loss: 0.874351978302002\n","Epoch no: 1/1, Batch no: 175/300, Loss: 0.8172309398651123\n","Epoch no: 1/1, Batch no: 176/300, Loss: 0.8460373878479004\n","Epoch no: 1/1, Batch no: 177/300, Loss: 0.8956525325775146\n","Epoch no: 1/1, Batch no: 178/300, Loss: 0.9210734367370605\n","Epoch no: 1/1, Batch no: 179/300, Loss: 0.8580001592636108\n","Epoch no: 1/1, Batch no: 180/300, Loss: 0.9666725397109985\n","Epoch no: 1/1, Batch no: 181/300, Loss: 0.7782303690910339\n","Epoch no: 1/1, Batch no: 182/300, Loss: 0.7895339727401733\n","Epoch no: 1/1, Batch no: 183/300, Loss: 0.9859001636505127\n","Epoch no: 1/1, Batch no: 184/300, Loss: 0.8300786018371582\n","Epoch no: 1/1, Batch no: 185/300, Loss: 0.6807716488838196\n","Epoch no: 1/1, Batch no: 186/300, Loss: 0.949543297290802\n","Epoch no: 1/1, Batch no: 187/300, Loss: 0.9675723314285278\n","Epoch no: 1/1, Batch no: 188/300, Loss: 0.8043718934059143\n","Epoch no: 1/1, Batch no: 189/300, Loss: 1.0181910991668701\n","Epoch no: 1/1, Batch no: 190/300, Loss: 0.934589147567749\n","Epoch no: 1/1, Batch no: 191/300, Loss: 0.9454279541969299\n","Epoch no: 1/1, Batch no: 192/300, Loss: 0.956989586353302\n","Epoch no: 1/1, Batch no: 193/300, Loss: 0.6805729269981384\n","Epoch no: 1/1, Batch no: 194/300, Loss: 0.8844782114028931\n","Epoch no: 1/1, Batch no: 195/300, Loss: 0.9158093929290771\n","Epoch no: 1/1, Batch no: 196/300, Loss: 0.8529700636863708\n","Epoch no: 1/1, Batch no: 197/300, Loss: 0.7211427688598633\n","Epoch no: 1/1, Batch no: 198/300, Loss: 0.8183358907699585\n","Epoch no: 1/1, Batch no: 199/300, Loss: 0.8721713423728943\n","Epoch no: 1/1, Batch no: 200/300, Loss: 1.0570423603057861\n","Epoch no: 1/1, Batch no: 201/300, Loss: 0.6964507102966309\n","Epoch no: 1/1, Batch no: 202/300, Loss: 0.9372826218605042\n","Epoch no: 1/1, Batch no: 203/300, Loss: 0.834131121635437\n","Epoch no: 1/1, Batch no: 204/300, Loss: 0.9221051335334778\n","Epoch no: 1/1, Batch no: 205/300, Loss: 0.8697556257247925\n","Epoch no: 1/1, Batch no: 206/300, Loss: 0.7428839802742004\n","Epoch no: 1/1, Batch no: 207/300, Loss: 0.9852340221405029\n","Epoch no: 1/1, Batch no: 208/300, Loss: 0.932684063911438\n","Epoch no: 1/1, Batch no: 209/300, Loss: 0.740753173828125\n","Epoch no: 1/1, Batch no: 210/300, Loss: 0.8792450428009033\n","Epoch no: 1/1, Batch no: 211/300, Loss: 0.9461893439292908\n","Epoch no: 1/1, Batch no: 212/300, Loss: 0.8778574466705322\n","Epoch no: 1/1, Batch no: 213/300, Loss: 0.6425172090530396\n","Epoch no: 1/1, Batch no: 214/300, Loss: 1.023867130279541\n","Epoch no: 1/1, Batch no: 215/300, Loss: 0.9472136497497559\n","Epoch no: 1/1, Batch no: 216/300, Loss: 0.8269762992858887\n","Epoch no: 1/1, Batch no: 217/300, Loss: 0.7615348100662231\n","Epoch no: 1/1, Batch no: 218/300, Loss: 0.7522109746932983\n","Epoch no: 1/1, Batch no: 219/300, Loss: 0.928838849067688\n","Epoch no: 1/1, Batch no: 220/300, Loss: 0.8729390501976013\n","Epoch no: 1/1, Batch no: 221/300, Loss: 0.8822900652885437\n","Epoch no: 1/1, Batch no: 222/300, Loss: 0.8041671514511108\n","Epoch no: 1/1, Batch no: 223/300, Loss: 0.8985631465911865\n","Epoch no: 1/1, Batch no: 224/300, Loss: 0.8404350876808167\n","Epoch no: 1/1, Batch no: 225/300, Loss: 0.8714812397956848\n","Epoch no: 1/1, Batch no: 226/300, Loss: 0.7283821702003479\n","Epoch no: 1/1, Batch no: 227/300, Loss: 0.7800726890563965\n","Epoch no: 1/1, Batch no: 228/300, Loss: 0.7845049500465393\n","Epoch no: 1/1, Batch no: 229/300, Loss: 0.5984185934066772\n","Epoch no: 1/1, Batch no: 230/300, Loss: 1.004597783088684\n","Epoch no: 1/1, Batch no: 231/300, Loss: 0.6581255793571472\n","Epoch no: 1/1, Batch no: 232/300, Loss: 0.9319995045661926\n","Epoch no: 1/1, Batch no: 233/300, Loss: 0.7931050658226013\n","Epoch no: 1/1, Batch no: 234/300, Loss: 0.825964093208313\n","Epoch no: 1/1, Batch no: 235/300, Loss: 0.6447062492370605\n","Epoch no: 1/1, Batch no: 236/300, Loss: 0.7616833448410034\n","Epoch no: 1/1, Batch no: 237/300, Loss: 0.8964104056358337\n","Epoch no: 1/1, Batch no: 238/300, Loss: 0.6716451048851013\n","Epoch no: 1/1, Batch no: 239/300, Loss: 0.9232569932937622\n","Epoch no: 1/1, Batch no: 240/300, Loss: 0.8284719586372375\n","Epoch no: 1/1, Batch no: 241/300, Loss: 0.9500246047973633\n","Epoch no: 1/1, Batch no: 242/300, Loss: 0.8007214665412903\n","Epoch no: 1/1, Batch no: 243/300, Loss: 1.0056049823760986\n","Epoch no: 1/1, Batch no: 244/300, Loss: 0.8218721151351929\n","Epoch no: 1/1, Batch no: 245/300, Loss: 0.8798876404762268\n","Epoch no: 1/1, Batch no: 246/300, Loss: 0.8083341717720032\n","Epoch no: 1/1, Batch no: 247/300, Loss: 0.8610149025917053\n","Epoch no: 1/1, Batch no: 248/300, Loss: 0.9374197125434875\n","Epoch no: 1/1, Batch no: 249/300, Loss: 0.8651695251464844\n","Epoch no: 1/1, Batch no: 250/300, Loss: 0.876517653465271\n","Epoch no: 1/1, Batch no: 251/300, Loss: 0.7632665634155273\n","Epoch no: 1/1, Batch no: 252/300, Loss: 0.9012136459350586\n","Epoch no: 1/1, Batch no: 253/300, Loss: 0.731229305267334\n","Epoch no: 1/1, Batch no: 254/300, Loss: 1.0059096813201904\n","Epoch no: 1/1, Batch no: 255/300, Loss: 0.9959266781806946\n","Epoch no: 1/1, Batch no: 256/300, Loss: 0.8502326011657715\n","Epoch no: 1/1, Batch no: 257/300, Loss: 0.6387515664100647\n","Epoch no: 1/1, Batch no: 258/300, Loss: 0.9574004411697388\n","Epoch no: 1/1, Batch no: 259/300, Loss: 0.8467260003089905\n","Epoch no: 1/1, Batch no: 260/300, Loss: 0.6720443964004517\n","Epoch no: 1/1, Batch no: 261/300, Loss: 0.9525400400161743\n","Epoch no: 1/1, Batch no: 262/300, Loss: 0.845268964767456\n","Epoch no: 1/1, Batch no: 263/300, Loss: 0.8664411306381226\n","Epoch no: 1/1, Batch no: 264/300, Loss: 0.9245361089706421\n","Epoch no: 1/1, Batch no: 265/300, Loss: 0.8659006953239441\n","Epoch no: 1/1, Batch no: 266/300, Loss: 0.7676281332969666\n","Epoch no: 1/1, Batch no: 267/300, Loss: 0.9623299241065979\n","Epoch no: 1/1, Batch no: 268/300, Loss: 0.8692886233329773\n","Epoch no: 1/1, Batch no: 269/300, Loss: 0.9357925653457642\n","Epoch no: 1/1, Batch no: 270/300, Loss: 0.932130753993988\n","Epoch no: 1/1, Batch no: 271/300, Loss: 0.9510976672172546\n","Epoch no: 1/1, Batch no: 272/300, Loss: 0.9671413898468018\n","Epoch no: 1/1, Batch no: 273/300, Loss: 0.8484273552894592\n","Epoch no: 1/1, Batch no: 274/300, Loss: 0.9138317704200745\n","Epoch no: 1/1, Batch no: 275/300, Loss: 0.7554017901420593\n","Epoch no: 1/1, Batch no: 276/300, Loss: 0.8178325295448303\n","Epoch no: 1/1, Batch no: 277/300, Loss: 0.9500274658203125\n","Epoch no: 1/1, Batch no: 278/300, Loss: 0.7004340887069702\n","Epoch no: 1/1, Batch no: 279/300, Loss: 0.9385025501251221\n","Epoch no: 1/1, Batch no: 280/300, Loss: 0.9135698080062866\n","Epoch no: 1/1, Batch no: 281/300, Loss: 0.9696253538131714\n","Epoch no: 1/1, Batch no: 282/300, Loss: 0.7377152442932129\n","Epoch no: 1/1, Batch no: 283/300, Loss: 0.9295579791069031\n","Epoch no: 1/1, Batch no: 284/300, Loss: 0.784563422203064\n","Epoch no: 1/1, Batch no: 285/300, Loss: 0.763130784034729\n","Epoch no: 1/1, Batch no: 286/300, Loss: 0.8665397763252258\n","Epoch no: 1/1, Batch no: 287/300, Loss: 0.8827013969421387\n","Epoch no: 1/1, Batch no: 288/300, Loss: 1.0001707077026367\n","Epoch no: 1/1, Batch no: 289/300, Loss: 0.8331896662712097\n","Epoch no: 1/1, Batch no: 290/300, Loss: 0.7882770895957947\n","Epoch no: 1/1, Batch no: 291/300, Loss: 0.9376698136329651\n","Epoch no: 1/1, Batch no: 292/300, Loss: 0.9212266802787781\n","Epoch no: 1/1, Batch no: 293/300, Loss: 0.6169424057006836\n","Epoch no: 1/1, Batch no: 294/300, Loss: 1.0052919387817383\n","Epoch no: 1/1, Batch no: 295/300, Loss: 0.9009132385253906\n","Epoch no: 1/1, Batch no: 296/300, Loss: 0.8566926717758179\n","Epoch no: 1/1, Batch no: 297/300, Loss: 0.8124542236328125\n","Epoch no: 1/1, Batch no: 298/300, Loss: 0.9183183908462524\n","Epoch no: 1/1, Batch no: 299/300, Loss: 0.7914906144142151\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ozAq3pQPw3wC"},"source":["import torch\n","torch.save(model,'/content/gdrive/My Drive/outPut_1/model2.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wQH3XNINf3k-"},"source":["from argparse import ArgumentParser, Namespace\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":350},"id":"AK-ZDsfX3q5x","executionInfo":{"status":"error","timestamp":1612507231329,"user_tz":-330,"elapsed":783,"user":{"displayName":"Mohit Tuli","photoUrl":"","userId":"05250823286326681737"}},"outputId":"c90a99a9-03da-4d36-f11a-93c0f459ef75"},"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","import pytorch_lightning as pl\n","\n","class DataModule(pl.LightningDataModule):\n","\n","    def __init__(self):\n","        super().__init__()\n","    \n","    def setup(self, stage):\n","        # called on each gpu\n","        n = 128\n","        x = torch.randn(n, 1, 64,64)\n","        data = list(zip(x,x))\n","        self.test  = DataLoader(data, batch_size=32)\n","        self.train = DataLoader(data, batch_size=32)\n","        self.val   = DataLoader(data, batch_size=32)\n","        \n","    def train_dataloader(self):\n","        return self.train\n","\n","    def val_dataloader(self):\n","        return self.val\n","\n","    def test_dataloader(self):\n","        return self.test\n","class SegModel(pl.LightningModule):\n","    def __init__(self):\n","        super(SegModel, self).__init__()\n","        self.batch_size = 4\n","        self.learning_rate = 1e-3\n","#         self.net = torchvision.models.segmentation.fcn_resnet50(pretrained = False, progress = True, num_classes = 19)\n","#         self.net = UNet(num_classes = 19, bilinear = False)\n","#         self.net = torchvision.models.segmentation.deeplabv3_resnet50(pretrained = False, progress = True, num_classes = 19)\n","        self.net = ENet(num_classes = 19)\n","        self.transform = transforms.Compose([\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean = [0.35675976, 0.37380189, 0.3764753], std = [0.32064945, 0.32098866, 0.32325324])\n","        ])\n","        self.trainset = semantic_dataset(split = 'train', transform = self.transform)\n","        self.testset = semantic_dataset(split = 'test', transform = self.transform)\n","        \n","    def forward(self, x):\n","        return self.net(x)\n","    \n","    def training_step(self, batch, batch_nb) :\n","        img, mask = batch\n","        img = img.float()\n","        mask = mask.long()\n","        out = self.forward(img)\n","        loss_val = F.cross_entropy(out, mask, ignore_index = 250)\n","#         print(loss.shape)\n","        return {'loss' : loss_val}\n","    \n","    def configure_optimizers(self):\n","        opt = torch.optim.Adam(self.net.parameters(), lr = self.learning_rate)\n","        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max = 10)\n","        return [opt], [sch]\n","    \n","    def train_dataloader(self):\n","        return DataLoader(self.trainset, batch_size = self.batch_size, shuffle = True)\n","    \n","    def test_dataloader(self):\n","        return DataLoader(self.testset, batch_size = 1, shuffle = True)\n","\n","dm = DataModule()\n","model = SegModel()\n","trainer = pl.Trainer(gpus=1, \n","                     distributed_backend=\"dp\",\n","                     max_epochs=1,\n","                    )\n","trainer.fit(model, dm)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-cb22057b0a14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mdm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataModule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSegModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m trainer = pl.Trainer(gpus=1, \n\u001b[1;32m     71\u001b[0m                      \u001b[0mdistributed_backend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"dp\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-6-cb22057b0a14>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;31m#         self.net = UNet(num_classes = 19, bilinear = False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m#         self.net = torchvision.models.segmentation.deeplabv3_resnet50(pretrained = False, progress = True, num_classes = 19)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mENet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m19\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         self.transform = transforms.Compose([\n\u001b[1;32m     39\u001b[0m             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'ENet' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DUNYTMLL3xLz","executionInfo":{"status":"ok","timestamp":1612506869026,"user_tz":-330,"elapsed":3071,"user":{"displayName":"Mohit Tuli","photoUrl":"","userId":"05250823286326681737"}},"outputId":"a4ed114d-778d-4c4e-84d4-1869574811fe"},"source":["!pip install pytorch_lightning"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: pytorch_lightning in /usr/local/lib/python3.6/dist-packages (1.1.7)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.19.5)\n","Requirement already satisfied: PyYAML!=5.4.*,>=5.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (5.3.1)\n","Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (0.18.2)\n","Requirement already satisfied: fsspec[http]>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (0.8.5)\n","Requirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (2.4.1)\n","Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (4.41.1)\n","Requirement already satisfied: torch>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch_lightning) (1.7.0+cu101)\n","Requirement already satisfied: requests; extra == \"http\" in /usr/local/lib/python3.6/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning) (2.23.0)\n","Requirement already satisfied: aiohttp; extra == \"http\" in /usr/local/lib/python3.6/dist-packages (from fsspec[http]>=0.8.1->pytorch_lightning) (3.7.3)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.10.0)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.2)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.32.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.36.2)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.12.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.3)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.24.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (53.0.0)\n","Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch_lightning) (0.8)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from torch>=1.3->pytorch_lightning) (3.7.4.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (2020.12.5)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (3.0.4)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (5.1.0)\n","Requirement already satisfied: idna-ssl>=1.0; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (1.1.0)\n","Requirement already satisfied: async-timeout<4.0,>=3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (3.0.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (20.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.6/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch_lightning) (1.6.3)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.4.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.7)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.1.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.4.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zrGEuKTR4FI-"},"source":[""],"execution_count":null,"outputs":[]}]}