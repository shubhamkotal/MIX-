{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LM_10layer.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"sE3Oy-IAOHTh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600663219079,"user_tz":-330,"elapsed":36055,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"dc80b843-4d25-4a00-b09e-dbfe0f5077e8"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PTkyimHpOcAT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600663279703,"user_tz":-330,"elapsed":2186,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"083eb416-18dc-4835-ed18-2c917a511746"},"source":["%cd /content/gdrive/My Drive/unilm"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/gdrive/.shortcut-targets-by-id/19rY6aGdoyGH5C22rAwdx79e5pwrlXHil/unilm\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zIoHzVydOzJm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":855},"executionInfo":{"status":"ok","timestamp":1600663292006,"user_tz":-330,"elapsed":11086,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"ae5a8780-631d-429e-c15c-64316500ac20"},"source":["!pip install seqeval transformers==2.9.0 tensorboardX"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting seqeval\n","  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n","Collecting transformers==2.9.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n","\u001b[K     |████████████████████████████████| 645kB 9.4MB/s \n","\u001b[?25hCollecting tensorboardX\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n","\u001b[K     |████████████████████████████████| 317kB 31.7MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n","Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.4.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (4.41.1)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 34.3MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (2019.12.20)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (3.0.12)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (0.7)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 56.4MB/s \n","\u001b[?25hCollecting tokenizers==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 57.7MB/s \n","\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.0) (0.16.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (3.0.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.0)\n","Building wheels for collected packages: seqeval, sacremoses\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7423 sha256=8f6a926590013dc99cfc414d541a72495241c846fb0e3f0ff349a801cc29f478\n","  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=f6713f03d04629c0db0cbde6e3857ba85a7bedd57170b3307bfa3c5e20c3bffc\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built seqeval sacremoses\n","Installing collected packages: seqeval, sacremoses, sentencepiece, tokenizers, transformers, tensorboardX\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 seqeval-0.0.12 tensorboardX-2.1 tokenizers-0.7.0 transformers-2.9.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tWE3YkBgR8VG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600663292007,"user_tz":-330,"elapsed":3967,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"837b844f-986a-4a50-a375-4a6946a183cd"},"source":["%cd \"/content/gdrive/My Drive/unilm/layoutlm/examples/seq_labeling/\""],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/gdrive/.shortcut-targets-by-id/19rY6aGdoyGH5C22rAwdx79e5pwrlXHil/unilm/layoutlm/examples/seq_labeling\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"adKoTEKBRT_4","colab_type":"text"},"source":["*preprocessing* as per funsd\n"]},{"cell_type":"code","metadata":{"id":"HDtI2ccHzOKX","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600663301597,"user_tz":-330,"elapsed":8096,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["import argparse\n","import json\n","import os\n","\n","from PIL import Image\n","from transformers import AutoTokenizer\n","\n","\n","def bbox_string(box, width, length):\n","    return (\n","        str(int(1000 * (int(box[0]) / width)))\n","        + \" \"\n","        + str(int(1000 * (int(box[1])/ length)))\n","        + \" \"\n","        + str(int(1000 * (int(box[2]) / width)))\n","        + \" \"\n","        + str(int(1000 * (int(box[3]) / length)))\n","    )\n","\n","\n","def actual_bbox_string(box, width, length):\n","    return (\n","        str(box[0])\n","        + \" \"\n","        + str(box[1])\n","        + \" \"\n","        + str(box[2])\n","        + \" \"\n","        + str(box[3])\n","        + \"\\t\"\n","        + str(width)\n","        + \" \"\n","        + str(length)\n","    )"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ZTbnZ0CQ5zb","colab_type":"code","colab":{}},"source":["\n","#path='/content/gdrive/My Drive/ICDAR-2019-SROIE/data/test1/'\n","path='/content/gdrive/My Drive/unilm/layoutlm/examples/seq_labeling/data/dataset/training_data/working/'\n","def convert(args):\n","    with open(\n","        os.path.join(args.output_dir, args.data_split + \".txt.tmp\"),\n","        \"w\",\n","        encoding=\"utf8\",\n","    ) as fw, open(\n","        os.path.join(args.output_dir, args.data_split + \"_box.txt.tmp\"),\n","        \"w\",\n","        encoding=\"utf8\",\n","    ) as fbw, open(\n","        os.path.join(args.output_dir, args.data_split + \"_image.txt.tmp\"),\n","        \"w\",\n","        encoding=\"utf8\",\n","    ) as fiw:\n","        for file in os.listdir(path):\n","          with open(path + file ,encoding=\"utf8\") as f: \n","            data=f.readlines()\n","           # data1=[]\n","           # for i in data:\n","           #   i=i.replace(\" \",\"\")\n","           #   #i[4]=i[4].replace(\" \",\"\")\n","           #   #i[5]=i[5].replace(\" \",\"\")\n","           #   data1.append(i)\n","          other=[]\n","          rest=[]\n","          for i in data:\n","            if 'other' in i or 'total' in i or 'date' in i:\n","              other.append(i)\n","            else:\n","              rest.append(i) \n","          company=[]\n","          address=[]\n","          heading=[]\n","\n","          for i in rest:\n","            if 'company' in i:\n","              a=i.split(',')\n","              b=[a[0], a[1], a[2] ,a[3]]\n","              c= {'text': a[4] , 'box' : b}\n","              company.append(c)\n","            if 'address' in i:\n","              a=i.split(',') \n","              b=[a[0], a[1], a[2] ,a[3]]\n","              c= {'text': a[4] , 'box' : b}\n","              address.append(c)\n","           \n","    \n","          ls1=[]\n","          ls2=[]\n","          for i in other:\n","            text=str(i.split(\",\")[4:-1]).replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace('\\\\n',' ')\n","            apple=[{'box':i.split(\",\")[0:4],'text':text}]\n","            ball=str(i.split(\",\")[-1]).replace('\\n',' ')\n","            ls1.append(apple)\n","            ls2.append(ball)\n","\n","          ls1.append(company)\n","          ls1.append(address)\n","          #ls1.append(heading)\n","\n","          ls2.append('company')\n","          ls2.append('address')\n","          #ls2.append('heading')\n","\n","\n","\n","\n","          words=ls1.copy()\n","          label=ls2.copy()\n","          #print(words)\n","          #print(label)\n","\n","          jpg=file.split('.')\n","          #print(jpg[0])\n","          jpg=jpg[0]\n","\n","          #print(jpg)  \n","          #image_path='/content/gdrive/My Drive/ICDAR-2019-SROIE/data/img/' + jpg + '.jpg'\n","          image_path='/content/gdrive/My Drive/image_check/images/' + jpg + '.jpg'\n","\n","\n","\n","     \n","          file_name = os.path.basename(image_path)\n","          image = Image.open(image_path)\n","          width, length = image.size\n","\n","       \n","  \n","          for i,j in zip(words,label):\n","            if i == '' or i==' ':\n","              continue\n","            if len(i) == 0:\n","              continue\n","            label=j  \n","            if label == \"other\":\n","              for w in i:\n","                fw.write(w[\"text\"] + \"\\tO\\n\")\n","                fbw.write(w[\"text\"]\n","                          + \"\\t\"\n","                          + bbox_string(w[\"box\"], width, length)\n","                          + \"\\n\")\n","                fiw.write(w[\"text\"]+\n","                          \"\\t\"\n","                          + actual_bbox_string(w[\"box\"], width, length)\n","                          + \"\\t\"\n","                          + file_name\n","                          + \"\\n\"\n","                          )\n","            elif len(i) == 1:\n","              fw.write(i[0][\"text\"] + \"\\tS-\" + label.upper() + \"\\n\")\n","              fbw.write(i[0][\"text\"]\n","                        + \"\\t\"\n","                        + bbox_string(i[0][\"box\"], width, length)\n","                        + \"\\n\"\n","                        )\n","              fiw.write(i[0][\"text\"]\n","                        + \"\\t\"\n","                        + actual_bbox_string(i[0][\"box\"], width, length)\n","                        + \"\\t\"\n","                        + file_name\n","                        + \"\\n\"\n","                        )\n","            else:\n","              fw.write(i[0][\"text\"] \n","                       + \"\\tB-\" + label.upper() + \"\\n\")\n","              fbw.write(i[0][\"text\"]\n","                        + \"\\t\"\n","                        + bbox_string(i[0][\"box\"], width, length)\n","                        + \"\\n\"\n","                        )\n","              fiw.write(i[0][\"text\"]\n","                        + \"\\t\"\n","                        + actual_bbox_string(i[0][\"box\"], width, length)\n","                        + \"\\t\"\n","                        + file_name\n","                        + \"\\n\"\n","                        )\n","              for w in i[1:-1]:\n","                fw.write(w[\"text\"] + \"\\tI-\" + label.upper() + \"\\n\")\n","                fbw.write(w[\"text\"]\n","                          + \"\\t\"\n","                          + bbox_string(w[\"box\"], width, length)\n","                          + \"\\n\")\n","                fiw.write(w[\"text\"]\n","                          + \"\\t\"\n","                          + actual_bbox_string(w[\"box\"], width, length)\n","                          + \"\\t\"\n","                          + file_name\n","                          + \"\\n\")\n","              fw.write(i[-1][\"text\"] + \"\\tE-\" + label + \"\\n\")\n","              fbw.write(i[-1][\"text\"]+ \"\\t\"+ bbox_string(i[-1][\"box\"], width, length)+ \"\\n\")\n","              fiw.write( i[-1][\"text\"]+ \"\\t\"+ actual_bbox_string(i[-1][\"box\"], width, length) + \"\\t\"+ file_name+ \"\\n\" )\n","                  \n","          fw.write(\"\\n\") \n","          fbw.write(\"\\n\")\n","          fiw.write(\"\\n\")\n","\n","\n","def seg_file(file_path, tokenizer, max_len):\n","    subword_len_counter = 0\n","    output_path = file_path[:-4]\n","    with open(file_path, \"r\", encoding=\"utf8\") as f_p, open(\n","        output_path, \"w\", encoding=\"utf8\"\n","    ) as fw_p:\n","        for line in f_p:\n","            line = line.rstrip()\n","\n","            if not line:\n","                fw_p.write(line + \"\\n\")\n","                subword_len_counter = 0\n","                continue\n","            token = line.split(\"\\t\")[0]\n","\n","            current_subwords_len = len(tokenizer.tokenize(token))\n","\n","            # Token contains strange control characters like \\x96 or \\x95\n","            # Just filter out the complete line\n","            if current_subwords_len == 0:\n","                continue\n","\n","            if (subword_len_counter + current_subwords_len) > max_len:\n","                fw_p.write(\"\\n\" + line + \"\\n\")\n","                subword_len_counter = current_subwords_len\n","                continue\n","\n","            subword_len_counter += current_subwords_len\n","\n","            fw_p.write(line + \"\\n\")\n","\n","\n","def seg(args):\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        args.model_name_or_path, do_lower_case=True\n","    )\n","    seg_file(\n","        os.path.join(args.output_dir, args.data_split + \".txt.tmp\"),\n","        tokenizer,\n","        args.max_len,\n","    )\n","    seg_file(\n","        os.path.join(args.output_dir, args.data_split + \"_box.txt.tmp\"),\n","        tokenizer,\n","        args.max_len,\n","    )\n","    seg_file(\n","        os.path.join(args.output_dir, args.data_split + \"_image.txt.tmp\"),\n","        tokenizer,\n","        args.max_len,\n","    )\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mz4fzziAV19V","colab_type":"code","colab":{}},"source":["class preprocessing_data_input:\n","  def __init__(self,data_dir = \"/content/gdrive/My Drive/unilm/layoutlm/examples/seq_labeling/data/dataset/training_data/working/\",data_split = \"train\",output_dir=\"data\",model_name_or_path=\"bert-base-uncased\",max_len=510):\n","    self.data_dir = data_dir\n","    self.data_split = data_split\n","    self.output_dir = output_dir\n","    self.model_name_or_path = model_name_or_path\n","    self.max_len = max_len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yWq622piEeST","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1600240111454,"user_tz":-330,"elapsed":1083,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"33126bbc-9e21-4f67-e372-e351af7047e8"},"source":["\"\"\"\n","class preprocessing_data_input:\n","  def __init__(self,data_dir = \"/content/gdrive/My Drive/ICDAR-2019-SROIE/data/test1/\",data_split = \"train\",output_dir=\"data\",model_name_or_path=\"bert-base-uncased\",max_len=510):\n","    self.data_dir = data_dir\n","    self.data_split = data_split\n","    self.output_dir = output_dir\n","    self.model_name_or_path = model_name_or_path\n","    self.max_len = max_len\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nclass preprocessing_data_input:\\n  def __init__(self,data_dir = \"/content/gdrive/My Drive/ICDAR-2019-SROIE/data/test1/\",data_split = \"train\",output_dir=\"data\",model_name_or_path=\"bert-base-uncased\",max_len=510):\\n    self.data_dir = data_dir\\n    self.data_split = data_split\\n    self.output_dir = output_dir\\n    self.model_name_or_path = model_name_or_path\\n    self.max_len = max_len\\n'"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"W-AhBI1dCYwr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"status":"ok","timestamp":1600240115880,"user_tz":-330,"elapsed":1057,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"d1cbdd2a-c2c4-43fd-822a-da810818207d"},"source":["\"\"\"\n","#path='data/dataset/training_data/new_annotations/'\n","for file in os.listdir(path):\n","  with open(path + file,encoding='utf8') as f: \n","    data=f.readlines()\n","    for i in data:\n","      a=i.split(',')\n","      #c= {'text': a[4] , 'box' : [a[0], a[1], a[2] ,a[3]] , 'tag': a[5]}\n","      #ls1=['other\\n','company\\n','address\\n','date\\n','total\\n']\n","      if len(a) > 6:\n","        print('error')\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["\"\\n#path='data/dataset/training_data/new_annotations/'\\nfor file in os.listdir(path):\\n  with open(path + file,encoding='utf8') as f: \\n    data=f.readlines()\\n    for i in data:\\n      a=i.split(',')\\n      #c= {'text': a[4] , 'box' : [a[0], a[1], a[2] ,a[3]] , 'tag': a[5]}\\n      #ls1=['other\\n','company\\n','address\\n','date\\n','total\\n']\\n      if len(a) > 6:\\n        print('error')\\n\""]},"metadata":{"tags":[]},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"Di7Hh2b29G0Z","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u7kzdgl9_sXo","colab_type":"text"},"source":["TRAINING"]},{"cell_type":"code","metadata":{"id":"naj9Q2B4YGZC","colab_type":"code","colab":{}},"source":["\n","args = preprocessing_data_input(\"/content/gdrive/My Drive/unilm/layoutlm/examples/seq_labeling/data/dataset/training_data/working/\",\"train\",\"data\",\"bert-base-uncased\",510)\n","convert(args)\n","seg(args)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJVt3iDdEoTM","colab_type":"code","colab":{}},"source":["\"\"\"\n","args = preprocessing_data_input(\"/content/gdrive/My Drive/ICDAR-2019-SROIE/data/test1/\",\"train\",\"data\",\"bert-base-uncased\",510)\n","convert(args)\n","seg(args)\n","\n","\"\"\""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hKpgikoVbwle","colab_type":"text"},"source":["preparing/preprocessing data for testing"]},{"cell_type":"markdown","metadata":{"id":"kAD-9DU-bAcE","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"9qf_1tlOYQkj","colab_type":"code","colab":{}},"source":["args = preprocessing_data_input(\"/content/gdrive/My Drive/unilm/layoutlm/examples/seq_labeling/data/dataset/training_data/working/\",\"test\",\"data\",\"bert-base-uncased\",510)\n","convert(args)\n","seg(args)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aT66PpovE9gu","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1600240061274,"user_tz":-330,"elapsed":1145,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"43768d9e-d750-4b41-d6fd-60288e38f87b"},"source":["\"\"\"\n","args = preprocessing_data_input(\"/content/gdrive/My Drive/ICDAR-2019-SROIE/data/test1/\",\"test\",\"data\",\"bert-base-uncased\",510)\n","convert(args)\n","seg(args)\n","\n","\"\"\""],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\nargs = preprocessing_data_input(\"/content/gdrive/My Drive/ICDAR-2019-SROIE/data/test1/\",\"test\",\"data\",\"bert-base-uncased\",510)\\nconvert(args)\\nseg(args)\\n\\n'"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"ylMZa20DvROb","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ZfZ5fagpqHw","colab_type":"text"},"source":["**creating labels.txt**"]},{"cell_type":"code","metadata":{"id":"sxZ2a2R3p8pl","colab_type":"code","colab":{}},"source":["!cat data/train.txt | cut -d$'\\t' -f 2 | grep -v \"^$\"| sort | uniq > data/labels.txt\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j77A2FlUqyNh","colab_type":"text"},"source":["**train the model**"]},{"cell_type":"markdown","metadata":{"id":"1V63aG5q-LJu","colab_type":"text"},"source":["\n","\n","```\n","# This is formatted as code\n","```\n","\n","***layoutlm.py***"]},{"cell_type":"code","metadata":{"id":"ripg2WCb97Zr","colab_type":"code","colab":{}},"source":["import logging\n","\n","import torch\n","from torch import nn\n","from torch.nn import CrossEntropyLoss, MSELoss\n","from transformers import BertConfig, BertModel, BertPreTrainedModel\n","from transformers.modeling_bert import BertLayerNorm\n","\n","logger = logging.getLogger(__name__)\n","\n","LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP = {}\n","\n","LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP = {}\n","\n","\n","class LayoutlmConfig(BertConfig):\n","    pretrained_config_archive_map = LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP\n","    model_type = \"bert\"\n","\n","    def __init__(self, max_2d_position_embeddings=1024, **kwargs):\n","        super().__init__(**kwargs)\n","        self.max_2d_position_embeddings = max_2d_position_embeddings\n","\n","\n","\n","\n","class LayoutlmEmbeddings(nn.Module): \n","    def __init__(self, config):\n","        super(LayoutlmEmbeddings, self).__init__()\n","        self.word_embeddings = nn.Embedding(\n","            config.vocab_size, config.hidden_size, padding_idx=0\n","        )\n","        self.position_embeddings = nn.Embedding(\n","            config.max_position_embeddings, config.hidden_size\n","        )\n","        self.x_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.y_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.h_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.w_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.token_type_embeddings = nn.Embedding(\n","            config.type_vocab_size, config.hidden_size\n","        )\n","\n","        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n","        # any TensorFlow checkpoint file\n","        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        token_type_ids=None,\n","        position_ids=None,\n","        inputs_embeds=None,\n","    ):\n","        seq_length = input_ids.size(1)\n","        if position_ids is None:\n","            position_ids = torch.arange(\n","                seq_length, dtype=torch.long, device=input_ids.device\n","            )\n","            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        words_embeddings = self.word_embeddings(input_ids)\n","        position_embeddings = self.position_embeddings(position_ids)\n","        left_position_embeddings = self.x_position_embeddings(bbox[:, :, 0])\n","        upper_position_embeddings = self.y_position_embeddings(bbox[:, :, 1])\n","        right_position_embeddings = self.x_position_embeddings(bbox[:, :, 2])\n","        lower_position_embeddings = self.y_position_embeddings(bbox[:, :, 3])\n","        h_position_embeddings = self.h_position_embeddings(\n","            bbox[:, :, 3] - bbox[:, :, 1]\n","        )\n","       # print(\"\\n bbox shape :- \",bbox.size())\n","       # print(\"\\nbbox[:,:,2] :- \", bbox[:, :, 2])\n","       # print(\"\\nbbox[:,:,2] shape :- \", bbox[:, :, 2].shape)\n","       # print(\"\\nbbox[:, :, 0] :- \", bbox[:, :, 0])\n","       # print(\"\\nbbox[:, :, 0] shape :- \", bbox[:, :, 0].shape)\n","       # print(\"\\n sub of two above matrix :- \",bbox[:, :, 2] - bbox[:, :, 0])\n","       # print(\"\\n printing the w_position_embeddings:- \",self.w_position_embeddings)\n","        bboxshape=bbox[:, :, 2] - bbox[:, :, 0]\n","       # print(\"\\n shape of the resultant sub array :- \",bboxshape.size())\n","        w_position_embeddings = self.w_position_embeddings(\n","            bbox[:, :, 2] - bbox[:, :, 0]\n","        )\n","        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n","\n","        embeddings = (\n","            words_embeddings\n","            + position_embeddings\n","            + left_position_embeddings\n","            + upper_position_embeddings\n","            + right_position_embeddings\n","            + lower_position_embeddings\n","            + h_position_embeddings\n","            + w_position_embeddings\n","            + token_type_embeddings\n","        )\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","        return embeddings\n","\n","\n","class LayoutlmModel(BertModel):\n","\n","    config_class = LayoutlmConfig\n","    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, config):\n","        super(LayoutlmModel, self).__init__(config)\n","        self.embeddings = LayoutlmEmbeddings(config)\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","    ):\n","        if attention_mask is None:\n","            attention_mask = torch.ones_like(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        # We create a 3D attention mask from a 2D tensor mask.\n","        # Sizes are [batch_size, 1, 1, to_seq_length]\n","        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n","        # this attention mask is more simple than the triangular masking of causal attention\n","        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n","        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n","\n","        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n","        # masked positions, this operation will create a tensor which is 0.0 for\n","        # positions we want to attend and -10000.0 for masked positions.\n","        # Since we are adding it to the raw scores before the softmax, this is\n","        # effectively the same as removing these entirely.\n","        extended_attention_mask = extended_attention_mask.to(\n","            dtype=next(self.parameters()).dtype\n","        )  # fp16 compatibility\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","        # Prepare head mask if needed\n","        # 1.0 in head_mask indicate we keep the head\n","        # attention_probs has shape bsz x n_heads x N x N\n","        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n","        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n","        if head_mask is not None:\n","            if head_mask.dim() == 1:\n","                head_mask = (\n","                    head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n","                )\n","                head_mask = head_mask.expand(\n","                    self.config.num_hidden_layers, -1, -1, -1, -1\n","                )\n","            elif head_mask.dim() == 2:\n","                head_mask = (\n","                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n","                )  # We can specify head_mask for each layer\n","            head_mask = head_mask.to(\n","                dtype=next(self.parameters()).dtype\n","            )  # switch to fload if need + fp16 compatibility\n","        else:\n","            head_mask = [None] * self.config.num_hidden_layers\n","\n","        #print(\"\\n The arguments in the embedding layer :- \\n\\n\\n\")\n","        #print(\"\\n input_ids in embedding layer :- \",input_ids)\n","        #print(\"\\n position_ids in embedding layer :- \",position_ids)\n","        #print(\"\\n token_type_ids in embedding layer :- \",token_type_ids)\n","        #print(\"\\n input_ids in embedding layer :- \\n\\n\")\n","        embedding_output = self.embeddings(\n","            input_ids, bbox, position_ids=position_ids, token_type_ids=token_type_ids\n","        )\n","        encoder_outputs = self.encoder(\n","            embedding_output, extended_attention_mask, head_mask=head_mask\n","        )\n","        sequence_output = encoder_outputs[0]\n","        pooled_output = self.pooler(sequence_output)\n","\n","        outputs = (sequence_output, pooled_output) + encoder_outputs[\n","            1:\n","        ]  # add hidden_states and attentions if they are here\n","        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n","\n","\n","class LayoutlmForTokenClassification(BertPreTrainedModel):\n","    config_class = LayoutlmConfig\n","    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        #print('num_labels', 9)\n","        self.bert = LayoutlmModel(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        #print('model archi2', self.classifier())\n","\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","    ):\n","\n","        outputs = self.bert(\n","            input_ids=input_ids,\n","            bbox=bbox,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","        #print('model archi3', self.classifier())\n","        outputs = (logits,) + outputs[\n","            2:\n","        ]  # add hidden states and attention if they are here\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                print('check_label',self.num_labels)\n","                active_logits = logits.view(-1, self.num_labels)[active_loss]\n","                active_labels = labels.view(-1)[active_loss]\n","                loss = loss_fct(active_logits, active_labels)\n","            else:\n","                print('check_label2', self.num_labels)\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), scores, (hidden_states), (attentions)\n","\n","\n","class LayoutlmForSequenceClassification(BertPreTrainedModel):\n","    config_class = LayoutlmConfig\n","    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, config):\n","        super(LayoutlmForSequenceClassification, self).__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.bert = LayoutlmModel(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","      #\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels) #config.num_labels\n","        #print('model archi4', self.classifier())\n","\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","    ):\n","\n","        outputs = self.bert(\n","            input_ids=input_ids,\n","            bbox=bbox,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","        )\n","\n","        pooled_output = outputs[1]\n","\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","        #print('model archi', self.classifier())\n","        outputs = (logits,) + outputs[\n","            2:\n","        ]  # add hidden states and attention if they are here\n","\n","        if labels is not None:\n","            if self.num_labels == 1:\n","                #  We are doing regression\n","                loss_fct = MSELoss()\n","                loss = loss_fct(logits.view(-1), labels.view(-1))\n","            else:\n","                loss_fct = CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x61CAAmf-hay","colab_type":"text"},"source":["***funsd.py***"]},{"cell_type":"code","metadata":{"id":"cQsu_uQj-W1l","colab_type":"code","colab":{}},"source":["import logging\n","import os\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","class FunsdDataset(Dataset):\n","    def __init__(self, args, tokenizer, labels, pad_token_label_id, mode):\n","        if args.local_rank not in [-1, 0] and mode == \"train\":\n","            torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","\n","        # Load data features from cache or dataset file\n","        cached_features_file = os.path.join(\n","            args.data_dir,\n","            \"cached_{}_{}_{}\".format(\n","                mode,\n","                list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n","                str(args.max_seq_length),\n","            ),\n","        )\n","        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n","            logger.info(\"Loading features from cached file %s\", cached_features_file)\n","            features = torch.load(cached_features_file)\n","        else:\n","            logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n","            examples = read_examples_from_file(args.data_dir, mode)\n","            features = convert_examples_to_features(\n","                examples,\n","                labels,\n","                args.max_seq_length,\n","                tokenizer,\n","                cls_token_at_end=bool(args.model_type in [\"xlnet\"]),\n","                # xlnet has a cls token at the end\n","                cls_token=tokenizer.cls_token,\n","                cls_token_segment_id=2 if args.model_type in [\"xlnet\"] else 0,\n","                sep_token=tokenizer.sep_token,\n","                sep_token_extra=bool(args.model_type in [\"roberta\"]),\n","                # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n","                pad_on_left=bool(args.model_type in [\"xlnet\"]),\n","                # pad on the left for xlnet\n","                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n","                pad_token_segment_id=4 if args.model_type in [\"xlnet\"] else 0,\n","                pad_token_label_id=pad_token_label_id,\n","            )\n","            if args.local_rank in [-1, 0]:\n","                logger.info(\"Saving features into cached file %s\", cached_features_file)\n","                torch.save(features, cached_features_file)\n","\n","        if args.local_rank == 0 and mode == \"train\":\n","            torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","\n","        self.features = features\n","        # Convert to Tensors and build dataset\n","        self.all_input_ids = torch.tensor(\n","            [f.input_ids for f in features], dtype=torch.long\n","        )\n","        self.all_input_mask = torch.tensor(\n","            [f.input_mask for f in features], dtype=torch.long\n","        )\n","        self.all_segment_ids = torch.tensor(\n","            [f.segment_ids for f in features], dtype=torch.long\n","        )\n","        self.all_label_ids = torch.tensor(\n","            [f.label_ids for f in features], dtype=torch.long\n","        )\n","        self.all_bboxes = torch.tensor([f.boxes for f in features], dtype=torch.long)\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, index):\n","        return (\n","            self.all_input_ids[index],\n","            self.all_input_mask[index],\n","            self.all_segment_ids[index],\n","            self.all_label_ids[index],\n","            self.all_bboxes[index],\n","        )\n","\n","\n","class InputExample(object):\n","    \"\"\"A single training/test example for token classification.\"\"\"\n","\n","    def __init__(self, guid, words, labels, boxes, actual_bboxes, file_name, page_size):\n","        \"\"\"Constructs a InputExample.\n","\n","        Args:\n","            guid: Unique id for the example.\n","            words: list. The words of the sequence.\n","            labels: (Optional) list. The labels for each word of the sequence. This should be\n","            specified for train and dev examples, but not for test examples.\n","        \"\"\"\n","        self.guid = guid\n","        self.words = words\n","        self.labels = labels\n","        self.boxes = boxes\n","        self.actual_bboxes = actual_bboxes\n","        self.file_name = file_name\n","        self.page_size = page_size\n","\n","\n","class InputFeatures(object):\n","    \"\"\"A single set of features of data.\"\"\"\n","\n","    def __init__(\n","        self,\n","        input_ids,\n","        input_mask,\n","        segment_ids,\n","        label_ids,\n","        boxes,\n","        actual_bboxes,\n","        file_name,\n","        page_size,\n","    ):\n","        assert (\n","            0 <= all(boxes) <= 1000\n","        ), \"Error with input bbox ({}): the coordinate value is not between 0 and 1000\".format(\n","            boxes\n","        )\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.label_ids = label_ids\n","        self.boxes = boxes\n","        self.actual_bboxes = actual_bboxes\n","        self.file_name = file_name\n","        self.page_size = page_size\n","\n","\n","def read_examples_from_file(data_dir, mode):\n","    file_path = os.path.join(data_dir, \"{}.txt\".format(mode))\n","    box_file_path = os.path.join(data_dir, \"{}_box.txt\".format(mode))\n","    image_file_path = os.path.join(data_dir, \"{}_image.txt\".format(mode))\n","    guid_index = 1\n","    examples = []\n","    with open(file_path, encoding=\"utf-8\") as f, open(\n","        box_file_path, encoding=\"utf-8\"\n","    ) as fb, open(image_file_path, encoding=\"utf-8\") as fi:\n","        words = []\n","        boxes = []\n","        actual_bboxes = []\n","        file_name = None\n","        page_size = None\n","        labels = []\n","        for line, bline, iline in zip(f, fb, fi):\n","            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n","                if words:\n","                    examples.append(\n","                        InputExample(\n","                            guid=\"{}-{}\".format(mode, guid_index),\n","                            words=words,\n","                            labels=labels,\n","                            boxes=boxes,\n","                            actual_bboxes=actual_bboxes,\n","                            file_name=file_name,\n","                            page_size=page_size,\n","                        )\n","                    )\n","                    guid_index += 1\n","                    words = []\n","                    boxes = []\n","                    actual_bboxes = []\n","                    file_name = None\n","                    page_size = None\n","                    labels = []\n","            else:\n","                splits = line.split(\"\\t\")\n","                bsplits = bline.split(\"\\t\")\n","                isplits = iline.split(\"\\t\")\n","                assert len(splits) == 2\n","                assert len(bsplits) == 2\n","                assert len(isplits) == 4\n","                assert splits[0] == bsplits[0]\n","                words.append(splits[0])\n","                if len(splits) > 1:\n","                    labels.append(splits[-1].replace(\"\\n\", \"\"))\n","                    box = bsplits[-1].replace(\"\\n\", \"\")\n","                    box = [int(b) for b in box.split()]\n","                    boxes.append(box)\n","                    actual_bbox = [int(b) for b in isplits[1].split()]\n","                    actual_bboxes.append(actual_bbox)\n","                    page_size = [int(i) for i in isplits[2].split()]\n","                    file_name = isplits[3].strip()\n","                else:\n","                    # Examples could have no label for mode = \"test\"\n","                    labels.append(\"O\")\n","        if words:\n","            examples.append(\n","                InputExample(\n","                    guid=\"%s-%d\".format(mode, guid_index),\n","                    words=words,\n","                    labels=labels,\n","                    boxes=boxes,\n","                    actual_bboxes=actual_bboxes,\n","                    file_name=file_name,\n","                    page_size=page_size,\n","                )\n","            )\n","    return examples\n","\n","\n","def convert_examples_to_features(\n","    examples,\n","    label_list,\n","    max_seq_length,\n","    tokenizer,\n","    cls_token_at_end=False,\n","    cls_token=\"[CLS]\",\n","    cls_token_segment_id=1,\n","    sep_token=\"[SEP]\",\n","    sep_token_extra=False,\n","    pad_on_left=False,\n","    pad_token=0,\n","    cls_token_box=[0, 0, 0, 0],\n","    sep_token_box=[1000, 1000, 1000, 1000],\n","    pad_token_box=[0, 0, 0, 0],\n","    pad_token_segment_id=0,\n","    pad_token_label_id=-1,\n","    sequence_a_segment_id=0,\n","    mask_padding_with_zero=True,\n","):\n","    \"\"\" Loads a data file into a list of `InputBatch`s\n","        `cls_token_at_end` define the location of the CLS token:\n","            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n","            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n","        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n","    \"\"\"\n","\n","    label_map = {label: i for i, label in enumerate(label_list)}\n","\n","    features = []\n","    for (ex_index, example) in enumerate(examples):\n","        file_name = example.file_name\n","        page_size = example.page_size\n","        width, height = page_size\n","        if ex_index % 10000 == 0:\n","            logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n","\n","        tokens = []\n","        token_boxes = []\n","        actual_bboxes = []\n","        label_ids = []\n","        for word, label, box, actual_bbox in zip(\n","            example.words, example.labels, example.boxes, example.actual_bboxes\n","        ):\n","            word_tokens = tokenizer.tokenize(word)\n","            tokens.extend(word_tokens)\n","            token_boxes.extend([box] * len(word_tokens))\n","            actual_bboxes.extend([actual_bbox] * len(word_tokens))\n","            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n","            label_ids.extend(\n","                [label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1)\n","            )\n","\n","        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n","        special_tokens_count = 3 if sep_token_extra else 2\n","        if len(tokens) > max_seq_length - special_tokens_count:\n","            tokens = tokens[: (max_seq_length - special_tokens_count)]\n","            token_boxes = token_boxes[: (max_seq_length - special_tokens_count)]\n","            actual_bboxes = actual_bboxes[: (max_seq_length - special_tokens_count)]\n","            label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n","\n","        # The convention in BERT is:\n","        # (a) For sequence pairs:\n","        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n","        # (b) For single sequences:\n","        #  tokens:   [CLS] the dog is hairy . [SEP]\n","        #  type_ids:   0   0   0   0  0     0   0\n","        #\n","        # Where \"type_ids\" are used to indicate whether this is the first\n","        # sequence or the second sequence. The embedding vectors for `type=0` and\n","        # `type=1` were learned during pre-training and are added to the wordpiece\n","        # embedding vector (and position vector). This is not *strictly* necessary\n","        # since the [SEP] token unambiguously separates the sequences, but it makes\n","        # it easier for the model to learn the concept of sequences.\n","        #\n","        # For classification tasks, the first vector (corresponding to [CLS]) is\n","        # used as as the \"sentence vector\". Note that this only makes sense because\n","        # the entire model is fine-tuned.\n","        tokens += [sep_token]\n","        token_boxes += [sep_token_box]\n","        actual_bboxes += [[0, 0, width, height]]\n","        label_ids += [pad_token_label_id]\n","        if sep_token_extra:\n","            # roberta uses an extra separator b/w pairs of sentences\n","            tokens += [sep_token]\n","            token_boxes += [sep_token_box]\n","            actual_bboxes += [[0, 0, width, height]]\n","            label_ids += [pad_token_label_id]\n","        segment_ids = [sequence_a_segment_id] * len(tokens)\n","\n","        if cls_token_at_end:\n","            tokens += [cls_token]\n","            token_boxes += [cls_token_box]\n","            actual_bboxes += [[0, 0, width, height]]\n","            label_ids += [pad_token_label_id]\n","            segment_ids += [cls_token_segment_id]\n","        else:\n","            tokens = [cls_token] + tokens\n","            token_boxes = [cls_token_box] + token_boxes\n","            actual_bboxes = [[0, 0, width, height]] + actual_bboxes\n","            label_ids = [pad_token_label_id] + label_ids\n","            segment_ids = [cls_token_segment_id] + segment_ids\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","        # tokens are attended to.\n","        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n","\n","        # Zero-pad up to the sequence length.\n","        padding_length = max_seq_length - len(input_ids)\n","        if pad_on_left:\n","            input_ids = ([pad_token] * padding_length) + input_ids\n","            input_mask = (\n","                [0 if mask_padding_with_zero else 1] * padding_length\n","            ) + input_mask\n","            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n","            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n","            token_boxes = ([pad_token_box] * padding_length) + token_boxes\n","        else:\n","            input_ids += [pad_token] * padding_length\n","            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n","            segment_ids += [pad_token_segment_id] * padding_length\n","            label_ids += [pad_token_label_id] * padding_length\n","            token_boxes += [pad_token_box] * padding_length\n","\n","        assert len(input_ids) == max_seq_length\n","        assert len(input_mask) == max_seq_length\n","        assert len(segment_ids) == max_seq_length\n","        assert len(label_ids) == max_seq_length\n","        assert len(token_boxes) == max_seq_length\n","\n","        if ex_index < 5:\n","            logger.info(\"*** Example ***\")\n","            logger.info(\"guid: %s\", example.guid)\n","            logger.info(\"tokens: %s\", \" \".join([str(x) for x in tokens]))\n","            logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n","            logger.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n","            logger.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n","            logger.info(\"label_ids: %s\", \" \".join([str(x) for x in label_ids]))\n","            logger.info(\"boxes: %s\", \" \".join([str(x) for x in token_boxes]))\n","            logger.info(\"actual_bboxes: %s\", \" \".join([str(x) for x in actual_bboxes]))\n","\n","        features.append(\n","            InputFeatures(\n","                input_ids=input_ids,\n","                input_mask=input_mask,\n","                segment_ids=segment_ids,\n","                label_ids=label_ids,\n","                boxes=token_boxes,\n","                actual_bboxes=actual_bboxes,\n","                file_name=file_name,\n","                page_size=page_size,\n","            )\n","        )\n","    return features\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zVJQt2N0q46e","colab_type":"text"},"source":["****run_sequence_labeling.py****"]},{"cell_type":"code","metadata":{"id":"T5CKUcz8q4ao","colab_type":"code","colab":{}},"source":["\"\"\" Fine-tuning the library models for named entity recognition on CoNLL-2003 (Bert or Roberta). \"\"\"\n","\n","from __future__ import absolute_import, division, print_function\n","\n","import argparse\n","import glob\n","import logging\n","import os\n","import random\n","import shutil\n","\n","import numpy as np\n","import torch\n","from seqeval.metrics import (\n","    classification_report,\n","    f1_score,\n","    precision_score,\n","    recall_score,\n",")\n","from tensorboardX import SummaryWriter\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm import tqdm, trange\n","from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    BertConfig,\n","    BertForTokenClassification,\n","    BertTokenizer,\n","    RobertaConfig,\n","    RobertaForTokenClassification,\n","    RobertaTokenizer,\n","    get_linear_schedule_with_warmup,\n",")\n","'''\n","Other file imports {take only important files for deployment}\n","'''\n","#from layoutlm import FunsdDataset, LayoutlmConfig, LayoutlmForTokenClassification\n","\n","logger = logging.getLogger(__name__)\n","\n","ALL_MODELS = sum(\n","    (\n","        tuple(conf.pretrained_config_archive_map.keys())\n","        for conf in (BertConfig, RobertaConfig, LayoutlmConfig)\n","    ),\n","    (),\n",")\n","\n","MODEL_CLASSES = {\n","    \"bert\": (BertConfig, BertForTokenClassification, BertTokenizer),\n","    \"roberta\": (RobertaConfig, RobertaForTokenClassification, RobertaTokenizer),\n","    \"layoutlm\": (LayoutlmConfig, LayoutlmForTokenClassification, BertTokenizer),\n","}\n","\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if args.n_gpu > 0:\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","\n","def collate_fn(data):\n","    batch = [i for i in zip(*data)]\n","    for i in range(len(batch)):\n","        if i < len(batch) - 2:\n","            batch[i] = torch.stack(batch[i], 0)\n","    return tuple(batch)\n","\n","\n","def get_labels(path):\n","    with open(path, \"r\") as f:\n","        labels = f.read().splitlines()\n","    if \"O\" not in labels:\n","        labels = [\"O\"] + labels\n","    return labels\n","\n","\n","def train(  # noqa C901\n","    args, train_dataset, model, tokenizer, labels, pad_token_label_id\n","):\n","    \"\"\" Train the model \"\"\"\n","    if args.local_rank in [-1, 0]:\n","        tb_writer = SummaryWriter(logdir=\"runs/\" + os.path.basename(args.output_dir))\n","\n","    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n","    train_sampler = (\n","        RandomSampler(train_dataset)\n","        if args.local_rank == -1\n","        else DistributedSampler(train_dataset)\n","    )\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        sampler=train_sampler,\n","        batch_size=args.train_batch_size,\n","        collate_fn=None,\n","    )\n","    #print('train_dataset',train_dataset[0])\n","    if args.max_steps > 0:\n","        t_total = args.max_steps\n","        args.num_train_epochs = (\n","            args.max_steps\n","            // (len(train_dataloader) // args.gradient_accumulation_steps)\n","            + 1\n","        )\n","    else:\n","        t_total = (\n","            len(train_dataloader)\n","            // args.gradient_accumulation_steps\n","            * args.num_train_epochs\n","        )\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [\n","                p\n","                for n, p in model.named_parameters()\n","                if not any(nd in n for nd in no_decay)\n","            ],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\n","            \"params\": [\n","                p\n","                for n, p in model.named_parameters()\n","                if any(nd in n for nd in no_decay)\n","            ],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon\n","    )\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n","    )\n","    if args.fp16:\n","        try:\n","            from apex import amp\n","        except ImportError:\n","            raise ImportError(\n","                \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"\n","            )\n","        model, optimizer = amp.initialize(\n","            model, optimizer, opt_level=args.fp16_opt_level\n","        )\n","\n","    # multi-gpu training (should be after apex fp16 initialization)\n","    if args.n_gpu > 1:\n","        model = torch.nn.DataParallel(model)\n","\n","    # Distributed training (should be after apex fp16 initialization)\n","    if args.local_rank != -1:\n","        model = torch.nn.parallel.DistributedDataParallel(\n","            model,\n","            device_ids=[args.local_rank],\n","            output_device=args.local_rank,\n","            find_unused_parameters=True,\n","        )\n","\n","    # Train!\n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num examples = %d\", len(train_dataset))\n","    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n","    logger.info(\n","        \"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size\n","    )\n","    logger.info(\n","        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n","        args.train_batch_size\n","        * args.gradient_accumulation_steps\n","        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n","    )\n","    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n","    logger.info(\"  Total optimization steps = %d\", t_total)\n","\n","    global_step = 0\n","    tr_loss, logging_loss = 0.0, 0.0\n","    #print('model arch', model)\n","    model.zero_grad()\n","    train_iterator = trange(\n","        int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n","    )\n","    set_seed(args)  # Added here for reproductibility (even between python 2 and 3)\n","    for _ in train_iterator:\n","        epoch_iterator = tqdm(\n","            train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0]\n","        )\n","        for step, batch in enumerate(epoch_iterator):\n","            model.train()\n","            inputs = {\n","                \"input_ids\": batch[0].to(args.device),\n","                \"attention_mask\": batch[1].to(args.device),\n","                \"labels\": batch[3].to(args.device),\n","            }\n","            if args.model_type in [\"layoutlm\"]:\n","                inputs[\"bbox\"] = batch[4].to(args.device)\n","            inputs[\"token_type_ids\"] = (\n","                batch[2].to(args.device) if args.model_type in [\"bert\", \"layoutlm\"] else None\n","            )  # RoBERTa don\"t use segment_ids\n","            #print('inputs_labels',inputs['labels'])\n","            outputs = model(**inputs)\n","            #print('output', outputs)\n","            # model outputs are always tuple in pytorch-transformers (see doc)\n","            loss = outputs[0]\n","\n","            if args.n_gpu > 1:\n","                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            tr_loss += loss.item()\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                if args.fp16:\n","                    torch.nn.utils.clip_grad_norm_(\n","                        amp.master_params(optimizer), args.max_grad_norm\n","                    )\n","                else:\n","                    torch.nn.utils.clip_grad_norm_(\n","                        model.parameters(), args.max_grad_norm\n","                    )\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","\n","                if (\n","                    args.local_rank in [-1, 0]\n","                    and args.logging_steps > 0\n","                    and global_step % args.logging_steps == 0\n","                ):\n","                    # Log metrics\n","                    if (\n","                        args.local_rank in [-1, 0] and args.evaluate_during_training\n","                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n","                        results, _ = evaluate(\n","                            args,\n","                            model,\n","                            tokenizer,\n","                            labels,\n","                            pad_token_label_id,\n","                            mode=\"dev\",\n","                        )\n","                        for key, value in results.items():\n","                            tb_writer.add_scalar(\n","                                \"eval_{}\".format(key), value, global_step\n","                            )\n","                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n","                    tb_writer.add_scalar(\n","                        \"loss\",\n","                        (tr_loss - logging_loss) / args.logging_steps,\n","                        global_step,\n","                    )\n","                    logging_loss = tr_loss\n","\n","                if (\n","                    args.local_rank in [-1, 0]\n","                    and args.save_steps > 0\n","                    and global_step % args.save_steps == 0\n","                ):\n","                    # Save model checkpoint\n","                    output_dir = os.path.join(\n","                        args.output_dir, \"checkpoint-{}\".format(global_step)\n","                    )\n","                    if not os.path.exists(output_dir):\n","                        os.makedirs(output_dir)\n","                    model_to_save = (\n","                        model.module if hasattr(model, \"module\") else model\n","                    )  # Take care of distributed/parallel training\n","                    model_to_save.save_pretrained(output_dir)\n","                    tokenizer.save_pretrained(output_dir)\n","                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n","                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n","\n","            if args.max_steps > 0 and global_step > args.max_steps:\n","                epoch_iterator.close()\n","                break\n","        if args.max_steps > 0 and global_step > args.max_steps:\n","            train_iterator.close()\n","            break\n","\n","    if args.local_rank in [-1, 0]:\n","        tb_writer.close()\n","\n","    return global_step, tr_loss / global_step\n","\n","\n","def evaluate(args, model, tokenizer, labels, pad_token_label_id, mode, prefix=\"\"):\n","    eval_dataset = FunsdDataset(args, tokenizer, labels, pad_token_label_id, mode=mode)\n","\n","    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n","    eval_sampler = SequentialSampler(eval_dataset)\n","    eval_dataloader = DataLoader(\n","        eval_dataset,\n","        sampler=eval_sampler,\n","        batch_size=args.eval_batch_size,\n","        collate_fn=None,\n","    )\n","\n","    # Eval!\n","    logger.info(\"***** Running evaluation %s *****\", prefix)\n","    logger.info(\"  Num examples = %d\", len(eval_dataset))\n","    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n","    eval_loss = 0.0\n","    nb_eval_steps = 0\n","    preds = None\n","    out_label_ids = None\n","    model.eval()\n","    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        with torch.no_grad():\n","            inputs = {\n","                \"input_ids\": batch[0].to(args.device),\n","                \"attention_mask\": batch[1].to(args.device),\n","                \"labels\": batch[3].to(args.device),\n","            }\n","            if args.model_type in [\"layoutlm\"]:\n","                inputs[\"bbox\"] = batch[4].to(args.device)\n","            inputs[\"token_type_ids\"] = (\n","                batch[2].to(args.device)\n","                if args.model_type in [\"bert\", \"layoutlm\"]\n","                else None\n","            )  # RoBERTa don\"t use segment_ids\n","           # print(\"\\n\\n lenght :- \",len(inputs))\n","           # print(\"\\n\\n keys :- \",inputs.keys())\n","           # print(\"\\n\\n\\n\\n\",)\n","            outputs = model(**inputs)\n","            tmp_eval_loss, logits = outputs[:2]\n","\n","            if args.n_gpu > 1:\n","                tmp_eval_loss = (\n","                    tmp_eval_loss.mean()\n","                )  # mean() to average on multi-gpu parallel evaluating\n","\n","            eval_loss += tmp_eval_loss.item()\n","        nb_eval_steps += 1\n","        if preds is None:\n","            preds = logits.detach().cpu().numpy()\n","            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n","        else:\n","            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n","            out_label_ids = np.append(\n","                out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n","            )\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    preds = np.argmax(preds, axis=2)\n","\n","    label_map = {i: label for i, label in enumerate(labels)}\n","\n","    out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n","    preds_list = [[] for _ in range(out_label_ids.shape[0])]\n","\n","    for i in range(out_label_ids.shape[0]):\n","        for j in range(out_label_ids.shape[1]):\n","            if out_label_ids[i, j] != pad_token_label_id:\n","                out_label_list[i].append(label_map[out_label_ids[i][j]])\n","                preds_list[i].append(label_map[preds[i][j]])\n","\n","    results = {\n","        \"loss\": eval_loss,\n","        \"precision\": precision_score(out_label_list, preds_list),\n","        \"recall\": recall_score(out_label_list, preds_list),\n","        \"f1\": f1_score(out_label_list, preds_list),\n","    }\n","\n","    report = classification_report(out_label_list, preds_list)\n","    logger.info(\"\\n\" + report)\n","\n","    logger.info(\"***** Eval results %s *****\", prefix)\n","    for key in sorted(results.keys()):\n","        logger.info(\"  %s = %s\", key, str(results[key]))\n","\n","    return results, preds_list"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AmJKaB49d-rC","colab_type":"code","colab":{}},"source":["class sequence_labelling_data_input:\n","  def __init__(self,data_dir = None,model_type = None,model_name_or_path = None,output_dir = None,labels = \"\",config_name = \"\",tokenizer_name=\"\",cache_dir = \"\",max_seq_length = 512,do_train = False,do_eval = False,do_predict = False,evaluate_during_training = False,do_lower_case = False,per_gpu_train_batch_size = 8,per_gpu_eval_batch_size=8,gradient_accumulation_steps = 1,learning_rate = 5e-5,weight_decay = 0.0,adam_epsilon = 1e-8,max_grad_norm = 1.0,num_train_epochs = 3.0,max_steps = -1,warmup_steps = 0,logging_steps = 50,eval_all_checkpoins = False,no_cuda = False,overwrite_output_dir = False,overwrite_cache = False,seed = 42,fp16 = False,fp16_opt_level = \"01\",local_rank = -1,server_ip = \"\",server_port = \"\",save_steps = -1):\n","    #The input data dir. Should contain the training files for the CoNLL-2003 NER task.\n","    self.data_dir = data_dir\n","    #Model type selected in the list: \n","    self.model_type = model_type\n","    #Path to pre-trained model or shortcut name selected in the list: \n","    self.model_name_or_path = model_name_or_path \n","    #The output directory where the model predictions and checkpoints will be written.\n","    self.output_dir = output_dir\n","    #Path to a file containing all labels. If not specified, CoNLL-2003 labels are used\n","    self.labels = labels\n","    #Pretrained config name or path if not the same as model_name\n","    self.config_name = config_name\n","    #Pretrained tokenizer name or path if not the same as model_name\n","    self.tokenizer_name = tokenizer_name\n","    #Where do you want to store the pre-trained models downloaded from s3\n","    self.cache_dir = cache_dir\n","    #The maximum total input sequence length after tokenization. Sequences longer \"than this will be truncated, sequences shorter will be padded.\"\n","    self.max_seq_length = max_seq_length\n","    #Whether to run training.\n","    self.do_train = do_train\n","    #Whether to run eval on the dev set.\n","    self.do_eval = do_eval\n","    #Whether to run predictions on the test set.\n","    self.do_predict = do_predict\n","    #Whether to run evaluation during training at each logging step.\n","    self.evaluate_during_training = evaluate_during_training\n","    #Set this flag if you are using an uncased model.\n","    self.do_lower_case = do_lower_case\n","    #Batch size per GPU/CPU for training.\n","    self.per_gpu_train_batch_size = per_gpu_train_batch_size\n","    #Batch size per GPU/CPU for evaluation.\n","    self.per_gpu_eval_batch_size = per_gpu_eval_batch_size\n","    #Number of updates steps to accumulate before performing a backward/update pass.\n","    self.gradient_accumulation_steps = gradient_accumulation_steps\n","    #The initial learning rate for Adam.\n","    self.learning_rate = learning_rate\n","    #Weight decay if we apply some.\n","    self.weight_decay = weight_decay\n","    #Epsilon for Adam optimizer.\n","    self.adam_epsilon = adam_epsilon\n","    #Max gradient norm.\n","    self.max_grad_norm = max_grad_norm\n","    #Total number of training epochs to perform.\n","    self.num_train_epochs = num_train_epochs\n","    #If > 0: set total number of training steps to perform. Override num_train_epochs.\n","    self.max_steps = max_steps\n","    #Linear warmup over warmup_steps.\n","    self.warmup_steps = warmup_steps\n","    #Log every X updates steps.\n","    self.logging_steps = logging_steps\n","    #Save checkpoint every X updates steps.\n","    self.save_steps = save_steps\n","    #Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\n","    self.eval_all_checkpoints = False\n","    #Avoid using CUDA when available\n","    self.no_cuda = False\n","    #Overwrite the content of the output directory\n","    self.overwrite_output_dir =False\n","    #Overwrite the cached training and evaluation sets\n","    self.overwrite_cache = overwrite_cache\n","    #random seed for initialization\n","    self.seed = seed\n","    #Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\n","    self.fp16 = fp16\n","    #For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\"See details at https://nvidia.github.io/apex/amp.html\"\n","    self.fp16_opt_level = fp16_opt_level\n","    #For distributed training: local_rank\n","    self.local_rank = local_rank\n","    #For distant debugging.\n","    self.server_ip = server_ip\n","    #For distant debugging.\n","    self.server_port = server_port\n","\n","#args = parser.parse_args()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I99OAm9i2llC","colab_type":"text"},"source":["**creating the object for training**"]},{"cell_type":"code","metadata":{"id":"bCftw30m3LGD","colab_type":"code","colab":{}},"source":["#args=sequence_labelling_data_input(data_dir='data',model_type = \"layoutlm\",model_name_or_path = \"data/layoutlm-base-uncased\",do_lower_case = True,max_seq_length = 512,do_train = True,num_train_epochs = 50,overwrite_output_dir = True,logging_steps = 1,save_steps = -1,output_dir = 'seq_out2',labels = '/content/gdrive/My Drive/unilm/layoutlm/examples/seq_labeling/data/labels.txt' ,per_gpu_train_batch_size = 1,per_gpu_eval_batch_size = 1)\n","\n","  args=sequence_labelling_data_input(data_dir='/content/gdrive/My Drive/ICDAR-2019-SROIE/data/output',model_type = \"layoutlm\",model_name_or_path = \"bert-base-uncased\",do_lower_case = True,max_seq_length = 512,do_train = True,num_train_epochs = 100,overwrite_output_dir = True,logging_steps = 1,save_steps = -1,output_dir = 'final1',labels = '/content/gdrive/My Drive/ICDAR-2019-SROIE/data/output/labels.txt' ,per_gpu_train_batch_size = 1,per_gpu_eval_batch_size = 1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JzUt1N43DeJe","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"em8XAsSBwT8V","colab_type":"code","colab":{}},"source":["def main(args):  # noqa C901\n","    if (\n","        os.path.exists(args.output_dir)\n","        and os.listdir(args.output_dir)\n","        and args.do_train\n","    ):\n","        if not args.overwrite_output_dir:\n","            raise ValueError(\n","                \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n","                    args.output_dir\n","                )\n","            )\n","        else:\n","            if args.local_rank in [-1, 0]:\n","                shutil.rmtree(args.output_dir)\n","\n","    if not os.path.exists(args.output_dir) and (args.do_eval or args.do_predict):\n","        raise ValueError(\n","            \"Output directory ({}) does not exist. Please train and save the model before inference stage.\".format(\n","                args.output_dir\n","            )\n","        )\n","\n","    if (\n","        not os.path.exists(args.output_dir)\n","        and args.do_train\n","        and args.local_rank in [-1, 0]\n","    ):\n","        os.makedirs(args.output_dir)\n","\n","    # Setup distant debugging if needed\n","    if args.server_ip and args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","     #   print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(\n","            address=(args.server_ip, args.server_port), redirect_output=True\n","        )\n","        ptvsd.wait_for_attach()\n","\n","    # Setup CUDA, GPU & distributed training\n","    if args.local_rank == -1 or args.no_cuda:\n","        device = torch.device(\n","            \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n","        )\n","        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n","    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","        torch.cuda.set_device(args.local_rank)\n","        device = torch.device(\"cuda\", args.local_rank)\n","        torch.distributed.init_process_group(backend=\"nccl\")\n","        args.n_gpu = 1\n","    args.device = device\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        filename=os.path.join(args.output_dir, \"train.log\")\n","        if args.local_rank in [-1, 0]\n","        else None,\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.warning(\n","        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","        args.local_rank,\n","        device,\n","        args.n_gpu,\n","        bool(args.local_rank != -1),\n","        args.fp16,\n","    )\n","\n","    # Set seed\n","    set_seed(args)\n","\n","    labels = get_labels(args.labels)\n","    num_labels = len(labels)\n","    # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n","    pad_token_label_id = CrossEntropyLoss().ignore_index\n","\n","    # Load pretrained model and tokenizer\n","    if args.local_rank not in [-1, 0]:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n","\n","    args.model_type = args.model_type.lower()\n","    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","    config = config_class.from_pretrained(\n","        args.config_name if args.config_name else args.model_name_or_path,\n","        num_labels=num_labels,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    tokenizer = tokenizer_class.from_pretrained(\n","        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n","        do_lower_case=args.do_lower_case,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    model = model_class.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","\n","    if args.local_rank == 0:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n","\n","    model.to(args.device)\n","\n","    logger.info(\"Training/evaluation parameters %s\", args)\n","\n","    \n","    \n","    # Training\n","    if args.do_train:\n","        train_dataset = FunsdDataset(\n","            args, tokenizer, labels, pad_token_label_id, mode=\"train\"\n","        )\n","        #print('new statment', labels)\n","        #print('pad_id', pad_token_label_id)\n","        global_step, tr_loss = train(\n","            args, train_dataset, model, tokenizer, labels, pad_token_label_id\n","        )\n","        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n","\n","    \n","    \n","    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n","        # Create output directory if needed\n","        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n","            os.makedirs(args.output_dir)\n","\n","        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n","        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","        # They can then be reloaded using `from_pretrained()`\n","        model_to_save = (\n","            model.module if hasattr(model, \"module\") else model\n","        )  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(args.output_dir)\n","        tokenizer.save_pretrained(args.output_dir)\n","\n","        # Good practice: save your training arguments together with the trained model\n","        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n","\n","    \n","    \n","    \n","    # Evaluation\n","    results = {}\n","    if args.do_eval and args.local_rank in [-1, 0]:\n","        tokenizer = tokenizer_class.from_pretrained(\n","            args.output_dir, do_lower_case=args.do_lower_case\n","        )\n","        checkpoints = [args.output_dir]\n","        if args.eval_all_checkpoints:\n","            checkpoints = list(\n","                os.path.dirname(c)\n","                for c in sorted(\n","                    glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True)\n","                )\n","            )\n","            logging.getLogger(\"pytorch_transformers.modeling_utils\").setLevel(\n","                logging.WARN\n","            )  # Reduce logging\n","        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","        for checkpoint in checkpoints:\n","            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n","            model = model_class.from_pretrained(checkpoint)\n","            model.to(args.device)\n","            result, _ = evaluate(\n","                args,\n","                model,\n","                tokenizer,\n","                labels,\n","                pad_token_label_id,\n","                mode=\"test\",\n","                prefix=global_step,\n","            )\n","            if global_step:\n","                result = {\"{}_{}\".format(global_step, k): v for k, v in result.items()}\n","            results.update(result)\n","        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n","        with open(output_eval_file, \"w\") as writer:\n","            for key in sorted(results.keys()):\n","                writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n","\n","    \n","    \n","    # do predict part\n","    if args.do_predict and args.local_rank in [-1, 0]:\n","        tokenizer = tokenizer_class.from_pretrained(\n","            args.model_name_or_path, do_lower_case=args.do_lower_case\n","        )\n","        model = model_class.from_pretrained(args.output_dir)\n","        model.to(args.device)\n","       # print(\"\\n evaluate function parameter1 args: - \", args)\n","        #print(\"\\n evaluate function parameter2 model : - \", model)\n","        #print(\"\\n evaluate function parameters3 tokenizer :- \", tokenizer)\n","        #print(\"\\n evaluate function parameter4 labels : - \", labels)\n","        #print(\"\\n  evaluate fnctions parameter 5 pad_token_label\", pad_token_label_id)\n","        result, predictions = evaluate(\n","            args, model, tokenizer, labels, pad_token_label_id, mode=\"test\"\n","        )\n","        # Save results\n","        output_test_results_file = os.path.join(args.output_dir, \"test_results.txt\")\n","        with open(output_test_results_file, \"w\") as writer:\n","            for key in sorted(result.keys()):\n","                writer.write(\"{} = {}\\n\".format(key, str(result[key])))\n","        # Save predictions\n","        output_test_predictions_file = os.path.join(\n","            args.output_dir, \"test_predictions.txt\"\n","        )\n","        with open(output_test_predictions_file, \"w\", encoding=\"utf8\") as writer:\n","            with open(\n","                os.path.join(args.data_dir, \"test.txt\"), \"r\", encoding=\"utf8\"\n","            ) as f:\n","                example_id = 0\n","                for line in f:\n","                    if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n","                        writer.write(line)\n","                        if not predictions[example_id]:\n","                            example_id += 1\n","                    elif predictions[example_id]:\n","                        output_line = (\n","                            line.split()[0]\n","                            + \" \"\n","                            + predictions[example_id].pop(0)\n","                            + \"\\n\"\n","                        )\n","                        writer.write(output_line)\n","                    else:\n","                        logger.warning(\n","                            \"Maximum sequence length exceeded: No prediction for '%s'.\",\n","                            line.split()[0],\n","                        )\n","\n","    return results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5ns8ujVXLCGe","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKnlp0Emwdyj","colab_type":"code","colab":{}},"source":["if __name__ == \"__main__\":\n","    main(args)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YXp7xV1zauY2","colab_type":"code","colab":{}},"source":["  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7ccG9bBVaX9o","colab_type":"code","colab":{}},"source":["args = sequence_labelling_data_input(data_dir = '/content/gdrive/My Drive/ICDAR-2019-SROIE/data/output',model_type = \"layoutlm\",output_dir = \"final1\",model_name_or_path = \"data/layoutlm-base-uncased\",do_lower_case = True,max_seq_length = 512,do_predict = True,labels = '/content/gdrive/My Drive/ICDAR-2019-SROIE/data/output/labels.txt',fp16 = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oBmYRZOojiEt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":572},"executionInfo":{"status":"ok","timestamp":1600255948725,"user_tz":-330,"elapsed":34714,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"92d6e16a-2a07-4356-c6d7-276be59301fb"},"source":["if __name__ == \"__main__\":\n","    main(args)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\rEvaluating:   0%|          | 0/31 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:   3%|▎         | 1/31 [00:00<00:17,  1.76it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:   6%|▋         | 2/31 [00:01<00:16,  1.79it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  10%|▉         | 3/31 [00:01<00:15,  1.82it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  13%|█▎        | 4/31 [00:02<00:14,  1.86it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  16%|█▌        | 5/31 [00:02<00:13,  1.90it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  19%|█▉        | 6/31 [00:03<00:12,  1.95it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  23%|██▎       | 7/31 [00:03<00:12,  1.98it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  26%|██▌       | 8/31 [00:04<00:11,  2.00it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  29%|██▉       | 9/31 [00:04<00:10,  2.01it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  32%|███▏      | 10/31 [00:05<00:10,  2.02it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  35%|███▌      | 11/31 [00:05<00:09,  2.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  39%|███▊      | 12/31 [00:06<00:09,  2.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  42%|████▏     | 13/31 [00:06<00:08,  2.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  45%|████▌     | 14/31 [00:07<00:08,  2.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  48%|████▊     | 15/31 [00:07<00:07,  2.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  52%|█████▏    | 16/31 [00:08<00:07,  2.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  55%|█████▍    | 17/31 [00:08<00:06,  2.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  58%|█████▊    | 18/31 [00:08<00:06,  2.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  61%|██████▏   | 19/31 [00:09<00:05,  2.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  65%|██████▍   | 20/31 [00:09<00:05,  2.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  68%|██████▊   | 21/31 [00:10<00:04,  2.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  71%|███████   | 22/31 [00:10<00:04,  2.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  74%|███████▍  | 23/31 [00:11<00:03,  2.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  77%|███████▋  | 24/31 [00:11<00:03,  2.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  81%|████████  | 25/31 [00:12<00:02,  2.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  84%|████████▍ | 26/31 [00:12<00:02,  2.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  87%|████████▋ | 27/31 [00:13<00:01,  2.04it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  90%|█████████ | 28/31 [00:13<00:01,  2.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  94%|█████████▎| 29/31 [00:14<00:00,  2.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:  97%|█████████▋| 30/31 [00:14<00:00,  2.03it/s]"],"name":"stderr"},{"output_type":"stream","text":["check_label 21\n"],"name":"stdout"},{"output_type":"stream","text":["Evaluating: 100%|██████████| 31/31 [00:15<00:00,  2.05it/s]\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"s8dymA4_RlqN","colab_type":"code","colab":{}},"source":["  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hE4T8Lq2zDAn","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fslyQUO_zDFE","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bEhmo8B4zC9g","colab_type":"code","colab":{}},"source":["\"\"\"\n","path='/content/gdrive/My Drive/unilm/layoutlm/examples/seq_labeling/data/dataset/training_data/working/'\n","\n","\n","with open(path + 'X51005301659.txt',encoding=\"utf8\") as f: \n","  data=f.readlines()\n","label=['date', 'total', 'other', 'company', 'address']\n","words=[[{'box': ['248', '478', '380', '500'], 'text': '22/12/2017'}],[{'box': ['446', '793', '513', '815'], 'text': '15.90'}], [{'box': ['369', '286', '554', '310'], 'text': 'sa03711551-p'}],[{'box': ['369', '286', '554', '310'], 'text': 'shubham'},{'box': ['369', '286', '554', '310'], 'text': 'is'},{'box': ['369', '286', '554', '310'], 'text': 'boy'}],[{'box': ['369', '286', '554', '310'], 'text': 'live'},{'box': ['369', '286', '554', '310'], 'text': 'in'},{'box': ['369', '286', '554', '310'], 'text': 'nagpur'},{'box': ['369', '286', '554', '310'], 'text': 'city'}]]\n","\n","with open('shubham.txt','w', encoding=\"utf8\",) as fw:\n","  for i,j in zip(words,label):\n","    #words = [w for w in i if w[\"text\"].strip() != \"\"]\n","    if len(i) == 0:\n","      continue\n","    label=j  \n","    if label == \"other\":\n","      for w in i:\n","        fw.write(w[\"text\"] + \"\\tO\\n\")\n","    elif len(i) == 1:\n","      fw.write(i[0][\"text\"] + \"\\tS-\" + label.upper() + \"\\n\")\n","    else:\n","      fw.write(i[0][\"text\"] + \"\\tB-\" + label.upper() + \"\\n\")\n","      for w in i[1:-1]:\n","        fw.write(w[\"text\"] + \"\\tI-\" + label.upper() + \"\\n\")\n","      fw.write(i[-1][\"text\"] + \"\\tE-\" + label + \"\\n\")\n","  fw.write(\"\\n\")\n","\"\"\"    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zzzpkii8zC69","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QlAqpNTTzC36","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}