{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Clear_implementation_LayoutLM_Final_notebook.ipynb","provenance":[{"file_id":"1OmL70CxYX2fGqjsyOlWu90unYEqpJqM-","timestamp":1599450190174},{"file_id":"1Q9Xg6TrpMRB5SX--TnBoBkT2PJH3TtDE","timestamp":1597216091956}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b5b1be5fbf734660a2fb8dd3460a548a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_577ac93a50c64c7f8b5e2405f5f513ee","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c1925fe34df64f7188e8c213137224b0","IPY_MODEL_802767e9329946dabebdf8b69be00025"]}},"577ac93a50c64c7f8b5e2405f5f513ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c1925fe34df64f7188e8c213137224b0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_f1bfbc2534794432b7e31db31d0abb24","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":433,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":433,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2717c2a0288b41f8ab0eb9b020f95dc8"}},"802767e9329946dabebdf8b69be00025":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f0a4722e6c0b46da95a044d2b2b035ec","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 433/433 [00:00&lt;00:00, 792B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a8c2459660bc42fa9ca1a3cad85a17ea"}},"f1bfbc2534794432b7e31db31d0abb24":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2717c2a0288b41f8ab0eb9b020f95dc8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f0a4722e6c0b46da95a044d2b2b035ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"a8c2459660bc42fa9ca1a3cad85a17ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9600bc78153541efa9943d89b92a9bef":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c130102a77114c33ba025570bbab2fbb","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_dfc83456065f4fb286975b9f0ee02a08","IPY_MODEL_9ed8b250a674467baf4c6f3c6269e68a"]}},"c130102a77114c33ba025570bbab2fbb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"dfc83456065f4fb286975b9f0ee02a08":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_afcabb6b001d4925b531f532b8e2d303","_dom_classes":[],"description":"Downloading: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":231508,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":231508,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a360ca76ca9b4c60981c2e0419ed0561"}},"9ed8b250a674467baf4c6f3c6269e68a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_90848b116e54459baeb8829a6d003363","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 232k/232k [00:00&lt;00:00, 1.46MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7b47d05b3cce4db89ef71d2a18ff1dc3"}},"afcabb6b001d4925b531f532b8e2d303":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a360ca76ca9b4c60981c2e0419ed0561":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"90848b116e54459baeb8829a6d003363":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7b47d05b3cce4db89ef71d2a18ff1dc3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"id":"sE3Oy-IAOHTh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600683939531,"user_tz":-330,"elapsed":25847,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"33b5443c-064e-4f47-ca00-29a462a0d36b"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PTkyimHpOcAT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600683963610,"user_tz":-330,"elapsed":1489,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"10005a14-f757-4986-9e24-4365e8c69713"},"source":["%cd /content/gdrive/My Drive/unilm"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/gdrive/.shortcut-targets-by-id/19rY6aGdoyGH5C22rAwdx79e5pwrlXHil/unilm\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zIoHzVydOzJm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":835},"executionInfo":{"status":"ok","timestamp":1600683974592,"user_tz":-330,"elapsed":11924,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"3057837e-a746-4ffc-9172-732667f12d3c"},"source":["!pip install seqeval transformers==2.9.0 tensorboardX"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting seqeval\n","  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n","Collecting transformers==2.9.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/38/c9527aa055241c66c4d785381eaf6f80a28c224cae97daa1f8b183b5fabb/transformers-2.9.0-py3-none-any.whl (635kB)\n","\u001b[K     |████████████████████████████████| 645kB 6.3MB/s \n","\u001b[?25hCollecting tensorboardX\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n","\u001b[K     |████████████████████████████████| 317kB 20.9MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n","Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.4.3)\n","Collecting sacremoses\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n","\u001b[K     |████████████████████████████████| 890kB 24.8MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (3.0.12)\n","Collecting sentencepiece\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n","\u001b[K     |████████████████████████████████| 1.1MB 32.7MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (4.41.1)\n","Collecting tokenizers==0.7.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/e5/a26eb4716523808bb0a799fcfdceb6ebf77a18169d9591b2f46a9adb87d9/tokenizers-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 29.2MB/s \n","\u001b[?25hRequirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (0.7)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (2.23.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.0) (0.16.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (1.24.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (50.3.0)\n","Building wheels for collected packages: seqeval, sacremoses\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7423 sha256=52edc12164f88fa6a3e0caa01813d653bac0558f44cb548bd78bc199076ed63e\n","  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893257 sha256=928e831c9e9012cb08a8d6e833da424f326119e253d60247e11045787c6ac291\n","  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n","Successfully built seqeval sacremoses\n","Installing collected packages: seqeval, sacremoses, sentencepiece, tokenizers, transformers, tensorboardX\n","Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 seqeval-0.0.12 tensorboardX-2.1 tokenizers-0.7.0 transformers-2.9.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tWE3YkBgR8VG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1600683981854,"user_tz":-330,"elapsed":1037,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"e042afc7-1f87-4a33-80b1-a68293c6313c"},"source":["%cd \"/content/gdrive/My Drive/unilm/layoutlm/examples/seq_labeling\""],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/gdrive/.shortcut-targets-by-id/19rY6aGdoyGH5C22rAwdx79e5pwrlXHil/unilm/layoutlm/examples/seq_labeling\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8ZTbnZ0CQ5zb","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600683989711,"user_tz":-330,"elapsed":6378,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["import argparse\n","import json\n","import os\n","\n","from PIL import Image\n","from transformers import AutoTokenizer\n","\n","\n","def bbox_string(box, width, length):\n","    return (\n","        str(int(1000 * (box[0] / width)))\n","        + \" \"\n","        + str(int(1000 * (box[1] / length)))\n","        + \" \"\n","        + str(int(1000 * (box[2] / width)))\n","        + \" \"\n","        + str(int(1000 * (box[3] / length)))\n","    )\n","\n","\n","def actual_bbox_string(box, width, length):\n","    return (\n","        str(box[0])\n","        + \" \"\n","        + str(box[1])\n","        + \" \"\n","        + str(box[2])\n","        + \" \"\n","        + str(box[3])\n","        + \"\\t\"\n","        + str(width)\n","        + \" \"\n","        + str(length)\n","    )\n","\n","\n","def convert(args):\n","    with open(\n","        os.path.join(args.output_dir, args.data_split + \".txt.tmp\"),\n","        \"w\",\n","        encoding=\"utf8\",\n","    ) as fw, open(\n","        os.path.join(args.output_dir, args.data_split + \"_box.txt.tmp\"),\n","        \"w\",\n","        encoding=\"utf8\",\n","    ) as fbw, open(\n","        os.path.join(args.output_dir, args.data_split + \"_image.txt.tmp\"),\n","        \"w\",\n","        encoding=\"utf8\",\n","    ) as fiw:\n","        for file in os.listdir(args.data_dir):\n","            file_path = os.path.join(args.data_dir, file)\n","            with open(file_path, \"r\", encoding=\"utf8\") as f:\n","                data = json.load(f)\n","            image_path = file_path.replace(\"annotations\", \"images\")\n","            image_path = image_path.replace(\"json\", \"png\")\n","            file_name = os.path.basename(image_path)\n","            image = Image.open(image_path)\n","            width, length = image.size\n","            #print(data)\n","            for item in data[\"form\"]:\n","                words, label = item[\"words\"], item[\"label\"]\n","                print('words', words)\n","                print('labels', label)\n","                break\n","                words = [w for w in words if w[\"text\"].strip() != \"\"]\n","                #print(words)\n","                if len(words) == 0:\n","                    continue\n","                if label == \"other\":\n","                    for w in words:\n","                        fw.write(w[\"text\"] + \"\\tO\\n\")\n","                        fbw.write(\n","                            w[\"text\"]\n","                            + \"\\t\"\n","                            + bbox_string(w[\"box\"], width, length)\n","                            + \"\\n\"\n","                        )\n","                        fiw.write(\n","                            w[\"text\"]\n","                            + \"\\t\"\n","                            + actual_bbox_string(w[\"box\"], width, length)\n","                            + \"\\t\"\n","                            + file_name\n","                            + \"\\n\"\n","                        )\n","                else:\n","                    if len(words) == 1:\n","                        fw.write(words[0][\"text\"] + \"\\tS-\" + label.upper() + \"\\n\")\n","                        fbw.write(\n","                            words[0][\"text\"]\n","                            + \"\\t\"\n","                            + bbox_string(words[0][\"box\"], width, length)\n","                            + \"\\n\"\n","                        )\n","                        fiw.write(\n","                            words[0][\"text\"]\n","                            + \"\\t\"\n","                            + actual_bbox_string(words[0][\"box\"], width, length)\n","                            + \"\\t\"\n","                            + file_name\n","                            + \"\\n\"\n","                        )\n","                    else:\n","                        fw.write(words[0][\"text\"] + \"\\tB-\" + label.upper() + \"\\n\")\n","                        fbw.write(\n","                            words[0][\"text\"]\n","                            + \"\\t\"\n","                            + bbox_string(words[0][\"box\"], width, length)\n","                            + \"\\n\"\n","                        )\n","                        fiw.write(\n","                            words[0][\"text\"]\n","                            + \"\\t\"\n","                            + actual_bbox_string(words[0][\"box\"], width, length)\n","                            + \"\\t\"\n","                            + file_name\n","                            + \"\\n\"\n","                        )\n","                        for w in words[1:-1]:\n","                            fw.write(w[\"text\"] + \"\\tI-\" + label.upper() + \"\\n\")\n","                            fbw.write(\n","                                w[\"text\"]\n","                                + \"\\t\"\n","                                + bbox_string(w[\"box\"], width, length)\n","                                + \"\\n\"\n","                            )\n","                            fiw.write(\n","                                w[\"text\"]\n","                                + \"\\t\"\n","                                + actual_bbox_string(w[\"box\"], width, length)\n","                                + \"\\t\"\n","                                + file_name\n","                                + \"\\n\"\n","                            )\n","                        fw.write(words[-1][\"text\"] + \"\\tE-\" + label.upper() + \"\\n\")\n","                        fbw.write(\n","                            words[-1][\"text\"]\n","                            + \"\\t\"\n","                            + bbox_string(words[-1][\"box\"], width, length)\n","                            + \"\\n\"\n","                        )\n","                        fiw.write(\n","                            words[-1][\"text\"]\n","                            + \"\\t\"\n","                            + actual_bbox_string(words[-1][\"box\"], width, length)\n","                            + \"\\t\"\n","                            + file_name\n","                            + \"\\n\"\n","                        )\n","            break\n","            fw.write(\"\\n\")\n","            fbw.write(\"\\n\")\n","            fiw.write(\"\\n\")\n","\n","\n","def seg_file(file_path, tokenizer, max_len):\n","    subword_len_counter = 0\n","    output_path = file_path[:-4]\n","    with open(file_path, \"r\", encoding=\"utf8\") as f_p, open(\n","        output_path, \"w\", encoding=\"utf8\"\n","    ) as fw_p:\n","        for line in f_p:\n","            line = line.rstrip()\n","\n","            if not line:\n","                fw_p.write(line + \"\\n\")\n","                subword_len_counter = 0\n","                continue\n","            token = line.split(\"\\t\")[0]\n","\n","            current_subwords_len = len(tokenizer.tokenize(token))\n","\n","            # Token contains strange control characters like \\x96 or \\x95\n","            # Just filter out the complete line\n","            if current_subwords_len == 0:\n","                continue\n","\n","            if (subword_len_counter + current_subwords_len) > max_len:\n","                fw_p.write(\"\\n\" + line + \"\\n\")\n","                subword_len_counter = current_subwords_len\n","                continue\n","\n","            subword_len_counter += current_subwords_len\n","\n","            fw_p.write(line + \"\\n\")\n","\n","\n","def seg(args):\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        args.model_name_or_path, do_lower_case=True\n","    )\n","    seg_file(\n","        os.path.join(args.output_dir, args.data_split + \".txt.tmp\"),\n","        tokenizer,\n","        args.max_len,\n","    )\n","    seg_file(\n","        os.path.join(args.output_dir, args.data_split + \"_box.txt.tmp\"),\n","        tokenizer,\n","        args.max_len,\n","    )\n","    seg_file(\n","        os.path.join(args.output_dir, args.data_split + \"_image.txt.tmp\"),\n","        tokenizer,\n","        args.max_len,\n","    )"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"lYYh2oKYFbJP","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VumQErgPjZEm","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mz4fzziAV19V","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600683989712,"user_tz":-330,"elapsed":2025,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["class preprocessing_data_input:\n","  def __init__(self,data_dir = \"data/training_data/annotations\",data_split = \"train\",output_dir=\"data\",model_name_or_path=\"bert-base-uncased\",max_len=510):\n","    self.data_dir = data_dir\n","    self.data_split = data_split\n","    self.output_dir = output_dir\n","    self.model_name_or_path = model_name_or_path\n","    self.max_len = max_len"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_ccxEXdPYJAd","colab_type":"text"},"source":["preparing/preprocessing data of FUNSD for training\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"naj9Q2B4YGZC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":151,"referenced_widgets":["b5b1be5fbf734660a2fb8dd3460a548a","577ac93a50c64c7f8b5e2405f5f513ee","c1925fe34df64f7188e8c213137224b0","802767e9329946dabebdf8b69be00025","f1bfbc2534794432b7e31db31d0abb24","2717c2a0288b41f8ab0eb9b020f95dc8","f0a4722e6c0b46da95a044d2b2b035ec","a8c2459660bc42fa9ca1a3cad85a17ea","9600bc78153541efa9943d89b92a9bef","c130102a77114c33ba025570bbab2fbb","dfc83456065f4fb286975b9f0ee02a08","9ed8b250a674467baf4c6f3c6269e68a","afcabb6b001d4925b531f532b8e2d303","a360ca76ca9b4c60981c2e0419ed0561","90848b116e54459baeb8829a6d003363","7b47d05b3cce4db89ef71d2a18ff1dc3"]},"executionInfo":{"status":"ok","timestamp":1600683998409,"user_tz":-330,"elapsed":5256,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"2b70f985-fc40-4b5f-caa3-bcce17e44206"},"source":["\n","args = preprocessing_data_input(\"/content/gdrive/My Drive/unilm/layoutlm/examples/seq_labeling/data/dataset/training_data/annotations\",\"train\",\"data\",\"bert-base-uncased\",510)\n","convert(args)\n","seg(args)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["words [{'box': [141, 249, 175, 260], 'text': 'BRAND'}]\n","labels question\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b5b1be5fbf734660a2fb8dd3460a548a","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9600bc78153541efa9943d89b92a9bef","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hKpgikoVbwle","colab_type":"text"},"source":["preparing/preprocessing data for testing"]},{"cell_type":"code","metadata":{"id":"9qf_1tlOYQkj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"status":"ok","timestamp":1600684004018,"user_tz":-330,"elapsed":2946,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"20bd0ff0-6000-4677-8418-a47fd27c913b"},"source":["args = preprocessing_data_input(\"/content/gdrive/My Drive/unilm/layoutlm/examples/seq_labeling/data/dataset/testing_data/annotations\",\"test\",\"data\",\"bert-base-uncased\",510)\n","convert(args)\n","seg(args)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["words [{'box': [78, 124, 100, 139], 'text': 'TO:'}]\n","labels question\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2ZfZ5fagpqHw","colab_type":"text"},"source":["**creating labels.txt**"]},{"cell_type":"code","metadata":{"id":"sxZ2a2R3p8pl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600684005002,"user_tz":-330,"elapsed":1275,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["!cat data/train.txt | cut -d$'\\t' -f 2 | grep -v \"^$\"| sort | uniq > data/labels.txt\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j77A2FlUqyNh","colab_type":"text"},"source":["**train the model**"]},{"cell_type":"markdown","metadata":{"id":"1V63aG5q-LJu","colab_type":"text"},"source":["***layoutlm.py***"]},{"cell_type":"code","metadata":{"id":"ripg2WCb97Zr","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600684075498,"user_tz":-330,"elapsed":1256,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["import logging\n","\n","import torch\n","from torch import nn\n","from torch.nn import CrossEntropyLoss, MSELoss\n","from transformers import BertConfig, BertModel, BertPreTrainedModel\n","from transformers.modeling_bert import BertLayerNorm\n","\n","logger = logging.getLogger(__name__)\n","\n","LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP = {}\n","\n","LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP = {}\n","\n","\n","class LayoutlmConfig(BertConfig):\n","    pretrained_config_archive_map = LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP\n","    model_type = \"bert\"\n","\n","    def __init__(self, max_2d_position_embeddings=1024, **kwargs):\n","        super().__init__(**kwargs)\n","        self.max_2d_position_embeddings = max_2d_position_embeddings\n","\n","\n","class LayoutlmEmbeddings(nn.Module): \n","    def __init__(self, config):\n","        super(LayoutlmEmbeddings, self).__init__()\n","        self.word_embeddings = nn.Embedding(\n","            config.vocab_size, config.hidden_size, padding_idx=0\n","        )\n","        self.position_embeddings = nn.Embedding(\n","            config.max_position_embeddings, config.hidden_size\n","        )\n","        self.x_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.y_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.h_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.w_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.token_type_embeddings = nn.Embedding(\n","            config.type_vocab_size, config.hidden_size\n","        )\n","\n","        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n","        # any TensorFlow checkpoint file\n","        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        token_type_ids=None,\n","        position_ids=None,\n","        inputs_embeds=None,\n","    ):\n","        seq_length = input_ids.size(1)\n","        if position_ids is None:\n","            position_ids = torch.arange(\n","                seq_length, dtype=torch.long, device=input_ids.device\n","            )\n","            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        words_embeddings = self.word_embeddings(input_ids)\n","        position_embeddings = self.position_embeddings(position_ids)\n","        left_position_embeddings = self.x_position_embeddings(bbox[:, :, 0])\n","        upper_position_embeddings = self.y_position_embeddings(bbox[:, :, 1])\n","        right_position_embeddings = self.x_position_embeddings(bbox[:, :, 2])\n","        lower_position_embeddings = self.y_position_embeddings(bbox[:, :, 3])\n","        h_position_embeddings = self.h_position_embeddings(\n","            bbox[:, :, 3] - bbox[:, :, 1]\n","        )\n","      ##  print(\"\\n bbox shape :- \",bbox.size())\n","       # print(\"\\nbbox[:,:,2] :- \", bbox[:, :, 2])\n","       # print(\"\\nbbox[:,:,2] shape :- \", bbox[:, :, 2].shape)\n","       # print(\"\\nbbox[:, :, 0] :- \", bbox[:, :, 0])\n","       # print(\"\\nbbox[:, :, 0] shape :- \", bbox[:, :, 0].shape)\n","       # print(\"\\n sub of two above matrix :- \",bbox[:, :, 2] - bbox[:, :, 0])\n","       # print(\"\\n printing the w_position_embeddings:- \",self.w_position_embeddings)\n","        bboxshape=bbox[:, :, 2] - bbox[:, :, 0]\n","       # print(\"\\n shape of the resultant sub array :- \",bboxshape.size())\n","        w_position_embeddings = self.w_position_embeddings(\n","            bbox[:, :, 2] - bbox[:, :, 0]\n","        )\n","        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n","\n","        embeddings = (\n","            words_embeddings\n","            + position_embeddings\n","            + left_position_embeddings\n","            + upper_position_embeddings\n","            + right_position_embeddings\n","            + lower_position_embeddings\n","            + h_position_embeddings\n","            + w_position_embeddings\n","            + token_type_embeddings\n","        )\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","        return embeddings\n","\n","\n","class LayoutlmModel(BertModel):\n","\n","    config_class = LayoutlmConfig\n","    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, config):\n","        super(LayoutlmModel, self).__init__(config)\n","        self.embeddings = LayoutlmEmbeddings(config)\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","    ):\n","        if attention_mask is None:\n","            attention_mask = torch.ones_like(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        # We create a 3D attention mask from a 2D tensor mask.\n","        # Sizes are [batch_size, 1, 1, to_seq_length]\n","        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n","        # this attention mask is more simple than the triangular masking of causal attention\n","        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n","        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n","\n","        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n","        # masked positions, this operation will create a tensor which is 0.0 for\n","        # positions we want to attend and -10000.0 for masked positions.\n","        # Since we are adding it to the raw scores before the softmax, this is\n","        # effectively the same as removing these entirely.\n","        extended_attention_mask = extended_attention_mask.to(\n","            dtype=next(self.parameters()).dtype\n","        )  # fp16 compatibility\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","        # Prepare head mask if needed\n","        # 1.0 in head_mask indicate we keep the head\n","        # attention_probs has shape bsz x n_heads x N x N\n","        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n","        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n","        if head_mask is not None:\n","            if head_mask.dim() == 1:\n","                head_mask = (\n","                    head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n","                )\n","                head_mask = head_mask.expand(\n","                    self.config.num_hidden_layers, -1, -1, -1, -1\n","                )\n","            elif head_mask.dim() == 2:\n","                head_mask = (\n","                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n","                )  # We can specify head_mask for each layer\n","            head_mask = head_mask.to(\n","                dtype=next(self.parameters()).dtype\n","            )  # switch to fload if need + fp16 compatibility\n","        else:\n","            head_mask = [None] * self.config.num_hidden_layers\n","\n","       # print(\"\\n The arguments in the embedding layer :- \\n\\n\\n\")\n","       # print(\"\\n input_ids in embedding layer :- \",input_ids)\n","       # print(\"\\n position_ids in embedding layer :- \",position_ids)\n","       # print(\"\\n token_type_ids in embedding layer :- \",token_type_ids)\n","       # print(\"\\n input_ids in embedding layer :- \\n\\n\")\n","        embedding_output = self.embeddings(\n","            input_ids, bbox, position_ids=position_ids, token_type_ids=token_type_ids\n","        )\n","        encoder_outputs = self.encoder(\n","            embedding_output, extended_attention_mask, head_mask=head_mask\n","        )\n","        sequence_output = encoder_outputs[0]\n","        pooled_output = self.pooler(sequence_output)\n","\n","        outputs = (sequence_output, pooled_output) + encoder_outputs[\n","            1:\n","        ]  # add hidden_states and attentions if they are here\n","        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n","\n","\n","class LayoutlmForTokenClassification(BertPreTrainedModel):\n","    config_class = LayoutlmConfig\n","    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        print('num_labels', self.num_labels)\n","        self.bert = LayoutlmModel(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","    ):\n","\n","        outputs = self.bert(\n","            input_ids=input_ids,\n","            bbox=bbox,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","\n","        outputs = (logits,) + outputs[\n","            2:\n","        ]  # add hidden states and attention if they are here\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.num_labels)[active_loss]\n","                active_labels = labels.view(-1)[active_loss]\n","                loss = loss_fct(active_logits, active_labels)\n","            else:\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), scores, (hidden_states), (attentions)\n","\n","\n","class LayoutlmForSequenceClassification(BertPreTrainedModel):\n","    config_class = LayoutlmConfig\n","    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, config):\n","        super(LayoutlmForSequenceClassification, self).__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.bert = LayoutlmModel(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, self.config.num_labels)\n","\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","    ):\n","\n","        outputs = self.bert(\n","            input_ids=input_ids,\n","            bbox=bbox,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","        )\n","\n","        pooled_output = outputs[1]\n","\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","\n","        outputs = (logits,) + outputs[\n","            2:\n","        ]  # add hidden states and attention if they are here\n","\n","        if labels is not None:\n","            if self.num_labels == 1:\n","                #  We are doing regression\n","                loss_fct = MSELoss()\n","                loss = loss_fct(logits.view(-1), labels.view(-1))\n","            else:\n","                loss_fct = CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)\n"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"OMiFP4688F6V","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x61CAAmf-hay","colab_type":"text"},"source":["***funsd.py***"]},{"cell_type":"code","metadata":{"id":"cQsu_uQj-W1l","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600684091191,"user_tz":-330,"elapsed":2209,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["import logging\n","import os\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","class FunsdDataset(Dataset):\n","    def __init__(self, args, tokenizer, labels, pad_token_label_id, mode):\n","        if args.local_rank not in [-1, 0] and mode == \"train\":\n","            torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","\n","        # Load data features from cache or dataset file\n","        cached_features_file = os.path.join(\n","            args.data_dir,\n","            \"cached_{}_{}_{}\".format(\n","                mode,\n","                list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n","                str(args.max_seq_length),\n","            ),\n","        )\n","        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n","            logger.info(\"Loading features from cached file %s\", cached_features_file)\n","            features = torch.load(cached_features_file)\n","        else:\n","            logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n","            examples = read_examples_from_file(args.data_dir, mode)\n","            features = convert_examples_to_features(\n","                examples,\n","                labels,\n","                args.max_seq_length,\n","                tokenizer,\n","                cls_token_at_end=bool(args.model_type in [\"xlnet\"]),\n","                # xlnet has a cls token at the end\n","                cls_token=tokenizer.cls_token,\n","                cls_token_segment_id=2 if args.model_type in [\"xlnet\"] else 0,\n","                sep_token=tokenizer.sep_token,\n","                sep_token_extra=bool(args.model_type in [\"roberta\"]),\n","                # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n","                pad_on_left=bool(args.model_type in [\"xlnet\"]),\n","                # pad on the left for xlnet\n","                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n","                pad_token_segment_id=4 if args.model_type in [\"xlnet\"] else 0,\n","                pad_token_label_id=pad_token_label_id,\n","            )\n","            if args.local_rank in [-1, 0]:\n","                logger.info(\"Saving features into cached file %s\", cached_features_file)\n","                torch.save(features, cached_features_file)\n","\n","        if args.local_rank == 0 and mode == \"train\":\n","            torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","\n","        self.features = features\n","        # Convert to Tensors and build dataset\n","        self.all_input_ids = torch.tensor(\n","            [f.input_ids for f in features], dtype=torch.long\n","        )\n","        self.all_input_mask = torch.tensor(\n","            [f.input_mask for f in features], dtype=torch.long\n","        )\n","        self.all_segment_ids = torch.tensor(\n","            [f.segment_ids for f in features], dtype=torch.long\n","        )\n","        self.all_label_ids = torch.tensor(\n","            [f.label_ids for f in features], dtype=torch.long\n","        )\n","        self.all_bboxes = torch.tensor([f.boxes for f in features], dtype=torch.long)\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, index):\n","        return (\n","            self.all_input_ids[index],\n","            self.all_input_mask[index],\n","            self.all_segment_ids[index],\n","            self.all_label_ids[index],\n","            self.all_bboxes[index],\n","        )\n","\n","\n","class InputExample(object):\n","    \"\"\"A single training/test example for token classification.\"\"\"\n","\n","    def __init__(self, guid, words, labels, boxes, actual_bboxes, file_name, page_size):\n","        \"\"\"Constructs a InputExample.\n","\n","        Args:\n","            guid: Unique id for the example.\n","            words: list. The words of the sequence.\n","            labels: (Optional) list. The labels for each word of the sequence. This should be\n","            specified for train and dev examples, but not for test examples.\n","        \"\"\"\n","        self.guid = guid\n","        self.words = words\n","        self.labels = labels\n","        self.boxes = boxes\n","        self.actual_bboxes = actual_bboxes\n","        self.file_name = file_name\n","        self.page_size = page_size\n","\n","\n","class InputFeatures(object):\n","    \"\"\"A single set of features of data.\"\"\"\n","\n","    def __init__(\n","        self,\n","        input_ids,\n","        input_mask,\n","        segment_ids,\n","        label_ids,\n","        boxes,\n","        actual_bboxes,\n","        file_name,\n","        page_size,\n","    ):\n","        assert (\n","            0 <= all(boxes) <= 1000\n","        ), \"Error with input bbox ({}): the coordinate value is not between 0 and 1000\".format(\n","            boxes\n","        )\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.label_ids = label_ids\n","        self.boxes = boxes\n","        self.actual_bboxes = actual_bboxes\n","        self.file_name = file_name\n","        self.page_size = page_size\n","\n","\n","def read_examples_from_file(data_dir, mode):\n","    file_path = os.path.join(data_dir, \"{}.txt\".format(mode))\n","    box_file_path = os.path.join(data_dir, \"{}_box.txt\".format(mode))\n","    image_file_path = os.path.join(data_dir, \"{}_image.txt\".format(mode))\n","    guid_index = 1\n","    examples = []\n","    with open(file_path, encoding=\"utf-8\") as f, open(\n","        box_file_path, encoding=\"utf-8\"\n","    ) as fb, open(image_file_path, encoding=\"utf-8\") as fi:\n","        words = []\n","        boxes = []\n","        actual_bboxes = []\n","        file_name = None\n","        page_size = None\n","        labels = []\n","        for line, bline, iline in zip(f, fb, fi):\n","            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n","                if words:\n","                    examples.append(\n","                        InputExample(\n","                            guid=\"{}-{}\".format(mode, guid_index),\n","                            words=words,\n","                            labels=labels,\n","                            boxes=boxes,\n","                            actual_bboxes=actual_bboxes,\n","                            file_name=file_name,\n","                            page_size=page_size,\n","                        )\n","                    )\n","                    guid_index += 1\n","                    words = []\n","                    boxes = []\n","                    actual_bboxes = []\n","                    file_name = None\n","                    page_size = None\n","                    labels = []\n","            else:\n","                splits = line.split(\"\\t\")\n","                bsplits = bline.split(\"\\t\")\n","                isplits = iline.split(\"\\t\")\n","                assert len(splits) == 2\n","                assert len(bsplits) == 2\n","                assert len(isplits) == 4\n","                assert splits[0] == bsplits[0]\n","                words.append(splits[0])\n","                if len(splits) > 1:\n","                    labels.append(splits[-1].replace(\"\\n\", \"\"))\n","                    box = bsplits[-1].replace(\"\\n\", \"\")\n","                    box = [int(b) for b in box.split()]\n","                    boxes.append(box)\n","                    actual_bbox = [int(b) for b in isplits[1].split()]\n","                    actual_bboxes.append(actual_bbox)\n","                    page_size = [int(i) for i in isplits[2].split()]\n","                    file_name = isplits[3].strip()\n","                else:\n","                    # Examples could have no label for mode = \"test\"\n","                    labels.append(\"O\")\n","        if words:\n","            examples.append(\n","                InputExample(\n","                    guid=\"%s-%d\".format(mode, guid_index),\n","                    words=words,\n","                    labels=labels,\n","                    boxes=boxes,\n","                    actual_bboxes=actual_bboxes,\n","                    file_name=file_name,\n","                    page_size=page_size,\n","                )\n","            )\n","    return examples\n","\n","\n","def convert_examples_to_features(\n","    examples,\n","    label_list,\n","    max_seq_length,\n","    tokenizer,\n","    cls_token_at_end=False,\n","    cls_token=\"[CLS]\",\n","    cls_token_segment_id=1,\n","    sep_token=\"[SEP]\",\n","    sep_token_extra=False,\n","    pad_on_left=False,\n","    pad_token=0,\n","    cls_token_box=[0, 0, 0, 0],\n","    sep_token_box=[1000, 1000, 1000, 1000],\n","    pad_token_box=[0, 0, 0, 0],\n","    pad_token_segment_id=0,\n","    pad_token_label_id=-1,\n","    sequence_a_segment_id=0,\n","    mask_padding_with_zero=True,\n","):\n","    \"\"\" Loads a data file into a list of `InputBatch`s\n","        `cls_token_at_end` define the location of the CLS token:\n","            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n","            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n","        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n","    \"\"\"\n","\n","    label_map = {label: i for i, label in enumerate(label_list)}\n","\n","    features = []\n","    for (ex_index, example) in enumerate(examples):\n","        file_name = example.file_name\n","        page_size = example.page_size\n","        width, height = page_size\n","        if ex_index % 10000 == 0:\n","            logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n","\n","        tokens = []\n","        token_boxes = []\n","        actual_bboxes = []\n","        label_ids = []\n","        for word, label, box, actual_bbox in zip(\n","            example.words, example.labels, example.boxes, example.actual_bboxes\n","        ):\n","            word_tokens = tokenizer.tokenize(word)\n","            tokens.extend(word_tokens)\n","            token_boxes.extend([box] * len(word_tokens))\n","            actual_bboxes.extend([actual_bbox] * len(word_tokens))\n","            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n","            label_ids.extend(\n","                [label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1)\n","            )\n","\n","        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n","        special_tokens_count = 3 if sep_token_extra else 2\n","        if len(tokens) > max_seq_length - special_tokens_count:\n","            tokens = tokens[: (max_seq_length - special_tokens_count)]\n","            token_boxes = token_boxes[: (max_seq_length - special_tokens_count)]\n","            actual_bboxes = actual_bboxes[: (max_seq_length - special_tokens_count)]\n","            label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n","\n","        # The convention in BERT is:\n","        # (a) For sequence pairs:\n","        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n","        # (b) For single sequences:\n","        #  tokens:   [CLS] the dog is hairy . [SEP]\n","        #  type_ids:   0   0   0   0  0     0   0\n","        #\n","        # Where \"type_ids\" are used to indicate whether this is the first\n","        # sequence or the second sequence. The embedding vectors for `type=0` and\n","        # `type=1` were learned during pre-training and are added to the wordpiece\n","        # embedding vector (and position vector). This is not *strictly* necessary\n","        # since the [SEP] token unambiguously separates the sequences, but it makes\n","        # it easier for the model to learn the concept of sequences.\n","        #\n","        # For classification tasks, the first vector (corresponding to [CLS]) is\n","        # used as as the \"sentence vector\". Note that this only makes sense because\n","        # the entire model is fine-tuned.\n","        tokens += [sep_token]\n","        token_boxes += [sep_token_box]\n","        actual_bboxes += [[0, 0, width, height]]\n","        label_ids += [pad_token_label_id]\n","        if sep_token_extra:\n","            # roberta uses an extra separator b/w pairs of sentences\n","            tokens += [sep_token]\n","            token_boxes += [sep_token_box]\n","            actual_bboxes += [[0, 0, width, height]]\n","            label_ids += [pad_token_label_id]\n","        segment_ids = [sequence_a_segment_id] * len(tokens)\n","\n","        if cls_token_at_end:\n","            tokens += [cls_token]\n","            token_boxes += [cls_token_box]\n","            actual_bboxes += [[0, 0, width, height]]\n","            label_ids += [pad_token_label_id]\n","            segment_ids += [cls_token_segment_id]\n","        else:\n","            tokens = [cls_token] + tokens\n","            token_boxes = [cls_token_box] + token_boxes\n","            actual_bboxes = [[0, 0, width, height]] + actual_bboxes\n","            label_ids = [pad_token_label_id] + label_ids\n","            segment_ids = [cls_token_segment_id] + segment_ids\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","        # tokens are attended to.\n","        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n","\n","        # Zero-pad up to the sequence length.\n","        padding_length = max_seq_length - len(input_ids)\n","        if pad_on_left:\n","            input_ids = ([pad_token] * padding_length) + input_ids\n","            input_mask = (\n","                [0 if mask_padding_with_zero else 1] * padding_length\n","            ) + input_mask\n","            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n","            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n","            token_boxes = ([pad_token_box] * padding_length) + token_boxes\n","        else:\n","            input_ids += [pad_token] * padding_length\n","            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n","            segment_ids += [pad_token_segment_id] * padding_length\n","            label_ids += [pad_token_label_id] * padding_length\n","            token_boxes += [pad_token_box] * padding_length\n","\n","        assert len(input_ids) == max_seq_length\n","        assert len(input_mask) == max_seq_length\n","        assert len(segment_ids) == max_seq_length\n","        assert len(label_ids) == max_seq_length\n","        assert len(token_boxes) == max_seq_length\n","\n","        if ex_index < 5:\n","            logger.info(\"*** Example ***\")\n","            logger.info(\"guid: %s\", example.guid)\n","            logger.info(\"tokens: %s\", \" \".join([str(x) for x in tokens]))\n","            logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n","            logger.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n","            logger.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n","            logger.info(\"label_ids: %s\", \" \".join([str(x) for x in label_ids]))\n","            logger.info(\"boxes: %s\", \" \".join([str(x) for x in token_boxes]))\n","            logger.info(\"actual_bboxes: %s\", \" \".join([str(x) for x in actual_bboxes]))\n","\n","        features.append(\n","            InputFeatures(\n","                input_ids=input_ids,\n","                input_mask=input_mask,\n","                segment_ids=segment_ids,\n","                label_ids=label_ids,\n","                boxes=token_boxes,\n","                actual_bboxes=actual_bboxes,\n","                file_name=file_name,\n","                page_size=page_size,\n","            )\n","        )\n","    return features\n"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zVJQt2N0q46e","colab_type":"text"},"source":["****run_sequence_labeling.py****"]},{"cell_type":"code","metadata":{"id":"T5CKUcz8q4ao","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600684107535,"user_tz":-330,"elapsed":1768,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["\"\"\" Fine-tuning the library models for named entity recognition on CoNLL-2003 (Bert or Roberta). \"\"\"\n","\n","from __future__ import absolute_import, division, print_function\n","\n","import argparse\n","import glob\n","import logging\n","import os\n","import random\n","import shutil\n","\n","import numpy as np\n","import torch\n","from seqeval.metrics import (\n","    classification_report,\n","    f1_score,\n","    precision_score,\n","    recall_score,\n",")\n","from tensorboardX import SummaryWriter\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm import tqdm, trange\n","from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    BertConfig,\n","    BertForTokenClassification,\n","    BertTokenizer,\n","    RobertaConfig,\n","    RobertaForTokenClassification,\n","    RobertaTokenizer,\n","    get_linear_schedule_with_warmup,\n",")\n","'''\n","Other file imports {take only important files for deployment}\n","'''\n","#from layoutlm import FunsdDataset, LayoutlmConfig, LayoutlmForTokenClassification\n","\n","logger = logging.getLogger(__name__)\n","\n","ALL_MODELS = sum(\n","    (\n","        tuple(conf.pretrained_config_archive_map.keys())\n","        for conf in (BertConfig, RobertaConfig, LayoutlmConfig)\n","    ),\n","    (),\n",")\n","\n","MODEL_CLASSES = {\n","    \"bert\": (BertConfig, BertForTokenClassification, BertTokenizer),\n","    \"roberta\": (RobertaConfig, RobertaForTokenClassification, RobertaTokenizer),\n","    \"layoutlm\": (LayoutlmConfig, LayoutlmForTokenClassification, BertTokenizer),\n","}\n","\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if args.n_gpu > 0:\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","\n","def collate_fn(data):\n","    batch = [i for i in zip(*data)]\n","    for i in range(len(batch)):\n","        if i < len(batch) - 2:\n","            batch[i] = torch.stack(batch[i], 0)\n","    return tuple(batch)\n","\n","\n","def get_labels(path):\n","    with open(path, \"r\") as f:\n","        labels = f.read().splitlines()\n","    if \"O\" not in labels:\n","        labels = [\"O\"] + labels\n","    return labels\n","\n","\n","def train(  # noqa C901\n","    args, train_dataset, model, tokenizer, labels, pad_token_label_id\n","):\n","    \"\"\" Train the model \"\"\"\n","    if args.local_rank in [-1, 0]:\n","        tb_writer = SummaryWriter(logdir=\"runs/\" + os.path.basename(args.output_dir))\n","\n","    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n","    train_sampler = (\n","        RandomSampler(train_dataset)\n","        if args.local_rank == -1\n","        else DistributedSampler(train_dataset)\n","    )\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        sampler=train_sampler,\n","        batch_size=args.train_batch_size,\n","        collate_fn=None,\n","    )\n","\n","    if args.max_steps > 0:\n","        t_total = args.max_steps\n","        args.num_train_epochs = (\n","            args.max_steps\n","            // (len(train_dataloader) // args.gradient_accumulation_steps)\n","            + 1\n","        )\n","    else:\n","        t_total = (\n","            len(train_dataloader)\n","            // args.gradient_accumulation_steps\n","            * args.num_train_epochs\n","        )\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [\n","                p\n","                for n, p in model.named_parameters()\n","                if not any(nd in n for nd in no_decay)\n","            ],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\n","            \"params\": [\n","                p\n","                for n, p in model.named_parameters()\n","                if any(nd in n for nd in no_decay)\n","            ],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon\n","    )\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n","    )\n","    if args.fp16:\n","        try:\n","            from apex import amp\n","        except ImportError:\n","            raise ImportError(\n","                \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"\n","            )\n","        model, optimizer = amp.initialize(\n","            model, optimizer, opt_level=args.fp16_opt_level\n","        )\n","\n","    # multi-gpu training (should be after apex fp16 initialization)\n","    if args.n_gpu > 1:\n","        model = torch.nn.DataParallel(model)\n","\n","    # Distributed training (should be after apex fp16 initialization)\n","    if args.local_rank != -1:\n","        model = torch.nn.parallel.DistributedDataParallel(\n","            model,\n","            device_ids=[args.local_rank],\n","            output_device=args.local_rank,\n","            find_unused_parameters=True,\n","        )\n","\n","    # Train!\n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num examples = %d\", len(train_dataset))\n","    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n","    logger.info(\n","        \"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size\n","    )\n","    logger.info(\n","        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n","        args.train_batch_size\n","        * args.gradient_accumulation_steps\n","        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n","    )\n","    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n","    logger.info(\"  Total optimization steps = %d\", t_total)\n","\n","    global_step = 0\n","    tr_loss, logging_loss = 0.0, 0.0\n","    model.zero_grad()\n","    train_iterator = trange(\n","        int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n","    )\n","    set_seed(args)  # Added here for reproductibility (even between python 2 and 3)\n","    for _ in train_iterator:\n","        epoch_iterator = tqdm(\n","            train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0]\n","        )\n","        for step, batch in enumerate(epoch_iterator):\n","            model.train()\n","            inputs = {\n","                \"input_ids\": batch[0].to(args.device),\n","                \"attention_mask\": batch[1].to(args.device),\n","                \"labels\": batch[3].to(args.device),\n","            }\n","            if args.model_type in [\"layoutlm\"]:\n","                inputs[\"bbox\"] = batch[4].to(args.device)\n","            inputs[\"token_type_ids\"] = (\n","                batch[2].to(args.device) if args.model_type in [\"bert\", \"layoutlm\"] else None\n","            )  # RoBERTa don\"t use segment_ids\n","            outputs = model(**inputs)\n","            # model outputs are always tuple in pytorch-transformers (see doc)\n","            loss = outputs[0]\n","\n","            if args.n_gpu > 1:\n","                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            tr_loss += loss.item()\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                if args.fp16:\n","                    torch.nn.utils.clip_grad_norm_(\n","                        amp.master_params(optimizer), args.max_grad_norm\n","                    )\n","                else:\n","                    torch.nn.utils.clip_grad_norm_(\n","                        model.parameters(), args.max_grad_norm\n","                    )\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","\n","                if (\n","                    args.local_rank in [-1, 0]\n","                    and args.logging_steps > 0\n","                    and global_step % args.logging_steps == 0\n","                ):\n","                    # Log metrics\n","                    if (\n","                        args.local_rank in [-1, 0] and args.evaluate_during_training\n","                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n","                        results, _ = evaluate(\n","                            args,\n","                            model,\n","                            tokenizer,\n","                            labels,\n","                            pad_token_label_id,\n","                            mode=\"dev\",\n","                        )\n","                        for key, value in results.items():\n","                            tb_writer.add_scalar(\n","                                \"eval_{}\".format(key), value, global_step\n","                            )\n","                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n","                    tb_writer.add_scalar(\n","                        \"loss\",\n","                        (tr_loss - logging_loss) / args.logging_steps,\n","                        global_step,\n","                    )\n","                    logging_loss = tr_loss\n","\n","                if (\n","                    args.local_rank in [-1, 0]\n","                    and args.save_steps > 0\n","                    and global_step % args.save_steps == 0\n","                ):\n","                    # Save model checkpoint\n","                    output_dir = os.path.join(\n","                        args.output_dir, \"checkpoint-{}\".format(global_step)\n","                    )\n","                    if not os.path.exists(output_dir):\n","                        os.makedirs(output_dir)\n","                    model_to_save = (\n","                        model.module if hasattr(model, \"module\") else model\n","                    )  # Take care of distributed/parallel training\n","                    model_to_save.save_pretrained(output_dir)\n","                    tokenizer.save_pretrained(output_dir)\n","                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n","                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n","\n","            if args.max_steps > 0 and global_step > args.max_steps:\n","                epoch_iterator.close()\n","                break\n","        if args.max_steps > 0 and global_step > args.max_steps:\n","            train_iterator.close()\n","            break\n","\n","    if args.local_rank in [-1, 0]:\n","        tb_writer.close()\n","\n","    return global_step, tr_loss / global_step\n","\n","\n","def evaluate(args, model, tokenizer, labels, pad_token_label_id, mode, prefix=\"\"):\n","    eval_dataset = FunsdDataset(args, tokenizer, labels, pad_token_label_id, mode=mode)\n","\n","    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n","    eval_sampler = SequentialSampler(eval_dataset)\n","    eval_dataloader = DataLoader(\n","        eval_dataset,\n","        sampler=eval_sampler,\n","        batch_size=args.eval_batch_size,\n","        collate_fn=None,\n","    )\n","\n","    # Eval!\n","    logger.info(\"***** Running evaluation %s *****\", prefix)\n","    logger.info(\"  Num examples = %d\", len(eval_dataset))\n","    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n","    eval_loss = 0.0\n","    nb_eval_steps = 0\n","    preds = None\n","    out_label_ids = None\n","    model.eval()\n","    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        with torch.no_grad():\n","            inputs = {\n","                \"input_ids\": batch[0].to(args.device),\n","                \"attention_mask\": batch[1].to(args.device),\n","                \"labels\": batch[3].to(args.device),\n","            }\n","            if args.model_type in [\"layoutlm\"]:\n","                inputs[\"bbox\"] = batch[4].to(args.device)\n","            inputs[\"token_type_ids\"] = (\n","                batch[2].to(args.device)\n","                if args.model_type in [\"bert\", \"layoutlm\"]\n","                else None\n","            )  # RoBERTa don\"t use segment_ids\n","           # print(\"\\n\\n lenght :- \",len(inputs))\n","           # print(\"\\n\\n keys :- \",inputs.keys())\n","           # print(\"\\n\\n\\n\\n\",)\n","            outputs = model(**inputs)\n","            tmp_eval_loss, logits = outputs[:2]\n","\n","            if args.n_gpu > 1:\n","                tmp_eval_loss = (\n","                    tmp_eval_loss.mean()\n","                )  # mean() to average on multi-gpu parallel evaluating\n","\n","            eval_loss += tmp_eval_loss.item()\n","        nb_eval_steps += 1\n","        if preds is None:\n","            preds = logits.detach().cpu().numpy()\n","            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n","        else:\n","            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n","            out_label_ids = np.append(\n","                out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n","            )\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    preds = np.argmax(preds, axis=2)\n","\n","    label_map = {i: label for i, label in enumerate(labels)}\n","\n","    out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n","    preds_list = [[] for _ in range(out_label_ids.shape[0])]\n","\n","    for i in range(out_label_ids.shape[0]):\n","        for j in range(out_label_ids.shape[1]):\n","            if out_label_ids[i, j] != pad_token_label_id:\n","                out_label_list[i].append(label_map[out_label_ids[i][j]])\n","                preds_list[i].append(label_map[preds[i][j]])\n","\n","    results = {\n","        \"loss\": eval_loss,\n","        \"precision\": precision_score(out_label_list, preds_list),\n","        \"recall\": recall_score(out_label_list, preds_list),\n","        \"f1\": f1_score(out_label_list, preds_list),\n","    }\n","\n","    report = classification_report(out_label_list, preds_list)\n","    logger.info(\"\\n\" + report)\n","\n","    logger.info(\"***** Eval results %s *****\", prefix)\n","    for key in sorted(results.keys()):\n","        logger.info(\"  %s = %s\", key, str(results[key]))\n","\n","    return results, preds_list"],"execution_count":14,"outputs":[]},{"cell_type":"code","metadata":{"id":"AmJKaB49d-rC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600684108613,"user_tz":-330,"elapsed":1001,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["class sequence_labelling_data_input:\n","  def __init__(self,data_dir = None,model_type = None,model_name_or_path = None,output_dir = None,labels = \"\",config_name = \"\",tokenizer_name=\"\",cache_dir = \"\",max_seq_length = 512,do_train = False,do_eval = False,do_predict = False,evaluate_during_training = False,do_lower_case = False,per_gpu_train_batch_size = 8,per_gpu_eval_batch_size=8,gradient_accumulation_steps = 1,learning_rate = 5e-5,weight_decay = 0.0,adam_epsilon = 1e-8,max_grad_norm = 1.0,num_train_epochs = 3.0,max_steps = -1,warmup_steps = 0,logging_steps = 50,eval_all_checkpoints = False,no_cuda = False,overwrite_output_dir = False,overwrite_cache = False,seed = 42,fp16 = False,fp16_opt_level = \"01\",local_rank = -1,server_ip = \"\",server_port = \"\",save_steps = -1):\n","    #The input data dir. Should contain the training files for the CoNLL-2003 NER task.\n","    self.data_dir = data_dir\n","    #Model type selected in the list: \n","    self.model_type = model_type\n","    #Path to pre-trained model or shortcut name selected in the list: \n","    self.model_name_or_path = model_name_or_path \n","    #The output directory where the model predictions and checkpoints will be written.\n","    self.output_dir = output_dir\n","    #Path to a file containing all labels. If not specified, CoNLL-2003 labels are used\n","    self.labels = labels\n","    #Pretrained config name or path if not the same as model_name\n","    self.config_name = config_name\n","    #Pretrained tokenizer name or path if not the same as model_name\n","    self.tokenizer_name = tokenizer_name\n","    #Where do you want to store the pre-trained models downloaded from s3\n","    self.cache_dir = cache_dir\n","    #The maximum total input sequence length after tokenization. Sequences longer \"than this will be truncated, sequences shorter will be padded.\"\n","    self.max_seq_length = max_seq_length\n","    #Whether to run training.\n","    self.do_train = do_train\n","    #Whether to run eval on the dev set.\n","    self.do_eval = do_eval\n","    #Whether to run predictions on the test set.\n","    self.do_predict = do_predict\n","    #Whether to run evaluation during training at each logging step.\n","    self.evaluate_during_training = evaluate_during_training\n","    #Set this flag if you are using an uncased model.\n","    self.do_lower_case = do_lower_case\n","    #Batch size per GPU/CPU for training.\n","    self.per_gpu_train_batch_size = per_gpu_train_batch_size\n","    #Batch size per GPU/CPU for evaluation.\n","    self.per_gpu_eval_batch_size = per_gpu_eval_batch_size\n","    #Number of updates steps to accumulate before performing a backward/update pass.\n","    self.gradient_accumulation_steps = gradient_accumulation_steps\n","    #The initial learning rate for Adam.\n","    self.learning_rate = learning_rate\n","    #Weight decay if we apply some.\n","    self.weight_decay = weight_decay\n","    #Epsilon for Adam optimizer.\n","    self.adam_epsilon = adam_epsilon\n","    #Max gradient norm.\n","    self.max_grad_norm = max_grad_norm\n","    #Total number of training epochs to perform.\n","    self.num_train_epochs = num_train_epochs\n","    #If > 0: set total number of training steps to perform. Override num_train_epochs.\n","    self.max_steps = max_steps\n","    #Linear warmup over warmup_steps.\n","    self.warmup_steps = warmup_steps\n","    #Log every X updates steps.\n","    self.logging_steps = logging_steps\n","    #Save checkpoint every X updates steps.\n","    self.save_steps = save_steps\n","    #Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\n","    self.eval_all_checkpoints = eval_all_checkpoints\n","    #Avoid using CUDA when available\n","    self.no_cuda = False\n","    #Overwrite the content of the output directory\n","    self.overwrite_output_dir =False\n","    #Overwrite the cached training and evaluation sets\n","    self.overwrite_cache = overwrite_cache\n","    #random seed for initialization\n","    self.seed = seed\n","    #Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\n","    self.fp16 = fp16\n","    #For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\"See details at https://nvidia.github.io/apex/amp.html\"\n","    self.fp16_opt_level = fp16_opt_level\n","    #For distributed training: local_rank\n","    self.local_rank = local_rank\n","    #For distant debugging.\n","    self.server_ip = server_ip\n","    #For distant debugging.\n","    self.server_port = server_port\n","\n","#args = parser.parse_args()"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I99OAm9i2llC","colab_type":"text"},"source":["**creating the object for training**"]},{"cell_type":"code","metadata":{"id":"bCftw30m3LGD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600684409218,"user_tz":-330,"elapsed":1342,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["args=sequence_labelling_data_input(data_dir='data',model_type = \"layoutlm\",model_name_or_path = \"bert-base-uncased\",do_lower_case = True,max_seq_length = 512,do_train = True,num_train_epochs = 1,overwrite_output_dir = True,logging_steps = 1,save_steps = -1,output_dir = 'sep_out3',labels = '/content/gdrive/My Drive/unilm/layoutlm/examples/seq_labeling/data/labels.txt' ,per_gpu_train_batch_size = 1,per_gpu_eval_batch_size = 1)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"em8XAsSBwT8V","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600684421976,"user_tz":-330,"elapsed":1229,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["def main(args):  # noqa C901\n","    if (\n","        os.path.exists(args.output_dir)\n","        and os.listdir(args.output_dir)\n","        and args.do_train\n","    ):\n","        if not args.overwrite_output_dir:\n","            raise ValueError(\n","                \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n","                    args.output_dir\n","                )\n","            )\n","        else:\n","            if args.local_rank in [-1, 0]:\n","                shutil.rmtree(args.output_dir)\n","\n","    if not os.path.exists(args.output_dir) and (args.do_eval or args.do_predict):\n","        raise ValueError(\n","            \"Output directory ({}) does not exist. Please train and save the model before inference stage.\".format(\n","                args.output_dir\n","            )\n","        )\n","\n","    if (\n","        not os.path.exists(args.output_dir)\n","        and args.do_train\n","        and args.local_rank in [-1, 0]\n","    ):\n","        os.makedirs(args.output_dir)\n","\n","    # Setup distant debugging if needed\n","    if args.server_ip and args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","        print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(\n","            address=(args.server_ip, args.server_port), redirect_output=True\n","        )\n","        ptvsd.wait_for_attach()\n","\n","    # Setup CUDA, GPU & distributed training\n","    if args.local_rank == -1 or args.no_cuda:\n","        device = torch.device(\n","            \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n","        )\n","        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n","    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","        torch.cuda.set_device(args.local_rank)\n","        device = torch.device(\"cuda\", args.local_rank)\n","        torch.distributed.init_process_group(backend=\"nccl\")\n","        args.n_gpu = 1\n","    args.device = device\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        filename=os.path.join(args.output_dir, \"train.log\")\n","        if args.local_rank in [-1, 0]\n","        else None,\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.warning(\n","        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","        args.local_rank,\n","        device,\n","        args.n_gpu,\n","        bool(args.local_rank != -1),\n","        args.fp16,\n","    )\n","\n","    # Set seed\n","    set_seed(args)\n","\n","    labels = get_labels(args.labels)\n","    num_labels = len(labels)\n","    # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n","    pad_token_label_id = CrossEntropyLoss().ignore_index\n","\n","    # Load pretrained model and tokenizer\n","    if args.local_rank not in [-1, 0]:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n","\n","    args.model_type = args.model_type.lower()\n","    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","    config = config_class.from_pretrained(\n","        args.config_name if args.config_name else args.model_name_or_path,\n","        num_labels=num_labels,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    tokenizer = tokenizer_class.from_pretrained(\n","        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n","        do_lower_case=args.do_lower_case,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    model = model_class.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","\n","    if args.local_rank == 0:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n","\n","    model.to(args.device)\n","\n","    logger.info(\"Training/evaluation parameters %s\", args)\n","\n","    \n","    \n","    # Training\n","    if args.do_train:\n","        train_dataset = FunsdDataset(\n","            args, tokenizer, labels, pad_token_label_id, mode=\"train\"\n","        )\n","        global_step, tr_loss = train(\n","            args, train_dataset, model, tokenizer, labels, pad_token_label_id\n","        )\n","        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n","\n","    \n","    \n","    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n","        # Create output directory if needed\n","        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n","            os.makedirs(args.output_dir)\n","\n","        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n","        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","        # They can then be reloaded using `from_pretrained()`\n","        model_to_save = (\n","            model.module if hasattr(model, \"module\") else model\n","        )  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(args.output_dir)\n","        tokenizer.save_pretrained(args.output_dir)\n","\n","        # Good practice: save your training arguments together with the trained model\n","        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n","\n","    \n","    \n","    \n","    # Evaluation\n","    results = {}\n","    if args.do_eval and args.local_rank in [-1, 0]:\n","        tokenizer = tokenizer_class.from_pretrained(\n","            args.output_dir, do_lower_case=args.do_lower_case\n","        )\n","        checkpoints = [args.output_dir]\n","        if args.eval_all_checkpoints:\n","            checkpoints = list(\n","                os.path.dirname(c)\n","                for c in sorted(\n","                    glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True)\n","                )\n","            )\n","            logging.getLogger(\"pytorch_transformers.modeling_utils\").setLevel(\n","                logging.WARN\n","            )  # Reduce logging\n","        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","        for checkpoint in checkpoints:\n","            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n","            model = model_class.from_pretrained(checkpoint)\n","            model.to(args.device)\n","            result, _ = evaluate(\n","                args,\n","                model,\n","                tokenizer,\n","                labels,\n","                pad_token_label_id,\n","                mode=\"test\",\n","                prefix=global_step,\n","            )\n","            if global_step:\n","                result = {\"{}_{}\".format(global_step, k): v for k, v in result.items()}\n","            results.update(result)\n","        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n","        with open(output_eval_file, \"w\") as writer:\n","            for key in sorted(results.keys()):\n","                writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n","\n","    \n","    \n","    # do predict part\n","    if args.do_predict and args.local_rank in [-1, 0]:\n","        tokenizer = tokenizer_class.from_pretrained(\n","            args.model_name_or_path, do_lower_case=args.do_lower_case\n","        )\n","        model = model_class.from_pretrained(args.output_dir)\n","        model.to(args.device)\n","        #print(\"\\n evaluate function parameter1 args: - \", args)\n","        #print(\"\\n evaluate function parameter2 model : - \", model)\n","        #print(\"\\n evaluate function parameters3 tokenizer :- \", tokenizer)\n","        #print(\"\\n evaluate function parameter4 labels : - \", labels)\n","        #print(\"\\n  evaluate fnctions parameter 5 pad_token_label\", pad_token_label_id)\n","        result, predictions = evaluate(\n","            args, model, tokenizer, labels, pad_token_label_id, mode=\"test\"\n","        )\n","        # Save results\n","        output_test_results_file = os.path.join(args.output_dir, \"test_results.txt\")\n","        with open(output_test_results_file, \"w\") as writer:\n","            for key in sorted(result.keys()):\n","                writer.write(\"{} = {}\\n\".format(key, str(result[key])))\n","        # Save predictions\n","        output_test_predictions_file = os.path.join(\n","            args.output_dir, \"test_predictions.txt\"\n","        )\n","        with open(output_test_predictions_file, \"w\", encoding=\"utf8\") as writer:\n","            with open(\n","                os.path.join(args.data_dir, \"test.txt\"), \"r\", encoding=\"utf8\"\n","            ) as f:\n","                example_id = 0\n","                for line in f:\n","                    if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n","                        writer.write(line)\n","                        if not predictions[example_id]:\n","                            example_id += 1\n","                    elif predictions[example_id]:\n","                        output_line = (\n","                            line.split()[0]\n","                            + \" \"\n","                            + predictions[example_id].pop(0)\n","                            + \"\\n\"\n","                        )\n","                        writer.write(output_line)\n","                    else:\n","                        logger.warning(\n","                            \"Maximum sequence length exceeded: No prediction for '%s'.\",\n","                            line.split()[0],\n","                        )\n","\n","    return results"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKnlp0Emwdyj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":519},"executionInfo":{"status":"error","timestamp":1600684439734,"user_tz":-330,"elapsed":10460,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"cc082d8e-ae55-4ddd-de5d-a02882b2eeb8"},"source":["if __name__ == \"__main__\":\n","    main(args)"],"execution_count":26,"outputs":[{"output_type":"stream","text":["num_labels 1\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","Iteration:   0%|          | 0/13 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-26-ed4bd21d196e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-25-0b8d799b4a54>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    117\u001b[0m         )\n\u001b[1;32m    118\u001b[0m         global_step, tr_loss = train(\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token_label_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         )\n\u001b[1;32m    121\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" global_step = %s, average loss = %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-0258346bc2d5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, train_dataset, model, tokenizer, labels, pad_token_label_id)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"bert\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"layoutlm\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             )  # RoBERTa don\"t use segment_ids\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0;31m# model outputs are always tuple in pytorch-transformers (see doc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-796278433609>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, bbox, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0mactive_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m                 \u001b[0mactive_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 948\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2422\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2216\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2217\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2218\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2219\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2220\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: Target 7 is out of bounds."]}]},{"cell_type":"markdown","metadata":{"id":"_Ejrzm03fs9Y","colab_type":"text"},"source":["testing data"]},{"cell_type":"code","metadata":{"id":"_dtM--mzgLnh","colab_type":"code","colab":{}},"source":["args = sequence_labelling_data_input(data_dir = \"data\",model_type = \"layoutlm\",output_dir = \"sep_out1\",model_name_or_path = 'bert-base-uncased',do_lower_case = True,max_seq_length = 512,do_predict = True,labels = \"/content/gdrive/My Drive/unilm/layoutlm/examples/seq_labeling/labels.txt\",fp16 = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U1wF8FRoiBgR","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1598335011359,"user_tz":-330,"elapsed":71659,"user":{"displayName":"Mohit Tuli","photoUrl":"","userId":"05250823286326681737"}},"outputId":"52f6f1ca-8e40-47bc-de8b-6105a156b73e"},"source":["if __name__ == \"__main__\":\n","    main(args)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["num_labels 13\n","num_labels 13\n","\n"," evaluate function parameter1 args: -  <__main__.sequence_labelling_data_input object at 0x7f2376b03f28>\n","\n"," evaluate function parameter2 model : -  LayoutlmForTokenClassification(\n","  (bert): LayoutlmModel(\n","    (embeddings): LayoutlmEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (x_position_embeddings): Embedding(1024, 768)\n","      (y_position_embeddings): Embedding(1024, 768)\n","      (h_position_embeddings): Embedding(1024, 768)\n","      (w_position_embeddings): Embedding(1024, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=13, bias=True)\n",")\n","\n"," evaluate function parameters3 tokenizer :-  <transformers.tokenization_bert.BertTokenizer object at 0x7f23753710f0>\n","\n"," evaluate function parameter4 labels : -  ['B-ANSWER', 'B-HEADER', 'B-QUESTION', 'E-ANSWER', 'E-HEADER', 'E-QUESTION', 'I-ANSWER', 'I-HEADER', 'I-QUESTION', 'O', 'S-ANSWER', 'S-HEADER', 'S-QUESTION']\n","\n","  evaluate fnctions parameter 5 pad_token_label -100\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:   0%|          | 0/103 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n"," lenght :-  5\n","\n","\n"," keys :-  dict_keys(['input_ids', 'attention_mask', 'labels', 'bbox', 'token_type_ids'])\n","\n","\n","\n","\n","\n","\n"," The arguments in the embedding layer :- \n","\n","\n","\n","\n"," input_ids in embedding layer :-  tensor([[  101,  2012,  2102,  ...,     0,     0,     0],\n","        [  101, 11703,  2184,  ...,     0,     0,     0],\n","        [  101,  1052,  1012,  ...,     0,     0,     0],\n","        ...,\n","        [  101, 18777,  6904,  ...,     0,     0,     0],\n","        [  101,  6726,  3104,  ...,     0,     0,     0],\n","        [  101,  6904,  2595,  ...,     0,     0,     0]])\n","\n"," position_ids in embedding layer :-  None\n","\n"," token_type_ids in embedding layer :-  tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]])\n","\n"," input_ids in embedding layer :- \n","\n","\n","\n"," bbox shape :-  torch.Size([8, 512, 4])\n","\n","bbox[:,:,2] :-  tensor([[  0, 173, 173,  ...,   0,   0,   0],\n","        [  0, 583, 608,  ...,   0,   0,   0],\n","        [  0, 761, 761,  ...,   0,   0,   0],\n","        ...,\n","        [  0, 502, 604,  ...,   0,   0,   0],\n","        [  0, 485, 554,  ...,   0,   0,   0],\n","        [  0, 428, 428,  ...,   0,   0,   0]])\n","\n","bbox[:,:,2] shape :-  torch.Size([8, 512])\n","\n","bbox[:, :, 0] :-  tensor([[  0, 136, 136,  ...,   0,   0,   0],\n","        [  0, 557, 588,  ...,   0,   0,   0],\n","        [  0, 745, 745,  ...,   0,   0,   0],\n","        ...,\n","        [  0, 366, 506,  ...,   0,   0,   0],\n","        [  0, 352, 493,  ...,   0,   0,   0],\n","        [  0, 391, 391,  ...,   0,   0,   0]])\n","\n","bbox[:, :, 0] shape :-  torch.Size([8, 512])\n","\n"," sub of two above matrix :-  tensor([[  0,  37,  37,  ...,   0,   0,   0],\n","        [  0,  26,  20,  ...,   0,   0,   0],\n","        [  0,  16,  16,  ...,   0,   0,   0],\n","        ...,\n","        [  0, 136,  98,  ...,   0,   0,   0],\n","        [  0, 133,  61,  ...,   0,   0,   0],\n","        [  0,  37,  37,  ...,   0,   0,   0]])\n","\n"," printing the w_position_embeddings:-  Embedding(1024, 768)\n","\n"," shape of the resultant sub array :-  torch.Size([8, 512])\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:   1%|          | 1/103 [00:14<24:13, 14.25s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n"," lenght :-  5\n","\n","\n"," keys :-  dict_keys(['input_ids', 'attention_mask', 'labels', 'bbox', 'token_type_ids'])\n","\n","\n","\n","\n","\n","\n"," The arguments in the embedding layer :- \n","\n","\n","\n","\n"," input_ids in embedding layer :-  tensor([[ 101, 2000, 1024,  ...,    0,    0,    0],\n","        [ 101, 2577, 3347,  ...,    0,    0,    0],\n","        [ 101, 6904, 2595,  ...,    0,    0,    0],\n","        ...,\n","        [ 101, 3058, 1024,  ...,    0,    0,    0],\n","        [ 101, 2260, 1013,  ...,    0,    0,    0],\n","        [ 101, 2193, 1997,  ...,    0,    0,    0]])\n","\n"," position_ids in embedding layer :-  None\n","\n"," token_type_ids in embedding layer :-  tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]])\n","\n"," input_ids in embedding layer :- \n","\n","\n","\n"," bbox shape :-  torch.Size([8, 512, 4])\n","\n","bbox[:,:,2] :-  tensor([[  0, 176, 176,  ...,   0,   0,   0],\n","        [  0, 245, 311,  ...,   0,   0,   0],\n","        [  0, 173, 173,  ...,   0,   0,   0],\n","        ...,\n","        [  0, 196, 196,  ...,   0,   0,   0],\n","        [  0, 311, 311,  ...,   0,   0,   0],\n","        [  0, 216, 248,  ...,   0,   0,   0]])\n","\n","bbox[:,:,2] shape :-  torch.Size([8, 512])\n","\n","bbox[:, :, 0] :-  tensor([[  0, 139, 139,  ...,   0,   0,   0],\n","        [  0, 193, 249,  ...,   0,   0,   0],\n","        [  0, 137, 137,  ...,   0,   0,   0],\n","        ...,\n","        [  0, 137, 137,  ...,   0,   0,   0],\n","        [  0, 245, 245,  ...,   0,   0,   0],\n","        [  0, 137, 224,  ...,   0,   0,   0]])\n","\n","bbox[:, :, 0] shape :-  torch.Size([8, 512])\n","\n"," sub of two above matrix :-  tensor([[ 0, 37, 37,  ...,  0,  0,  0],\n","        [ 0, 52, 62,  ...,  0,  0,  0],\n","        [ 0, 36, 36,  ...,  0,  0,  0],\n","        ...,\n","        [ 0, 59, 59,  ...,  0,  0,  0],\n","        [ 0, 66, 66,  ...,  0,  0,  0],\n","        [ 0, 79, 24,  ...,  0,  0,  0]])\n","\n"," printing the w_position_embeddings:-  Embedding(1024, 768)\n","\n"," shape of the resultant sub array :-  torch.Size([8, 512])\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:   2%|▏         | 2/103 [00:28<24:05, 14.31s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n"," lenght :-  5\n","\n","\n"," keys :-  dict_keys(['input_ids', 'attention_mask', 'labels', 'bbox', 'token_type_ids'])\n","\n","\n","\n","\n","\n","\n"," The arguments in the embedding layer :- \n","\n","\n","\n","\n"," input_ids in embedding layer :-  tensor([[ 101, 1017,  102,  ...,    0,    0,    0],\n","        [ 101, 4604, 2121,  ...,    0,    0,    0],\n","        [ 101, 2569, 8128,  ...,    0,    0,    0],\n","        ...,\n","        [ 101, 2004, 2574,  ...,    0,    0,    0],\n","        [ 101, 3602, 1024,  ...,    0,    0,    0],\n","        [ 101, 2023, 4471,  ...,    0,    0,    0]])\n","\n"," position_ids in embedding layer :-  None\n","\n"," token_type_ids in embedding layer :-  tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]])\n","\n"," input_ids in embedding layer :- \n","\n","\n","\n"," bbox shape :-  torch.Size([8, 512, 4])\n","\n","bbox[:,:,2] :-  tensor([[   0,  624, 1000,  ...,    0,    0,    0],\n","        [   0,  278,  278,  ...,    0,    0,    0],\n","        [   0,  214,  364,  ...,    0,    0,    0],\n","        ...,\n","        [   0,  409,  461,  ...,    0,    0,    0],\n","        [   0,  185,  185,  ...,    0,    0,    0],\n","        [   0,  251,  326,  ...,    0,    0,    0]])\n","\n","bbox[:,:,2] shape :-  torch.Size([8, 512])\n","\n","bbox[:, :, 0] :-  tensor([[   0,  615, 1000,  ...,    0,    0,    0],\n","        [   0,  136,  136,  ...,    0,    0,    0],\n","        [   0,  136,  220,  ...,    0,    0,    0],\n","        ...,\n","        [   0,  387,  413,  ...,    0,    0,    0],\n","        [   0,  136,  136,  ...,    0,    0,    0],\n","        [   0,  218,  255,  ...,    0,    0,    0]])\n","\n","bbox[:, :, 0] shape :-  torch.Size([8, 512])\n","\n"," sub of two above matrix :-  tensor([[  0,   9,   0,  ...,   0,   0,   0],\n","        [  0, 142, 142,  ...,   0,   0,   0],\n","        [  0,  78, 144,  ...,   0,   0,   0],\n","        ...,\n","        [  0,  22,  48,  ...,   0,   0,   0],\n","        [  0,  49,  49,  ...,   0,   0,   0],\n","        [  0,  33,  71,  ...,   0,   0,   0]])\n","\n"," printing the w_position_embeddings:-  Embedding(1024, 768)\n","\n"," shape of the resultant sub array :-  torch.Size([8, 512])\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:   3%|▎         | 3/103 [00:43<23:55, 14.35s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n"," lenght :-  5\n","\n","\n"," keys :-  dict_keys(['input_ids', 'attention_mask', 'labels', 'bbox', 'token_type_ids'])\n","\n","\n","\n","\n","\n","\n"," The arguments in the embedding layer :- \n","\n","\n","\n","\n"," input_ids in embedding layer :-  tensor([[  101,  3183,  2009,  ...,     0,     0,     0],\n","        [  101, 18777,  1010,  ...,     0,     0,     0],\n","        [  101,  8068,  1997,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  2363,  2023,  ...,     0,     0,     0],\n","        [  101,  2434,  4471,  ...,     0,     0,     0],\n","        [  101,  6792,  1012,  ...,     0,     0,     0]])\n","\n"," position_ids in embedding layer :-  None\n","\n"," token_type_ids in embedding layer :-  tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]])\n","\n"," input_ids in embedding layer :- \n","\n","\n","\n"," bbox shape :-  torch.Size([8, 512, 4])\n","\n","bbox[:,:,2] :-  tensor([[  0, 263, 290,  ...,   0,   0,   0],\n","        [  0, 334, 334,  ...,   0,   0,   0],\n","        [  0, 255, 271,  ...,   0,   0,   0],\n","        ...,\n","        [  0, 270, 299,  ...,   0,   0,   0],\n","        [  0, 267, 324,  ...,   0,   0,   0],\n","        [  0, 293, 293,  ...,   0,   0,   0]])\n","\n","bbox[:,:,2] shape :-  torch.Size([8, 512])\n","\n","bbox[:, :, 0] :-  tensor([[  0, 218, 273,  ...,   0,   0,   0],\n","        [  0, 218, 218,  ...,   0,   0,   0],\n","        [  0, 217, 258,  ...,   0,   0,   0],\n","        ...,\n","        [  0, 217, 274,  ...,   0,   0,   0],\n","        [  0, 217, 270,  ...,   0,   0,   0],\n","        [  0, 217, 217,  ...,   0,   0,   0]])\n","\n","bbox[:, :, 0] shape :-  torch.Size([8, 512])\n","\n"," sub of two above matrix :-  tensor([[  0,  45,  17,  ...,   0,   0,   0],\n","        [  0, 116, 116,  ...,   0,   0,   0],\n","        [  0,  38,  13,  ...,   0,   0,   0],\n","        ...,\n","        [  0,  53,  25,  ...,   0,   0,   0],\n","        [  0,  50,  54,  ...,   0,   0,   0],\n","        [  0,  76,  76,  ...,   0,   0,   0]])\n","\n"," printing the w_position_embeddings:-  Embedding(1024, 768)\n","\n"," shape of the resultant sub array :-  torch.Size([8, 512])\n"],"name":"stdout"},{"output_type":"stream","text":["\rEvaluating:   4%|▍         | 4/103 [00:57<23:37, 14.32s/it]"],"name":"stderr"},{"output_type":"stream","text":["\n","\n"," lenght :-  5\n","\n","\n"," keys :-  dict_keys(['input_ids', 'attention_mask', 'labels', 'bbox', 'token_type_ids'])\n","\n","\n","\n","\n","\n","\n"," The arguments in the embedding layer :- \n","\n","\n","\n","\n"," input_ids in embedding layer :-  tensor([[  101, 26667,  2683,  ...,     0,     0,     0],\n","        [  101,  2110,  2436,  ...,     0,     0,     0],\n","        [  101,  7479,  1012,  ...,     0,     0,     0],\n","        ...,\n","        [  101,  8356,  3319,  ...,     0,     0,     0],\n","        [  101,  9098,  2671,  ...,     0,     0,     0],\n","        [  101,  8819,  2053,  ...,     0,     0,     0]])\n","\n"," position_ids in embedding layer :-  None\n","\n"," token_type_ids in embedding layer :-  tensor([[0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        ...,\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0],\n","        [0, 0, 0,  ..., 0, 0, 0]])\n","\n"," input_ids in embedding layer :- \n","\n","\n","\n"," bbox shape :-  torch.Size([8, 512, 4])\n","\n","bbox[:,:,2] :-  tensor([[  0, 838, 838,  ...,   0,   0,   0],\n","        [  0, 275, 319,  ...,   0,   0,   0],\n","        [  0, 541, 541,  ...,   0,   0,   0],\n","        ...,\n","        [  0, 218, 278,  ...,   0,   0,   0],\n","        [  0, 518, 597,  ...,   0,   0,   0],\n","        [  0, 220, 258,  ...,   0,   0,   0]])\n","\n","bbox[:,:,2] shape :-  torch.Size([8, 512])\n","\n","bbox[:, :, 0] :-  tensor([[  0, 866, 866,  ...,   0,   0,   0],\n","        [  0, 242, 279,  ...,   0,   0,   0],\n","        [  0, 413, 413,  ...,   0,   0,   0],\n","        ...,\n","        [  0, 125, 220,  ...,   0,   0,   0],\n","        [  0, 439, 527,  ...,   0,   0,   0],\n","        [  0, 122, 224,  ...,   0,   0,   0]])\n","\n","bbox[:, :, 0] shape :-  torch.Size([8, 512])\n","\n"," sub of two above matrix :-  tensor([[  0, -28, -28,  ...,   0,   0,   0],\n","        [  0,  33,  40,  ...,   0,   0,   0],\n","        [  0, 128, 128,  ...,   0,   0,   0],\n","        ...,\n","        [  0,  93,  58,  ...,   0,   0,   0],\n","        [  0,  79,  70,  ...,   0,   0,   0],\n","        [  0,  98,  34,  ...,   0,   0,   0]])\n","\n"," printing the w_position_embeddings:-  Embedding(1024, 768)\n","\n"," shape of the resultant sub array :-  torch.Size([8, 512])\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-ed4bd21d196e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-16-46a7e8fbd553>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n  evaluate fnctions parameter 5 pad_token_label\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token_label_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         result, predictions = evaluate(\n\u001b[0;32m--> 200\u001b[0;31m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token_label_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"test\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m         )\n\u001b[1;32m    202\u001b[0m         \u001b[0;31m# Save results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-2a557158c6a6>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(args, model, tokenizer, labels, pad_token_label_id, mode, prefix)\u001b[0m\n\u001b[1;32m    332\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n keys :- \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\\n\\n\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m             \u001b[0mtmp_eval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-7ee1ec5ef7b5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, bbox, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m         )\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-7ee1ec5ef7b5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, bbox, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask)\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n input_ids in embedding layer :- \\n\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         embedding_output = self.embeddings(\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m         )\n\u001b[1;32m    187\u001b[0m         encoder_outputs = self.encoder(\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-12-7ee1ec5ef7b5>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, bbox, token_type_ids, position_ids, inputs_embeds)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n shape of the resultant sub array :- \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbboxshape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         w_position_embeddings = self.w_position_embeddings(\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbbox\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         )\n\u001b[1;32m     93\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m         return F.embedding(\n\u001b[1;32m    125\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: index out of range in self"]}]},{"cell_type":"code","metadata":{"id":"GStjy6MkO5Cr","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5A4UvnATVdT1","colab_type":"text"},"source":["**Post Processing**"]},{"cell_type":"code","metadata":{"id":"0zJC8UfkVz_g","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":54},"executionInfo":{"status":"ok","timestamp":1597834224531,"user_tz":-330,"elapsed":3059,"user":{"displayName":"Mohit Tuli","photoUrl":"","userId":"05250823286326681737"}},"outputId":"ef94c39a-170b-409e-da46-c9fb72b87150"},"source":["with open(\"custom_data/test.txt\",\"r\") as fp:\n","  text=fp.readlines()\n","  print(text)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['Manuscript\\tO\\n', 'Review\\tO\\n', 'Form\\tO\\n', '\\n', 'TOBACCO\\tB-HEADER\\n', 'SCIENCE\\tE-HEADER\\n', '\\n', 'Registration\\tB-QUESTION\\n', 'No.\\tE-QUESTION\\n', '\\n', '533\\tS-ANSWER\\n', '\\n', 'Date\\tS-QUESTION\\n', '\\n', 'March\\tB-ANSWER\\n', '18,\\tI-ANSWER\\n', '1968\\tE-ANSWER\\n', '\\n', 'AUTHORS\\tS-QUESTION\\n', '\\n', 'Andrew\\tB-ANSWER\\n', 'G.\\tI-ANSWER\\n', 'Kallianos,\\tI-ANSWER\\n', 'Richard\\tI-ANSWER\\n', 'E.\\tI-ANSWER\\n', 'Means,\\tI-ANSWER\\n', 'James\\tI-ANSWER\\n', 'D.\\tI-ANSWER\\n', 'Mold\\tE-ANSWER\\n', '\\n', 'TITLE\\tS-QUESTION\\n', '\\n', '\"Effect\\tB-ANSWER\\n', 'of\\tI-ANSWER\\n', 'Nitrates\\tI-ANSWER\\n', 'in\\tI-ANSWER\\n', 'Tobacco\\tI-ANSWER\\n', 'on\\tI-ANSWER\\n', 'the\\tI-ANSWER\\n', 'Catechol\\tI-ANSWER\\n', 'Yield\\tI-ANSWER\\n', 'in\\tE-ANSWER\\n', '\\n', 'Cigarette\\tB-ANSWER\\n', 'Smoke\"\\tE-ANSWER\\n', '\\n', 'REVIEW\\tO\\n', 'COMPLETED.\\tO\\n', '\\n', '3/29/68\\tO\\n', '\\n', 'RECOMMENDATION:\\tO\\n', 'X\\tO\\n', 'APPROVE\\tO\\n', 'IN\\tO\\n', 'ITS\\tO\\n', '\\n', 'PRESENT\\tO\\n', 'FORM;\\tO\\n', '\\n', 'NOT\\tO\\n', 'APPROVE\\tO\\n', '(Give\\tO\\n', 'reasons\\tO\\n', 'below)\\tO\\n', ';\\tO\\n', '\\n', 'APPROVE\\tO\\n', 'TENTATIVELY,\\tO\\n', '\\n', 'SUBJECT\\tO\\n', 'TO\\tO\\n', 'THE\\tO\\n', 'FOLLOWING\\tO\\n', 'SUGGESTED\\tO\\n', \"REVISIONS':\\tO\\n\", '(itemize\\tO\\n', 'below)\\tO\\n', ':\\tO\\n', '\\n', 'Paye\\tO\\n', '4\\tO\\n', '-\\tO\\n', 'Last\\tO\\n', 'line\\tO\\n', 'should\\tO\\n', 'be\\tO\\n', 'Mass\\tO\\n', 'spectrometric,\\tO\\n', 'instead\\tO\\n', 'of\\tO\\n', '\\n', 'Mass\\tO\\n', 'spectroscopic.\\tO\\n', '\\n', '00070353\\tO\\n', '\\n', 'NOTE-Execute\\tO\\n', 'in\\tO\\n', 'triplicate\\tO\\n', 'using\\tO\\n', 'additional\\tO\\n', 'sheets\\tO\\n', 'if\\tO\\n', 'more\\tO\\n', 'space\\tO\\n', 'is\\tO\\n', 'required.\\tO\\n', 'Retain\\tO\\n', 'the\\tO\\n', 'third\\tO\\n', '\\n', 'copy\\tO\\n', 'for\\tO\\n', 'your\\tO\\n', 'file.\\tO\\n', 'Return\\tO\\n', 'the\\tO\\n', 'original\\tO\\n', '(signed)\\tO\\n', 'and\\tO\\n', 'the\\tO\\n', 'first\\tO\\n', 'carbon\\tO\\n', '(unsigned)\\tO\\n', 'along\\tO\\n', 'with\\tO\\n', 'the\\tO\\n', '\\n', 'manuscript\\tO\\n', 'to\\tO\\n', 'this\\tO\\n', 'office.\\tO\\n', 'The\\tO\\n', 'unsigned\\tO\\n', 'copy\\tO\\n', 'and\\tO\\n', 'the\\tO\\n', 'manuscript\\tO\\n', 'will\\tO\\n', 'be\\tO\\n', 'returned\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', 'author\\tO\\n', '\\n', 'for\\tO\\n', 'his\\tO\\n', 'consideration.\\tO\\n', '\\n', 'Manuscript\\tO\\n', 'Review\\tO\\n', 'Form\\tO\\n', '\\n', 'TOBACCO\\tB-HEADER\\n', 'SCIENCE\\tE-HEADER\\n', '\\n', 'Registration\\tB-QUESTION\\n', 'No.\\tE-QUESTION\\n', '\\n', '533\\tS-ANSWER\\n', '\\n', 'Date\\tS-QUESTION\\n', '\\n', 'March\\tB-ANSWER\\n', '18,\\tI-ANSWER\\n', '1968\\tE-ANSWER\\n', '\\n', 'AUTHORS\\tS-QUESTION\\n', '\\n', 'Andrew\\tB-ANSWER\\n', 'G.\\tI-ANSWER\\n', 'Kallianos,\\tI-ANSWER\\n', 'Richard\\tI-ANSWER\\n', 'E.\\tI-ANSWER\\n', 'Means,\\tI-ANSWER\\n', 'James\\tI-ANSWER\\n', 'D.\\tI-ANSWER\\n', 'Mold\\tE-ANSWER\\n', '\\n', 'TITLE\\tS-QUESTION\\n', '\\n', '\"Effect\\tB-ANSWER\\n', 'of\\tI-ANSWER\\n', 'Nitrates\\tI-ANSWER\\n', 'in\\tI-ANSWER\\n', 'Tobacco\\tI-ANSWER\\n', 'on\\tI-ANSWER\\n', 'the\\tI-ANSWER\\n', 'Catechol\\tI-ANSWER\\n', 'Yield\\tI-ANSWER\\n', 'in\\tE-ANSWER\\n', '\\n', 'Cigarette\\tB-ANSWER\\n', 'Smoke\"\\tE-ANSWER\\n', '\\n', 'REVIEW\\tO\\n', 'COMPLETED.\\tO\\n', '\\n', '3/29/68\\tO\\n', '\\n', 'RECOMMENDATION:\\tO\\n', 'X\\tO\\n', 'APPROVE\\tO\\n', 'IN\\tO\\n', 'ITS\\tO\\n', '\\n', 'PRESENT\\tO\\n', 'FORM;\\tO\\n', '\\n', 'NOT\\tO\\n', 'APPROVE\\tO\\n', '(Give\\tO\\n', 'reasons\\tO\\n', 'below)\\tO\\n', ';\\tO\\n', '\\n', 'APPROVE\\tO\\n', 'TENTATIVELY,\\tO\\n', '\\n', 'SUBJECT\\tO\\n', 'TO\\tO\\n', 'THE\\tO\\n', 'FOLLOWING\\tO\\n', 'SUGGESTED\\tO\\n', \"REVISIONS':\\tO\\n\", '(itemize\\tO\\n', 'below)\\tO\\n', ':\\tO\\n', '\\n', 'Paye\\tO\\n', '4\\tO\\n', '-\\tO\\n', 'Last\\tO\\n', 'line\\tO\\n', 'should\\tO\\n', 'be\\tO\\n', 'Mass\\tO\\n', 'spectrometric,\\tO\\n', 'instead\\tO\\n', 'of\\tO\\n', '\\n', 'Mass\\tO\\n', 'spectroscopic.\\tO\\n', '\\n', '00070353\\tO\\n', '\\n', 'NOTE-Execute\\tO\\n', 'in\\tO\\n', 'triplicate\\tO\\n', 'using\\tO\\n', 'additional\\tO\\n', 'sheets\\tO\\n', 'if\\tO\\n', 'more\\tO\\n', 'space\\tO\\n', 'is\\tO\\n', 'required.\\tO\\n', 'Retain\\tO\\n', 'the\\tO\\n', 'third\\tO\\n', '\\n', 'copy\\tO\\n', 'for\\tO\\n', 'your\\tO\\n', 'file.\\tO\\n', 'Return\\tO\\n', 'the\\tO\\n', 'original\\tO\\n', '(signed)\\tO\\n', 'and\\tO\\n', 'the\\tO\\n', 'first\\tO\\n', 'carbon\\tO\\n', '(unsigned)\\tO\\n', 'along\\tO\\n', 'with\\tO\\n', 'the\\tO\\n', '\\n', 'manuscript\\tO\\n', 'to\\tO\\n', 'this\\tO\\n', 'office.\\tO\\n', 'The\\tO\\n', 'unsigned\\tO\\n', 'copy\\tO\\n', 'and\\tO\\n', 'the\\tO\\n', 'manuscript\\tO\\n', 'will\\tO\\n', 'be\\tO\\n', 'returned\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', 'author\\tO\\n', '\\n', 'for\\tO\\n', 'his\\tO\\n', 'consideration.\\tO\\n', '\\n', 'Manuscript\\tO\\n', 'Review\\tO\\n', 'Form\\tO\\n', '\\n', 'TOBACCO\\tB-HEADER\\n', 'SCIENCE\\tE-HEADER\\n', '\\n', 'Registration\\tB-QUESTION\\n', 'No.\\tE-QUESTION\\n', '\\n', '533\\tS-ANSWER\\n', '\\n', 'Date\\tS-QUESTION\\n', '\\n', 'March\\tB-ANSWER\\n', '18,\\tI-ANSWER\\n', '1968\\tE-ANSWER\\n', '\\n', 'AUTHORS\\tS-QUESTION\\n', '\\n', 'Andrew\\tB-ANSWER\\n', 'G.\\tI-ANSWER\\n', 'Kallianos,\\tI-ANSWER\\n', 'Richard\\tI-ANSWER\\n', 'E.\\tI-ANSWER\\n', 'Means,\\tI-ANSWER\\n', 'James\\tI-ANSWER\\n', 'D.\\tI-ANSWER\\n', 'Mold\\tE-ANSWER\\n', '\\n', 'TITLE\\tS-QUESTION\\n', '\\n', '\"Effect\\tB-ANSWER\\n', 'of\\tI-ANSWER\\n', 'Nitrates\\tI-ANSWER\\n', 'in\\tI-ANSWER\\n', 'Tobacco\\tI-ANSWER\\n', 'on\\tI-ANSWER\\n', 'the\\tI-ANSWER\\n', 'Catechol\\tI-ANSWER\\n', 'Yield\\tI-ANSWER\\n', 'in\\tE-ANSWER\\n', '\\n', 'Cigarette\\tB-ANSWER\\n', 'Smoke\"\\tE-ANSWER\\n', '\\n', 'REVIEW\\tO\\n', 'COMPLETED.\\tO\\n', '\\n', '3/29/68\\tO\\n', '\\n', 'RECOMMENDATION:\\tO\\n', 'X\\tO\\n', 'APPROVE\\tO\\n', 'IN\\tO\\n', 'ITS\\tO\\n', '\\n', 'PRESENT\\tO\\n', 'FORM;\\tO\\n', '\\n', 'NOT\\tO\\n', 'APPROVE\\tO\\n', '(Give\\tO\\n', 'reasons\\tO\\n', 'below)\\tO\\n', ';\\tO\\n', '\\n', 'APPROVE\\tO\\n', 'TENTATIVELY,\\tO\\n', '\\n', 'SUBJECT\\tO\\n', 'TO\\tO\\n', 'THE\\tO\\n', 'FOLLOWING\\tO\\n', 'SUGGESTED\\tO\\n', \"REVISIONS':\\tO\\n\", '(itemize\\tO\\n', 'below)\\tO\\n', ':\\tO\\n', '\\n', 'Paye\\tO\\n', '4\\tO\\n', '-\\tO\\n', 'Last\\tO\\n', 'line\\tO\\n', 'should\\tO\\n', 'be\\tO\\n', 'Mass\\tO\\n', 'spectrometric,\\tO\\n', 'instead\\tO\\n', 'of\\tO\\n', '\\n', 'Mass\\tO\\n', 'spectroscopic.\\tO\\n', '\\n', '00070353\\tO\\n', '\\n', 'NOTE-Execute\\tO\\n', 'in\\tO\\n', 'triplicate\\tO\\n', 'using\\tO\\n', 'additional\\tO\\n', 'sheets\\tO\\n', 'if\\tO\\n', 'more\\tO\\n', 'space\\tO\\n', 'is\\tO\\n', 'required.\\tO\\n', 'Retain\\tO\\n', 'the\\tO\\n', 'third\\tO\\n', '\\n', 'copy\\tO\\n', 'for\\tO\\n', 'your\\tO\\n', 'file.\\tO\\n', 'Return\\tO\\n', 'the\\tO\\n', 'original\\tO\\n', '(signed)\\tO\\n', 'and\\tO\\n', 'the\\tO\\n', 'first\\tO\\n', 'carbon\\tO\\n', '(unsigned)\\tO\\n', 'along\\tO\\n', 'with\\tO\\n', 'the\\tO\\n', '\\n', 'manuscript\\tO\\n', 'to\\tO\\n', 'this\\tO\\n', 'office.\\tO\\n', 'The\\tO\\n', 'unsigned\\tO\\n', 'copy\\tO\\n', 'and\\tO\\n', 'the\\tO\\n', 'manuscript\\tO\\n', 'will\\tO\\n', 'be\\tO\\n', 'returned\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', 'author\\tO\\n', '\\n', 'for\\tO\\n', 'his\\tO\\n', 'consideration.\\tO\\n', '\\n', 'Manuscript\\tO\\n', 'Review\\tO\\n', 'Form\\tO\\n', '\\n', 'TOBACCO\\tB-HEADER\\n', 'SCIENCE\\tE-HEADER\\n', '\\n', 'Registration\\tB-QUESTION\\n', 'No.\\tE-QUESTION\\n', '\\n', '533\\tS-ANSWER\\n', '\\n', 'Date\\tS-QUESTION\\n', '\\n', 'March\\tB-ANSWER\\n', '18,\\tI-ANSWER\\n', '1968\\tE-ANSWER\\n', '\\n', 'AUTHORS\\tS-QUESTION\\n', '\\n', 'Andrew\\tB-ANSWER\\n', 'G.\\tI-ANSWER\\n', 'Kallianos,\\tI-ANSWER\\n', 'Richard\\tI-ANSWER\\n', 'E.\\tI-ANSWER\\n', 'Means,\\tI-ANSWER\\n', 'James\\tI-ANSWER\\n', 'D.\\tI-ANSWER\\n', 'Mold\\tE-ANSWER\\n', '\\n', 'TITLE\\tS-QUESTION\\n', '\\n', '\"Effect\\tB-ANSWER\\n', 'of\\tI-ANSWER\\n', 'Nitrates\\tI-ANSWER\\n', 'in\\tI-ANSWER\\n', 'Tobacco\\tI-ANSWER\\n', 'on\\tI-ANSWER\\n', 'the\\tI-ANSWER\\n', 'Catechol\\tI-ANSWER\\n', 'Yield\\tI-ANSWER\\n', 'in\\tE-ANSWER\\n', '\\n', 'Cigarette\\tB-ANSWER\\n', 'Smoke\"\\tE-ANSWER\\n', '\\n', 'REVIEW\\tO\\n', 'COMPLETED.\\tO\\n', '\\n', '3/29/68\\tO\\n', '\\n', 'RECOMMENDATION:\\tO\\n', 'X\\tO\\n', 'APPROVE\\tO\\n', 'IN\\tO\\n', 'ITS\\tO\\n', '\\n', 'PRESENT\\tO\\n', 'FORM;\\tO\\n', '\\n', 'NOT\\tO\\n', 'APPROVE\\tO\\n', '(Give\\tO\\n', 'reasons\\tO\\n', 'below)\\tO\\n', ';\\tO\\n', '\\n', 'APPROVE\\tO\\n', 'TENTATIVELY,\\tO\\n', '\\n', 'SUBJECT\\tO\\n', 'TO\\tO\\n', 'THE\\tO\\n', 'FOLLOWING\\tO\\n', 'SUGGESTED\\tO\\n', \"REVISIONS':\\tO\\n\", '(itemize\\tO\\n', 'below)\\tO\\n', ':\\tO\\n', '\\n', 'Paye\\tO\\n', '4\\tO\\n', '-\\tO\\n', 'Last\\tO\\n', 'line\\tO\\n', 'should\\tO\\n', 'be\\tO\\n', 'Mass\\tO\\n', 'spectrometric,\\tO\\n', 'instead\\tO\\n', 'of\\tO\\n', '\\n', 'Mass\\tO\\n', 'spectroscopic.\\tO\\n', '\\n', '00070353\\tO\\n', '\\n', 'NOTE-Execute\\tO\\n', 'in\\tO\\n', 'triplicate\\tO\\n', 'using\\tO\\n', 'additional\\tO\\n', 'sheets\\tO\\n', 'if\\tO\\n', 'more\\tO\\n', 'space\\tO\\n', 'is\\tO\\n', 'required.\\tO\\n', 'Retain\\tO\\n', 'the\\tO\\n', 'third\\tO\\n', '\\n', 'copy\\tO\\n', 'for\\tO\\n', 'your\\tO\\n', 'file.\\tO\\n', 'Return\\tO\\n', 'the\\tO\\n', 'original\\tO\\n', '(signed)\\tO\\n', 'and\\tO\\n', 'the\\tO\\n', 'first\\tO\\n', 'carbon\\tO\\n', '(unsigned)\\tO\\n', 'along\\tO\\n', 'with\\tO\\n', 'the\\tO\\n', '\\n', 'manuscript\\tO\\n', 'to\\tO\\n', 'this\\tO\\n', 'office.\\tO\\n', 'The\\tO\\n', 'unsigned\\tO\\n', 'copy\\tO\\n', 'and\\tO\\n', 'the\\tO\\n', 'manuscript\\tO\\n', 'will\\tO\\n', 'be\\tO\\n', 'returned\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', 'author\\tO\\n', '\\n', 'for\\tO\\n', 'his\\tO\\n', 'consideration.\\tO\\n', '\\n', 'Manuscript\\tO\\n', 'Review\\tO\\n', 'Form\\tO\\n', '\\n', 'TOBACCO\\tB-HEADER\\n', 'SCIENCE\\tE-HEADER\\n', '\\n', 'Registration\\tB-QUESTION\\n', 'No.\\tE-QUESTION\\n', '\\n', '533\\tS-ANSWER\\n', '\\n', 'Date\\tS-QUESTION\\n', '\\n', 'March\\tB-ANSWER\\n', '18,\\tI-ANSWER\\n', '1968\\tE-ANSWER\\n', '\\n', 'AUTHORS\\tS-QUESTION\\n', '\\n', 'Andrew\\tB-ANSWER\\n', 'G.\\tI-ANSWER\\n', 'Kallianos,\\tI-ANSWER\\n', 'Richard\\tI-ANSWER\\n', 'E.\\tI-ANSWER\\n', 'Means,\\tI-ANSWER\\n', 'James\\tI-ANSWER\\n', 'D.\\tI-ANSWER\\n', 'Mold\\tE-ANSWER\\n', '\\n', 'TITLE\\tS-QUESTION\\n', '\\n', '\"Effect\\tB-ANSWER\\n', 'of\\tI-ANSWER\\n', 'Nitrates\\tI-ANSWER\\n', 'in\\tI-ANSWER\\n', 'Tobacco\\tI-ANSWER\\n', 'on\\tI-ANSWER\\n', 'the\\tI-ANSWER\\n', 'Catechol\\tI-ANSWER\\n', 'Yield\\tI-ANSWER\\n', 'in\\tE-ANSWER\\n', '\\n', 'Cigarette\\tB-ANSWER\\n', 'Smoke\"\\tE-ANSWER\\n', '\\n', 'REVIEW\\tO\\n', 'COMPLETED.\\tO\\n', '\\n', '3/29/68\\tO\\n', '\\n', 'RECOMMENDATION:\\tO\\n', 'X\\tO\\n', 'APPROVE\\tO\\n', 'IN\\tO\\n', 'ITS\\tO\\n', '\\n', 'PRESENT\\tO\\n', 'FORM;\\tO\\n', '\\n', 'NOT\\tO\\n', 'APPROVE\\tO\\n', '(Give\\tO\\n', 'reasons\\tO\\n', 'below)\\tO\\n', ';\\tO\\n', '\\n', 'APPROVE\\tO\\n', 'TENTATIVELY,\\tO\\n', '\\n', 'SUBJECT\\tO\\n', 'TO\\tO\\n', 'THE\\tO\\n', 'FOLLOWING\\tO\\n', 'SUGGESTED\\tO\\n', \"REVISIONS':\\tO\\n\", '(itemize\\tO\\n', 'below)\\tO\\n', ':\\tO\\n', '\\n', 'Paye\\tO\\n', '4\\tO\\n', '-\\tO\\n', 'Last\\tO\\n', 'line\\tO\\n', 'should\\tO\\n', 'be\\tO\\n', 'Mass\\tO\\n', 'spectrometric,\\tO\\n', 'instead\\tO\\n', 'of\\tO\\n', '\\n', 'Mass\\tO\\n', 'spectroscopic.\\tO\\n', '\\n', '00070353\\tO\\n', '\\n', 'NOTE-Execute\\tO\\n', 'in\\tO\\n', 'triplicate\\tO\\n', 'using\\tO\\n', 'additional\\tO\\n', 'sheets\\tO\\n', 'if\\tO\\n', 'more\\tO\\n', 'space\\tO\\n', 'is\\tO\\n', 'required.\\tO\\n', 'Retain\\tO\\n', 'the\\tO\\n', 'third\\tO\\n', '\\n', 'copy\\tO\\n', 'for\\tO\\n', 'your\\tO\\n', 'file.\\tO\\n', 'Return\\tO\\n', 'the\\tO\\n', 'original\\tO\\n', '(signed)\\tO\\n', 'and\\tO\\n', 'the\\tO\\n', 'first\\tO\\n', 'carbon\\tO\\n', '(unsigned)\\tO\\n', 'along\\tO\\n', 'with\\tO\\n', 'the\\tO\\n', '\\n', 'manuscript\\tO\\n', 'to\\tO\\n', 'this\\tO\\n', 'office.\\tO\\n', 'The\\tO\\n', 'unsigned\\tO\\n', 'copy\\tO\\n', 'and\\tO\\n', 'the\\tO\\n', 'manuscript\\tO\\n', 'will\\tO\\n', 'be\\tO\\n', 'returned\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', 'author\\tO\\n', '\\n', 'for\\tO\\n', 'his\\tO\\n', 'consideration.\\tO\\n', '\\n', 'Manuscript\\tO\\n', 'Review\\tO\\n', 'Form\\tO\\n', '\\n', 'TOBACCO\\tB-HEADER\\n', 'SCIENCE\\tE-HEADER\\n', '\\n', 'Registration\\tB-QUESTION\\n', 'No.\\tE-QUESTION\\n', '\\n', '533\\tS-ANSWER\\n', '\\n', 'Date\\tS-QUESTION\\n', '\\n', 'March\\tB-ANSWER\\n', '18,\\tI-ANSWER\\n', '1968\\tE-ANSWER\\n', '\\n', 'AUTHORS\\tS-QUESTION\\n', '\\n', 'Andrew\\tB-ANSWER\\n', 'G.\\tI-ANSWER\\n', 'Kallianos,\\tI-ANSWER\\n', 'Richard\\tI-ANSWER\\n', 'E.\\tI-ANSWER\\n', 'Means,\\tI-ANSWER\\n', 'James\\tI-ANSWER\\n', 'D.\\tI-ANSWER\\n', 'Mold\\tE-ANSWER\\n', '\\n', 'TITLE\\tS-QUESTION\\n', '\\n', '\"Effect\\tB-ANSWER\\n', 'of\\tI-ANSWER\\n', 'Nitrates\\tI-ANSWER\\n', 'in\\tI-ANSWER\\n', 'Tobacco\\tI-ANSWER\\n', 'on\\tI-ANSWER\\n', 'the\\tI-ANSWER\\n', 'Catechol\\tI-ANSWER\\n', 'Yield\\tI-ANSWER\\n', 'in\\tE-ANSWER\\n', '\\n', 'Cigarette\\tB-ANSWER\\n', 'Smoke\"\\tE-ANSWER\\n', '\\n', 'REVIEW\\tO\\n', 'COMPLETED.\\tO\\n', '\\n', '3/29/68\\tO\\n', '\\n', 'RECOMMENDATION:\\tO\\n', 'X\\tO\\n', 'APPROVE\\tO\\n', 'IN\\tO\\n', 'ITS\\tO\\n', '\\n', 'PRESENT\\tO\\n', 'FORM;\\tO\\n', '\\n', 'NOT\\tO\\n', 'APPROVE\\tO\\n', '(Give\\tO\\n', 'reasons\\tO\\n', 'below)\\tO\\n', ';\\tO\\n', '\\n', 'APPROVE\\tO\\n', 'TENTATIVELY,\\tO\\n', '\\n', 'SUBJECT\\tO\\n', 'TO\\tO\\n', 'THE\\tO\\n', 'FOLLOWING\\tO\\n', 'SUGGESTED\\tO\\n', \"REVISIONS':\\tO\\n\", '(itemize\\tO\\n', 'below)\\tO\\n', ':\\tO\\n', '\\n', 'Paye\\tO\\n', '4\\tO\\n', '-\\tO\\n', 'Last\\tO\\n', 'line\\tO\\n', 'should\\tO\\n', 'be\\tO\\n', 'Mass\\tO\\n', 'spectrometric,\\tO\\n', 'instead\\tO\\n', 'of\\tO\\n', '\\n', 'Mass\\tO\\n', 'spectroscopic.\\tO\\n', '\\n', '00070353\\tO\\n', '\\n', 'NOTE-Execute\\tO\\n', 'in\\tO\\n', 'triplicate\\tO\\n', 'using\\tO\\n', 'additional\\tO\\n', 'sheets\\tO\\n', 'if\\tO\\n', 'more\\tO\\n', 'space\\tO\\n', 'is\\tO\\n', 'required.\\tO\\n', 'Retain\\tO\\n', 'the\\tO\\n', 'third\\tO\\n', '\\n', 'copy\\tO\\n', 'for\\tO\\n', 'your\\tO\\n', 'file.\\tO\\n', 'Return\\tO\\n', 'the\\tO\\n', 'original\\tO\\n', '(signed)\\tO\\n', 'and\\tO\\n', 'the\\tO\\n', 'first\\tO\\n', 'carbon\\tO\\n', '(unsigned)\\tO\\n', 'along\\tO\\n', 'with\\tO\\n', 'the\\tO\\n', '\\n', 'manuscript\\tO\\n', 'to\\tO\\n', 'this\\tO\\n', 'office.\\tO\\n', 'The\\tO\\n', 'unsigned\\tO\\n', 'copy\\tO\\n', 'and\\tO\\n', 'the\\tO\\n', 'manuscript\\tO\\n', 'will\\tO\\n', 'be\\tO\\n', 'returned\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', 'author\\tO\\n', '\\n', 'for\\tO\\n', 'his\\tO\\n', 'consideration.\\tO\\n', '\\n', 'Manuscript\\tO\\n', 'Review\\tO\\n', 'Form\\tO\\n', '\\n', 'TOBACCO\\tB-HEADER\\n', 'SCIENCE\\tE-HEADER\\n', '\\n', 'Registration\\tB-QUESTION\\n', 'No.\\tE-QUESTION\\n', '\\n', '533\\tS-ANSWER\\n', '\\n', 'Date\\tS-QUESTION\\n', '\\n', 'March\\tB-ANSWER\\n', '18,\\tI-ANSWER\\n', '1968\\tE-ANSWER\\n', '\\n', 'AUTHORS\\tS-QUESTION\\n', '\\n', 'Andrew\\tB-ANSWER\\n', 'G.\\tI-ANSWER\\n', 'Kallianos,\\tI-ANSWER\\n', 'Richard\\tI-ANSWER\\n', 'E.\\tI-ANSWER\\n', 'Means,\\tI-ANSWER\\n', 'James\\tI-ANSWER\\n', 'D.\\tI-ANSWER\\n', 'Mold\\tE-ANSWER\\n', '\\n', 'TITLE\\tS-QUESTION\\n', '\\n', '\"Effect\\tB-ANSWER\\n', 'of\\tI-ANSWER\\n', 'Nitrates\\tI-ANSWER\\n', 'in\\tI-ANSWER\\n', 'Tobacco\\tI-ANSWER\\n', 'on\\tI-ANSWER\\n', 'the\\tI-ANSWER\\n', 'Catechol\\tI-ANSWER\\n', 'Yield\\tI-ANSWER\\n', 'in\\tE-ANSWER\\n', '\\n', 'Cigarette\\tB-ANSWER\\n', 'Smoke\"\\tE-ANSWER\\n', '\\n', 'REVIEW\\tO\\n', 'COMPLETED.\\tO\\n', '\\n', '3/29/68\\tO\\n', '\\n', 'RECOMMENDATION:\\tO\\n', 'X\\tO\\n', 'APPROVE\\tO\\n', 'IN\\tO\\n', 'ITS\\tO\\n', '\\n', 'PRESENT\\tO\\n', 'FORM;\\tO\\n', '\\n', 'NOT\\tO\\n', 'APPROVE\\tO\\n', '(Give\\tO\\n', 'reasons\\tO\\n', 'below)\\tO\\n', ';\\tO\\n', '\\n', 'APPROVE\\tO\\n', 'TENTATIVELY,\\tO\\n', '\\n', 'SUBJECT\\tO\\n', 'TO\\tO\\n', 'THE\\tO\\n', 'FOLLOWING\\tO\\n', 'SUGGESTED\\tO\\n', \"REVISIONS':\\tO\\n\", '(itemize\\tO\\n', 'below)\\tO\\n', ':\\tO\\n', '\\n', 'Paye\\tO\\n', '4\\tO\\n', '-\\tO\\n', 'Last\\tO\\n', 'line\\tO\\n', 'should\\tO\\n', 'be\\tO\\n', 'Mass\\tO\\n', 'spectrometric,\\tO\\n', 'instead\\tO\\n', 'of\\tO\\n', '\\n', 'Mass\\tO\\n', 'spectroscopic.\\tO\\n', '\\n', '00070353\\tO\\n', '\\n', 'NOTE-Execute\\tO\\n', 'in\\tO\\n', 'triplicate\\tO\\n', 'using\\tO\\n', 'additional\\tO\\n', 'sheets\\tO\\n', 'if\\tO\\n', 'more\\tO\\n', 'space\\tO\\n', 'is\\tO\\n', 'required.\\tO\\n', 'Retain\\tO\\n', 'the\\tO\\n', 'third\\tO\\n', '\\n', 'copy\\tO\\n', 'for\\tO\\n', 'your\\tO\\n', 'file.\\tO\\n', 'Return\\tO\\n', 'the\\tO\\n', 'original\\tO\\n', '(signed)\\tO\\n', 'and\\tO\\n', 'the\\tO\\n', 'first\\tO\\n', 'carbon\\tO\\n', '(unsigned)\\tO\\n', 'along\\tO\\n', 'with\\tO\\n', 'the\\tO\\n', '\\n', 'manuscript\\tO\\n', 'to\\tO\\n', 'this\\tO\\n', 'office.\\tO\\n', 'The\\tO\\n', 'unsigned\\tO\\n', 'copy\\tO\\n', 'and\\tO\\n', 'the\\tO\\n', 'manuscript\\tO\\n', 'will\\tO\\n', 'be\\tO\\n', 'returned\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', 'author\\tO\\n', '\\n', 'for\\tO\\n', 'his\\tO\\n', 'consideration.\\tO\\n', '\\n', 'Manuscript\\tO\\n', 'Review\\tO\\n', 'Form\\tO\\n', '\\n', 'TOBACCO\\tB-HEADER\\n', 'SCIENCE\\tE-HEADER\\n', '\\n', 'Registration\\tB-QUESTION\\n', 'No.\\tE-QUESTION\\n', '\\n', '533\\tS-ANSWER\\n', '\\n', 'Date\\tS-QUESTION\\n', '\\n', 'March\\tB-ANSWER\\n', '18,\\tI-ANSWER\\n', '1968\\tE-ANSWER\\n', '\\n', 'AUTHORS\\tS-QUESTION\\n', '\\n', 'Andrew\\tB-ANSWER\\n', 'G.\\tI-ANSWER\\n', 'Kallianos,\\tI-ANSWER\\n', 'Richard\\tI-ANSWER\\n', 'E.\\tI-ANSWER\\n', 'Means,\\tI-ANSWER\\n', 'James\\tI-ANSWER\\n', 'D.\\tI-ANSWER\\n', 'Mold\\tE-ANSWER\\n', '\\n', 'TITLE\\tS-QUESTION\\n', '\\n', '\"Effect\\tB-ANSWER\\n', 'of\\tI-ANSWER\\n', 'Nitrates\\tI-ANSWER\\n', 'in\\tI-ANSWER\\n', 'Tobacco\\tI-ANSWER\\n', 'on\\tI-ANSWER\\n', 'the\\tI-ANSWER\\n', 'Catechol\\tI-ANSWER\\n', 'Yield\\tI-ANSWER\\n', 'in\\tE-ANSWER\\n', '\\n', 'Cigarette\\tB-ANSWER\\n', 'Smoke\"\\tE-ANSWER\\n', '\\n', 'REVIEW\\tO\\n', 'COMPLETED.\\tO\\n', '\\n', '3/29/68\\tO\\n', '\\n', 'RECOMMENDATION:\\tO\\n', 'X\\tO\\n', 'APPROVE\\tO\\n', 'IN\\tO\\n', 'ITS\\tO\\n', '\\n', 'PRESENT\\tO\\n', 'FORM;\\tO\\n', '\\n', 'NOT\\tO\\n', 'APPROVE\\tO\\n', '(Give\\tO\\n', 'reasons\\tO\\n', 'below)\\tO\\n', ';\\tO\\n', '\\n', 'APPROVE\\tO\\n', 'TENTATIVELY,\\tO\\n', '\\n', 'SUBJECT\\tO\\n', 'TO\\tO\\n', 'THE\\tO\\n', 'FOLLOWING\\tO\\n', 'SUGGESTED\\tO\\n', \"REVISIONS':\\tO\\n\", '(itemize\\tO\\n', 'below)\\tO\\n', ':\\tO\\n', '\\n', 'Paye\\tO\\n', '4\\tO\\n', '-\\tO\\n', 'Last\\tO\\n', 'line\\tO\\n', 'should\\tO\\n', 'be\\tO\\n', 'Mass\\tO\\n', 'spectrometric,\\tO\\n', 'instead\\tO\\n', 'of\\tO\\n', '\\n', 'Mass\\tO\\n', 'spectroscopic.\\tO\\n', '\\n', '00070353\\tO\\n', '\\n', 'NOTE-Execute\\tO\\n', 'in\\tO\\n', 'triplicate\\tO\\n', 'using\\tO\\n', 'additional\\tO\\n', 'sheets\\tO\\n', 'if\\tO\\n', 'more\\tO\\n', 'space\\tO\\n', 'is\\tO\\n', 'required.\\tO\\n', 'Retain\\tO\\n', 'the\\tO\\n', 'third\\tO\\n', '\\n', 'copy\\tO\\n', 'for\\tO\\n', 'your\\tO\\n', 'file.\\tO\\n', 'Return\\tO\\n', 'the\\tO\\n', 'original\\tO\\n', '(signed)\\tO\\n', 'and\\tO\\n', 'the\\tO\\n', 'first\\tO\\n', 'carbon\\tO\\n', '(unsigned)\\tO\\n', 'along\\tO\\n', 'with\\tO\\n', 'the\\tO\\n', '\\n', 'manuscript\\tO\\n', 'to\\tO\\n', 'this\\tO\\n', 'office.\\tO\\n', 'The\\tO\\n', 'unsigned\\tO\\n', 'copy\\tO\\n', 'and\\tO\\n', 'the\\tO\\n', 'manuscript\\tO\\n', 'will\\tO\\n', 'be\\tO\\n', 'returned\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', 'author\\tO\\n', '\\n', 'for\\tO\\n', 'his\\tO\\n', 'consideration.\\tO\\n', '\\n', 'ATT.\\tO\\n', 'GEN.\\tO\\n', 'ADMIN.\\tO\\n', 'OFFICE\\tO\\n', 'Fax:614-466-5087\\tO\\n', '\\n', 'Dec\\tO\\n', '10\\tO\\n', \"'98\\tO\\n\", '17:46\\tO\\n', '\\n', 'P.\\tO\\n', '01\\tO\\n', '\\n', 'Attorney\\tO\\n', 'General\\tO\\n', '\\n', 'Betty\\tO\\n', 'D.\\tO\\n', 'Montgomery\\tO\\n', '\\n', 'CONFIDENTIAL\\tB-HEADER\\n', 'FACSIMILE\\tE-HEADER\\n', '\\n', 'TRANSMISSION\\tB-HEADER\\n', 'COVER\\tI-HEADER\\n', 'SHEET\\tE-HEADER\\n', '\\n', 'FAX\\tO\\n', 'NO.\\tO\\n', '(614)\\tO\\n', '466-5087\\tO\\n', '\\n', 'TO:\\tS-QUESTION\\n', '\\n', 'George\\tB-ANSWER\\n', 'Baroody\\tE-ANSWER\\n', '\\n', 'FAX\\tB-ANSWER\\n', 'NUMBER:\\tI-ANSWER\\n', '(336)\\tI-ANSWER\\n', '335-7392\\tE-ANSWER\\n', '\\n', 'PHONE\\tB-QUESTION\\n', 'NUMBER:\\tE-QUESTION\\n', '\\n', '(336)\\tB-ANSWER\\n', '335-7363\\tE-ANSWER\\n', '\\n', 'DATE:\\tS-QUESTION\\n', '\\n', '12/10/98\\tS-ANSWER\\n', '\\n', 'NUMBER\\tB-QUESTION\\n', 'OF\\tI-QUESTION\\n', 'PAGES\\tI-QUESTION\\n', 'INCLUDING\\tI-QUESTION\\n', 'COVER\\tI-QUESTION\\n', 'SHEET:\\tE-QUESTION\\n', '\\n', '3\\tS-ANSWER\\n', '\\n', 'SENDER/PHONE\\tB-ANSWER\\n', 'NUMBER:\\tI-ANSWER\\n', 'June\\tI-ANSWER\\n', 'Flynn\\tI-ANSWER\\n', 'for\\tI-ANSWER\\n', 'Eric\\tI-ANSWER\\n', 'Brown/(614)\\tI-ANSWER\\n', '466-8980\\tE-ANSWER\\n', '\\n', 'SPECIAL\\tO\\n', 'INSTRUCTIONS:\\tO\\n', '\\n', 'IF\\tO\\n', 'YOU\\tO\\n', 'DO\\tO\\n', 'NOT\\tO\\n', 'RECEIVE\\tO\\n', 'ANY\\tO\\n', 'OF\\tO\\n', 'THE\\tO\\n', 'PAGES\\tO\\n', 'PROPERLY,\\tO\\n', '\\n', 'PLEASE\\tO\\n', 'CONTACT\\tO\\n', 'SENDER\\tO\\n', '\\n', 'AS\\tO\\n', 'SOON\\tO\\n', 'AS\\tO\\n', 'POSSIBLE\\tO\\n', '\\n', 'NOTE:\\tO\\n', '\\n', 'THIS\\tO\\n', 'MESSAGE\\tO\\n', 'IS\\tO\\n', 'INTENDED\\tO\\n', 'ONLY\\tO\\n', 'FOR\\tO\\n', 'THE\\tO\\n', 'USE\\tO\\n', 'OF\\tO\\n', 'THE\\tO\\n', 'INDIVIDUAL\\tO\\n', 'OR\\tO\\n', 'ENTITY\\tO\\n', 'TO\\tO\\n', '\\n', 'WHOM\\tO\\n', 'IT\\tO\\n', 'IS\\tO\\n', 'ADDRESSED\\tO\\n', 'AND\\tO\\n', 'MAY\\tO\\n', 'CONTAIN\\tO\\n', 'INFORMATION\\tO\\n', 'THAT\\tO\\n', 'IS\\tO\\n', 'PRIVILEGED,\\tO\\n', '\\n', 'CONFIDENTIAL,\\tO\\n', 'AND\\tO\\n', 'EXEMPT\\tO\\n', 'FROM\\tO\\n', 'DISCLOSURE\\tO\\n', 'UNDER\\tO\\n', 'APPLICABLE\\tO\\n', 'LAW.\\tO\\n', 'If\\tO\\n', 'the\\tO\\n', '\\n', 'reader\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'message\\tO\\n', 'is\\tO\\n', 'not\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'employee\\tO\\n', 'or\\tO\\n', 'agent\\tO\\n', 'responsible\\tO\\n', 'for\\tO\\n', 'delivering\\tO\\n', '\\n', 'the\\tO\\n', 'message\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient,\\tO\\n', 'you\\tO\\n', 'are\\tO\\n', 'hereby\\tO\\n', 'notified\\tO\\n', 'that\\tO\\n', 'any\\tO\\n', 'dissemination,\\tO\\n', 'distribution,\\tO\\n', '\\n', 'copying,\\tO\\n', 'or\\tO\\n', 'conveying\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'communication\\tO\\n', 'in\\tO\\n', 'any\\tO\\n', 'manner\\tO\\n', 'Is\\tO\\n', 'strictly\\tO\\n', 'prohibited.\\tO\\n', 'If\\tO\\n', 'you\\tO\\n', 'have\\tO\\n', '\\n', 'received\\tO\\n', 'this\\tO\\n', 'communication\\tO\\n', 'in\\tO\\n', 'error,\\tO\\n', 'please\\tO\\n', 'notify\\tO\\n', 'us\\tO\\n', 'immediately\\tO\\n', 'by\\tO\\n', 'telephone\\tO\\n', 'and\\tO\\n', 'ream\\tO\\n', 'the\\tO\\n', '\\n', 'original\\tO\\n', 'message\\tO\\n', 'to\\tO\\n', 'us\\tO\\n', 'at\\tO\\n', 'the\\tO\\n', 'address\\tO\\n', 'below\\tO\\n', 'via\\tO\\n', 'the\\tO\\n', 'U.S.\\tO\\n', 'Postal\\tO\\n', 'Service.\\tO\\n', 'Thank\\tO\\n', 'you\\tO\\n', 'for\\tO\\n', 'your\\tO\\n', '\\n', 'cooperation.\\tO\\n', '\\n', '82092117\\tO\\n', '\\n', 'State\\tO\\n', 'Office\\tO\\n', 'Tower\\tO\\n', '/\\tO\\n', '30\\tO\\n', 'East\\tO\\n', 'Broad\\tO\\n', 'Street\\tO\\n', '/\\tO\\n', 'Columbus,\\tO\\n', 'Ohio\\tO\\n', '43215-3428\\tO\\n', '\\n', 'www.ag.state.oh.us\\tO\\n', '\\n', 'An\\tO\\n', 'Equal\\tO\\n', 'Opportunity\\tO\\n', 'Employer\\tO\\n', '\\n', '@Printed\\tO\\n', 'on\\tO\\n', 'Recycled\\tO\\n', 'Paper\\tO\\n', '\\n', 'ATT.\\tO\\n', 'GEN.\\tO\\n', 'ADMIN.\\tO\\n', 'OFFICE\\tO\\n', 'Fax:614-466-5087\\tO\\n', '\\n', 'Dec\\tO\\n', '10\\tO\\n', \"'98\\tO\\n\", '17:46\\tO\\n', '\\n', 'P.\\tO\\n', '01\\tO\\n', '\\n', 'Attorney\\tO\\n', 'General\\tO\\n', '\\n', 'Betty\\tO\\n', 'D.\\tO\\n', 'Montgomery\\tO\\n', '\\n', 'CONFIDENTIAL\\tB-HEADER\\n', 'FACSIMILE\\tE-HEADER\\n', '\\n', 'TRANSMISSION\\tB-HEADER\\n', 'COVER\\tI-HEADER\\n', 'SHEET\\tE-HEADER\\n', '\\n', 'FAX\\tO\\n', 'NO.\\tO\\n', '(614)\\tO\\n', '466-5087\\tO\\n', '\\n', 'TO:\\tS-QUESTION\\n', '\\n', 'George\\tB-ANSWER\\n', 'Baroody\\tE-ANSWER\\n', '\\n', 'FAX\\tB-ANSWER\\n', 'NUMBER:\\tI-ANSWER\\n', '(336)\\tI-ANSWER\\n', '335-7392\\tE-ANSWER\\n', '\\n', 'PHONE\\tB-QUESTION\\n', 'NUMBER:\\tE-QUESTION\\n', '\\n', '(336)\\tB-ANSWER\\n', '335-7363\\tE-ANSWER\\n', '\\n', 'DATE:\\tS-QUESTION\\n', '\\n', '12/10/98\\tS-ANSWER\\n', '\\n', 'NUMBER\\tB-QUESTION\\n', 'OF\\tI-QUESTION\\n', 'PAGES\\tI-QUESTION\\n', 'INCLUDING\\tI-QUESTION\\n', 'COVER\\tI-QUESTION\\n', 'SHEET:\\tE-QUESTION\\n', '\\n', '3\\tS-ANSWER\\n', '\\n', 'SENDER/PHONE\\tB-ANSWER\\n', 'NUMBER:\\tI-ANSWER\\n', 'June\\tI-ANSWER\\n', 'Flynn\\tI-ANSWER\\n', 'for\\tI-ANSWER\\n', 'Eric\\tI-ANSWER\\n', 'Brown/(614)\\tI-ANSWER\\n', '466-8980\\tE-ANSWER\\n', '\\n', 'SPECIAL\\tO\\n', 'INSTRUCTIONS:\\tO\\n', '\\n', 'IF\\tO\\n', 'YOU\\tO\\n', 'DO\\tO\\n', 'NOT\\tO\\n', 'RECEIVE\\tO\\n', 'ANY\\tO\\n', 'OF\\tO\\n', 'THE\\tO\\n', 'PAGES\\tO\\n', 'PROPERLY,\\tO\\n', '\\n', 'PLEASE\\tO\\n', 'CONTACT\\tO\\n', 'SENDER\\tO\\n', '\\n', 'AS\\tO\\n', 'SOON\\tO\\n', 'AS\\tO\\n', 'POSSIBLE\\tO\\n', '\\n', 'NOTE:\\tO\\n', '\\n', 'THIS\\tO\\n', 'MESSAGE\\tO\\n', 'IS\\tO\\n', 'INTENDED\\tO\\n', 'ONLY\\tO\\n', 'FOR\\tO\\n', 'THE\\tO\\n', 'USE\\tO\\n', 'OF\\tO\\n', 'THE\\tO\\n', 'INDIVIDUAL\\tO\\n', 'OR\\tO\\n', 'ENTITY\\tO\\n', 'TO\\tO\\n', '\\n', 'WHOM\\tO\\n', 'IT\\tO\\n', 'IS\\tO\\n', 'ADDRESSED\\tO\\n', 'AND\\tO\\n', 'MAY\\tO\\n', 'CONTAIN\\tO\\n', 'INFORMATION\\tO\\n', 'THAT\\tO\\n', 'IS\\tO\\n', 'PRIVILEGED,\\tO\\n', '\\n', 'CONFIDENTIAL,\\tO\\n', 'AND\\tO\\n', 'EXEMPT\\tO\\n', 'FROM\\tO\\n', 'DISCLOSURE\\tO\\n', 'UNDER\\tO\\n', 'APPLICABLE\\tO\\n', 'LAW.\\tO\\n', 'If\\tO\\n', 'the\\tO\\n', '\\n', 'reader\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'message\\tO\\n', 'is\\tO\\n', 'not\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'employee\\tO\\n', 'or\\tO\\n', 'agent\\tO\\n', 'responsible\\tO\\n', 'for\\tO\\n', 'delivering\\tO\\n', '\\n', 'the\\tO\\n', 'message\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient,\\tO\\n', 'you\\tO\\n', 'are\\tO\\n', 'hereby\\tO\\n', 'notified\\tO\\n', 'that\\tO\\n', 'any\\tO\\n', 'dissemination,\\tO\\n', 'distribution,\\tO\\n', '\\n', 'copying,\\tO\\n', 'or\\tO\\n', 'conveying\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'communication\\tO\\n', 'in\\tO\\n', 'any\\tO\\n', 'manner\\tO\\n', 'Is\\tO\\n', 'strictly\\tO\\n', 'prohibited.\\tO\\n', 'If\\tO\\n', 'you\\tO\\n', 'have\\tO\\n', '\\n', 'received\\tO\\n', 'this\\tO\\n', 'communication\\tO\\n', 'in\\tO\\n', 'error,\\tO\\n', 'please\\tO\\n', 'notify\\tO\\n', 'us\\tO\\n', 'immediately\\tO\\n', 'by\\tO\\n', 'telephone\\tO\\n', 'and\\tO\\n', 'ream\\tO\\n', 'the\\tO\\n', '\\n', 'original\\tO\\n', 'message\\tO\\n', 'to\\tO\\n', 'us\\tO\\n', 'at\\tO\\n', 'the\\tO\\n', 'address\\tO\\n', 'below\\tO\\n', 'via\\tO\\n', 'the\\tO\\n', 'U.S.\\tO\\n', 'Postal\\tO\\n', 'Service.\\tO\\n', 'Thank\\tO\\n', 'you\\tO\\n', 'for\\tO\\n', 'your\\tO\\n', '\\n', 'cooperation.\\tO\\n', '\\n', '82092117\\tO\\n', '\\n', 'State\\tO\\n', 'Office\\tO\\n', 'Tower\\tO\\n', '/\\tO\\n', '30\\tO\\n', 'East\\tO\\n', 'Broad\\tO\\n', 'Street\\tO\\n', '/\\tO\\n', 'Columbus,\\tO\\n', 'Ohio\\tO\\n', '43215-3428\\tO\\n', '\\n', 'www.ag.state.oh.us\\tO\\n', '\\n', 'An\\tO\\n', 'Equal\\tO\\n', 'Opportunity\\tO\\n', 'Employer\\tO\\n', '\\n', '@Printed\\tO\\n', 'on\\tO\\n', 'Recycled\\tO\\n', 'Paper\\tO\\n', '\\n', 'ATT.\\tO\\n', 'GEN.\\tO\\n', 'ADMIN.\\tO\\n', 'OFFICE\\tO\\n', 'Fax:614-466-5087\\tO\\n', '\\n', 'Dec\\tO\\n', '10\\tO\\n', \"'98\\tO\\n\", '17:46\\tO\\n', '\\n', 'P.\\tO\\n', '01\\tO\\n', '\\n', 'Attorney\\tO\\n', 'General\\tO\\n', '\\n', 'Betty\\tO\\n', 'D.\\tO\\n', 'Montgomery\\tO\\n', '\\n', 'CONFIDENTIAL\\tB-HEADER\\n', 'FACSIMILE\\tE-HEADER\\n', '\\n', 'TRANSMISSION\\tB-HEADER\\n', 'COVER\\tI-HEADER\\n', 'SHEET\\tE-HEADER\\n', '\\n', 'FAX\\tO\\n', 'NO.\\tO\\n', '(614)\\tO\\n', '466-5087\\tO\\n', '\\n', 'TO:\\tS-QUESTION\\n', '\\n', 'George\\tB-ANSWER\\n', 'Baroody\\tE-ANSWER\\n', '\\n', 'FAX\\tB-ANSWER\\n', 'NUMBER:\\tI-ANSWER\\n', '(336)\\tI-ANSWER\\n', '335-7392\\tE-ANSWER\\n', '\\n', 'PHONE\\tB-QUESTION\\n', 'NUMBER:\\tE-QUESTION\\n', '\\n', '(336)\\tB-ANSWER\\n', '335-7363\\tE-ANSWER\\n', '\\n', 'DATE:\\tS-QUESTION\\n', '\\n', '12/10/98\\tS-ANSWER\\n', '\\n', 'NUMBER\\tB-QUESTION\\n', 'OF\\tI-QUESTION\\n', 'PAGES\\tI-QUESTION\\n', 'INCLUDING\\tI-QUESTION\\n', 'COVER\\tI-QUESTION\\n', 'SHEET:\\tE-QUESTION\\n', '\\n', '3\\tS-ANSWER\\n', '\\n', 'SENDER/PHONE\\tB-ANSWER\\n', 'NUMBER:\\tI-ANSWER\\n', 'June\\tI-ANSWER\\n', 'Flynn\\tI-ANSWER\\n', 'for\\tI-ANSWER\\n', 'Eric\\tI-ANSWER\\n', 'Brown/(614)\\tI-ANSWER\\n', '466-8980\\tE-ANSWER\\n', '\\n', 'SPECIAL\\tO\\n', 'INSTRUCTIONS:\\tO\\n', '\\n', 'IF\\tO\\n', 'YOU\\tO\\n', 'DO\\tO\\n', 'NOT\\tO\\n', 'RECEIVE\\tO\\n', 'ANY\\tO\\n', 'OF\\tO\\n', 'THE\\tO\\n', 'PAGES\\tO\\n', 'PROPERLY,\\tO\\n', '\\n', 'PLEASE\\tO\\n', 'CONTACT\\tO\\n', 'SENDER\\tO\\n', '\\n', 'AS\\tO\\n', 'SOON\\tO\\n', 'AS\\tO\\n', 'POSSIBLE\\tO\\n', '\\n', 'NOTE:\\tO\\n', '\\n', 'THIS\\tO\\n', 'MESSAGE\\tO\\n', 'IS\\tO\\n', 'INTENDED\\tO\\n', 'ONLY\\tO\\n', 'FOR\\tO\\n', 'THE\\tO\\n', 'USE\\tO\\n', 'OF\\tO\\n', 'THE\\tO\\n', 'INDIVIDUAL\\tO\\n', 'OR\\tO\\n', 'ENTITY\\tO\\n', 'TO\\tO\\n', '\\n', 'WHOM\\tO\\n', 'IT\\tO\\n', 'IS\\tO\\n', 'ADDRESSED\\tO\\n', 'AND\\tO\\n', 'MAY\\tO\\n', 'CONTAIN\\tO\\n', 'INFORMATION\\tO\\n', 'THAT\\tO\\n', 'IS\\tO\\n', 'PRIVILEGED,\\tO\\n', '\\n', 'CONFIDENTIAL,\\tO\\n', 'AND\\tO\\n', 'EXEMPT\\tO\\n', 'FROM\\tO\\n', 'DISCLOSURE\\tO\\n', 'UNDER\\tO\\n', 'APPLICABLE\\tO\\n', 'LAW.\\tO\\n', 'If\\tO\\n', 'the\\tO\\n', '\\n', 'reader\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'message\\tO\\n', 'is\\tO\\n', 'not\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'employee\\tO\\n', 'or\\tO\\n', 'agent\\tO\\n', 'responsible\\tO\\n', 'for\\tO\\n', 'delivering\\tO\\n', '\\n', 'the\\tO\\n', 'message\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient,\\tO\\n', 'you\\tO\\n', 'are\\tO\\n', 'hereby\\tO\\n', 'notified\\tO\\n', 'that\\tO\\n', 'any\\tO\\n', 'dissemination,\\tO\\n', 'distribution,\\tO\\n', '\\n', 'copying,\\tO\\n', 'or\\tO\\n', 'conveying\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'communication\\tO\\n', 'in\\tO\\n', 'any\\tO\\n', 'manner\\tO\\n', 'Is\\tO\\n', 'strictly\\tO\\n', 'prohibited.\\tO\\n', 'If\\tO\\n', 'you\\tO\\n', 'have\\tO\\n', '\\n', 'received\\tO\\n', 'this\\tO\\n', 'communication\\tO\\n', 'in\\tO\\n', 'error,\\tO\\n', 'please\\tO\\n', 'notify\\tO\\n', 'us\\tO\\n', 'immediately\\tO\\n', 'by\\tO\\n', 'telephone\\tO\\n', 'and\\tO\\n', 'ream\\tO\\n', 'the\\tO\\n', '\\n', 'original\\tO\\n', 'message\\tO\\n', 'to\\tO\\n', 'us\\tO\\n', 'at\\tO\\n', 'the\\tO\\n', 'address\\tO\\n', 'below\\tO\\n', 'via\\tO\\n', 'the\\tO\\n', 'U.S.\\tO\\n', 'Postal\\tO\\n', 'Service.\\tO\\n', 'Thank\\tO\\n', 'you\\tO\\n', 'for\\tO\\n', 'your\\tO\\n', '\\n', 'cooperation.\\tO\\n', '\\n', '82092117\\tO\\n', '\\n', 'State\\tO\\n', 'Office\\tO\\n', 'Tower\\tO\\n', '/\\tO\\n', '30\\tO\\n', 'East\\tO\\n', 'Broad\\tO\\n', 'Street\\tO\\n', '/\\tO\\n', 'Columbus,\\tO\\n', 'Ohio\\tO\\n', '43215-3428\\tO\\n', '\\n', 'www.ag.state.oh.us\\tO\\n', '\\n', 'An\\tO\\n', 'Equal\\tO\\n', 'Opportunity\\tO\\n', 'Employer\\tO\\n', '\\n', '@Printed\\tO\\n', 'on\\tO\\n', 'Recycled\\tO\\n', 'Paper\\tO\\n', '\\n', 'ATT.\\tO\\n', 'GEN.\\tO\\n', 'ADMIN.\\tO\\n', 'OFFICE\\tO\\n', 'Fax:614-466-5087\\tO\\n', '\\n', 'Dec\\tO\\n', '10\\tO\\n', \"'98\\tO\\n\", '17:46\\tO\\n', '\\n', 'P.\\tO\\n', '01\\tO\\n', '\\n', 'Attorney\\tO\\n', 'General\\tO\\n', '\\n', 'Betty\\tO\\n', 'D.\\tO\\n', 'Montgomery\\tO\\n', '\\n', 'CONFIDENTIAL\\tB-HEADER\\n', 'FACSIMILE\\tE-HEADER\\n', '\\n', 'TRANSMISSION\\tB-HEADER\\n', 'COVER\\tI-HEADER\\n', 'SHEET\\tE-HEADER\\n', '\\n', 'FAX\\tO\\n', 'NO.\\tO\\n', '(614)\\tO\\n', '466-5087\\tO\\n', '\\n', 'TO:\\tS-QUESTION\\n', '\\n', 'George\\tB-ANSWER\\n', 'Baroody\\tE-ANSWER\\n', '\\n', 'FAX\\tB-ANSWER\\n', 'NUMBER:\\tI-ANSWER\\n', '(336)\\tI-ANSWER\\n', '335-7392\\tE-ANSWER\\n', '\\n', 'PHONE\\tB-QUESTION\\n', 'NUMBER:\\tE-QUESTION\\n', '\\n', '(336)\\tB-ANSWER\\n', '335-7363\\tE-ANSWER\\n', '\\n', 'DATE:\\tS-QUESTION\\n', '\\n', '12/10/98\\tS-ANSWER\\n', '\\n', 'NUMBER\\tB-QUESTION\\n', 'OF\\tI-QUESTION\\n', 'PAGES\\tI-QUESTION\\n', 'INCLUDING\\tI-QUESTION\\n', 'COVER\\tI-QUESTION\\n', 'SHEET:\\tE-QUESTION\\n', '\\n', '3\\tS-ANSWER\\n', '\\n', 'SENDER/PHONE\\tB-ANSWER\\n', 'NUMBER:\\tI-ANSWER\\n', 'June\\tI-ANSWER\\n', 'Flynn\\tI-ANSWER\\n', 'for\\tI-ANSWER\\n', 'Eric\\tI-ANSWER\\n', 'Brown/(614)\\tI-ANSWER\\n', '466-8980\\tE-ANSWER\\n', '\\n', 'SPECIAL\\tO\\n', 'INSTRUCTIONS:\\tO\\n', '\\n', 'IF\\tO\\n', 'YOU\\tO\\n', 'DO\\tO\\n', 'NOT\\tO\\n', 'RECEIVE\\tO\\n', 'ANY\\tO\\n', 'OF\\tO\\n', 'THE\\tO\\n', 'PAGES\\tO\\n', 'PROPERLY,\\tO\\n', '\\n', 'PLEASE\\tO\\n', 'CONTACT\\tO\\n', 'SENDER\\tO\\n', '\\n', 'AS\\tO\\n', 'SOON\\tO\\n', 'AS\\tO\\n', 'POSSIBLE\\tO\\n', '\\n', 'NOTE:\\tO\\n', '\\n', 'THIS\\tO\\n', 'MESSAGE\\tO\\n', 'IS\\tO\\n', 'INTENDED\\tO\\n', 'ONLY\\tO\\n', 'FOR\\tO\\n', 'THE\\tO\\n', 'USE\\tO\\n', 'OF\\tO\\n', 'THE\\tO\\n', 'INDIVIDUAL\\tO\\n', 'OR\\tO\\n', 'ENTITY\\tO\\n', 'TO\\tO\\n', '\\n', 'WHOM\\tO\\n', 'IT\\tO\\n', 'IS\\tO\\n', 'ADDRESSED\\tO\\n', 'AND\\tO\\n', 'MAY\\tO\\n', 'CONTAIN\\tO\\n', 'INFORMATION\\tO\\n', 'THAT\\tO\\n', 'IS\\tO\\n', 'PRIVILEGED,\\tO\\n', '\\n', 'CONFIDENTIAL,\\tO\\n', 'AND\\tO\\n', 'EXEMPT\\tO\\n', 'FROM\\tO\\n', 'DISCLOSURE\\tO\\n', 'UNDER\\tO\\n', 'APPLICABLE\\tO\\n', 'LAW.\\tO\\n', 'If\\tO\\n', 'the\\tO\\n', '\\n', 'reader\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'message\\tO\\n', 'is\\tO\\n', 'not\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'employee\\tO\\n', 'or\\tO\\n', 'agent\\tO\\n', 'responsible\\tO\\n', 'for\\tO\\n', 'delivering\\tO\\n', '\\n', 'the\\tO\\n', 'message\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient,\\tO\\n', 'you\\tO\\n', 'are\\tO\\n', 'hereby\\tO\\n', 'notified\\tO\\n', 'that\\tO\\n', 'any\\tO\\n', 'dissemination,\\tO\\n', 'distribution,\\tO\\n', '\\n', 'copying,\\tO\\n', 'or\\tO\\n', 'conveying\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'communication\\tO\\n', 'in\\tO\\n', 'any\\tO\\n', 'manner\\tO\\n', 'Is\\tO\\n', 'strictly\\tO\\n', 'prohibited.\\tO\\n', 'If\\tO\\n', 'you\\tO\\n', 'have\\tO\\n', '\\n', 'received\\tO\\n', 'this\\tO\\n', 'communication\\tO\\n', 'in\\tO\\n', 'error,\\tO\\n', 'please\\tO\\n', 'notify\\tO\\n', 'us\\tO\\n', 'immediately\\tO\\n', 'by\\tO\\n', 'telephone\\tO\\n', 'and\\tO\\n', 'ream\\tO\\n', 'the\\tO\\n', '\\n', 'original\\tO\\n', 'message\\tO\\n', 'to\\tO\\n', 'us\\tO\\n', 'at\\tO\\n', 'the\\tO\\n', 'address\\tO\\n', 'below\\tO\\n', 'via\\tO\\n', 'the\\tO\\n', 'U.S.\\tO\\n', 'Postal\\tO\\n', 'Service.\\tO\\n', 'Thank\\tO\\n', 'you\\tO\\n', 'for\\tO\\n', 'your\\tO\\n', '\\n', 'cooperation.\\tO\\n', '\\n', '82092117\\tO\\n', '\\n', 'State\\tO\\n', 'Office\\tO\\n', 'Tower\\tO\\n', '/\\tO\\n', '30\\tO\\n', 'East\\tO\\n', 'Broad\\tO\\n', 'Street\\tO\\n', '/\\tO\\n', 'Columbus,\\tO\\n', 'Ohio\\tO\\n', '43215-3428\\tO\\n', '\\n', 'www.ag.state.oh.us\\tO\\n', '\\n', 'An\\tO\\n', 'Equal\\tO\\n', 'Opportunity\\tO\\n', 'Employer\\tO\\n', '\\n', '@Printed\\tO\\n', 'on\\tO\\n', 'Recycled\\tO\\n', 'Paper\\tO\\n', '\\n', 'ATT.\\tO\\n', 'GEN.\\tO\\n', 'ADMIN.\\tO\\n', 'OFFICE\\tO\\n', 'Fax:614-466-5087\\tO\\n', '\\n', 'Dec\\tO\\n', '10\\tO\\n', \"'98\\tO\\n\", '17:46\\tO\\n', '\\n', 'P.\\tO\\n', '01\\tO\\n', '\\n', 'Attorney\\tO\\n', 'General\\tO\\n', '\\n', 'Betty\\tO\\n', 'D.\\tO\\n', 'Montgomery\\tO\\n', '\\n', 'CONFIDENTIAL\\tB-HEADER\\n', 'FACSIMILE\\tE-HEADER\\n', '\\n', 'TRANSMISSION\\tB-HEADER\\n', 'COVER\\tI-HEADER\\n', 'SHEET\\tE-HEADER\\n', '\\n', 'FAX\\tO\\n', 'NO.\\tO\\n', '(614)\\tO\\n', '466-5087\\tO\\n', '\\n', 'TO:\\tS-QUESTION\\n', '\\n', 'George\\tB-ANSWER\\n', 'Baroody\\tE-ANSWER\\n', '\\n', 'FAX\\tB-ANSWER\\n', 'NUMBER:\\tI-ANSWER\\n', '(336)\\tI-ANSWER\\n', '335-7392\\tE-ANSWER\\n', '\\n', 'PHONE\\tB-QUESTION\\n', 'NUMBER:\\tE-QUESTION\\n', '\\n', '(336)\\tB-ANSWER\\n', '335-7363\\tE-ANSWER\\n', '\\n', 'DATE:\\tS-QUESTION\\n', '\\n', '12/10/98\\tS-ANSWER\\n', '\\n', 'NUMBER\\tB-QUESTION\\n', 'OF\\tI-QUESTION\\n', 'PAGES\\tI-QUESTION\\n', 'INCLUDING\\tI-QUESTION\\n', 'COVER\\tI-QUESTION\\n', 'SHEET:\\tE-QUESTION\\n', '\\n', '3\\tS-ANSWER\\n', '\\n', 'SENDER/PHONE\\tB-ANSWER\\n', 'NUMBER:\\tI-ANSWER\\n', 'June\\tI-ANSWER\\n', 'Flynn\\tI-ANSWER\\n', 'for\\tI-ANSWER\\n', 'Eric\\tI-ANSWER\\n', 'Brown/(614)\\tI-ANSWER\\n', '466-8980\\tE-ANSWER\\n', '\\n', 'SPECIAL\\tO\\n', 'INSTRUCTIONS:\\tO\\n', '\\n', 'IF\\tO\\n', 'YOU\\tO\\n', 'DO\\tO\\n', 'NOT\\tO\\n', 'RECEIVE\\tO\\n', 'ANY\\tO\\n', 'OF\\tO\\n', 'THE\\tO\\n', 'PAGES\\tO\\n', 'PROPERLY,\\tO\\n', '\\n', 'PLEASE\\tO\\n', 'CONTACT\\tO\\n', 'SENDER\\tO\\n', '\\n', 'AS\\tO\\n', 'SOON\\tO\\n', 'AS\\tO\\n', 'POSSIBLE\\tO\\n', '\\n', 'NOTE:\\tO\\n', '\\n', 'THIS\\tO\\n', 'MESSAGE\\tO\\n', 'IS\\tO\\n', 'INTENDED\\tO\\n', 'ONLY\\tO\\n', 'FOR\\tO\\n', 'THE\\tO\\n', 'USE\\tO\\n', 'OF\\tO\\n', 'THE\\tO\\n', 'INDIVIDUAL\\tO\\n', 'OR\\tO\\n', 'ENTITY\\tO\\n', 'TO\\tO\\n', '\\n', 'WHOM\\tO\\n', 'IT\\tO\\n', 'IS\\tO\\n', 'ADDRESSED\\tO\\n', 'AND\\tO\\n', 'MAY\\tO\\n', 'CONTAIN\\tO\\n', 'INFORMATION\\tO\\n', 'THAT\\tO\\n', 'IS\\tO\\n', 'PRIVILEGED,\\tO\\n', '\\n', 'CONFIDENTIAL,\\tO\\n', 'AND\\tO\\n', 'EXEMPT\\tO\\n', 'FROM\\tO\\n', 'DISCLOSURE\\tO\\n', 'UNDER\\tO\\n', 'APPLICABLE\\tO\\n', 'LAW.\\tO\\n', 'If\\tO\\n', 'the\\tO\\n', '\\n', 'reader\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'message\\tO\\n', 'is\\tO\\n', 'not\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'employee\\tO\\n', 'or\\tO\\n', 'agent\\tO\\n', 'responsible\\tO\\n', 'for\\tO\\n', 'delivering\\tO\\n', '\\n', 'the\\tO\\n', 'message\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient,\\tO\\n', 'you\\tO\\n', 'are\\tO\\n', 'hereby\\tO\\n', 'notified\\tO\\n', 'that\\tO\\n', 'any\\tO\\n', 'dissemination,\\tO\\n', 'distribution,\\tO\\n', '\\n', 'copying,\\tO\\n', 'or\\tO\\n', 'conveying\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'communication\\tO\\n', 'in\\tO\\n', 'any\\tO\\n', 'manner\\tO\\n', 'Is\\tO\\n', 'strictly\\tO\\n', 'prohibited.\\tO\\n', 'If\\tO\\n', 'you\\tO\\n', 'have\\tO\\n', '\\n', 'received\\tO\\n', 'this\\tO\\n', 'communication\\tO\\n', 'in\\tO\\n', 'error,\\tO\\n', 'please\\tO\\n', 'notify\\tO\\n', 'us\\tO\\n', 'immediately\\tO\\n', 'by\\tO\\n', 'telephone\\tO\\n', 'and\\tO\\n', 'ream\\tO\\n', 'the\\tO\\n', '\\n', 'original\\tO\\n', 'message\\tO\\n', 'to\\tO\\n', 'us\\tO\\n', 'at\\tO\\n', 'the\\tO\\n', 'address\\tO\\n', 'below\\tO\\n', 'via\\tO\\n', 'the\\tO\\n', 'U.S.\\tO\\n', 'Postal\\tO\\n', 'Service.\\tO\\n', 'Thank\\tO\\n', 'you\\tO\\n', 'for\\tO\\n', 'your\\tO\\n', '\\n', 'cooperation.\\tO\\n', '\\n', '82092117\\tO\\n', '\\n', 'State\\tO\\n', 'Office\\tO\\n', 'Tower\\tO\\n', '/\\tO\\n', '30\\tO\\n', 'East\\tO\\n', 'Broad\\tO\\n', 'Street\\tO\\n', '/\\tO\\n', 'Columbus,\\tO\\n', 'Ohio\\tO\\n', '43215-3428\\tO\\n', '\\n', 'www.ag.state.oh.us\\tO\\n', '\\n', 'An\\tO\\n', 'Equal\\tO\\n', 'Opportunity\\tO\\n', 'Employer\\tO\\n', '\\n', '@Printed\\tO\\n', 'on\\tO\\n', 'Recycled\\tO\\n', 'Paper\\tO\\n', '\\n', 'ATT.\\tO\\n', 'GEN.\\tO\\n', 'ADMIN.\\tO\\n', 'OFFICE\\tO\\n', 'Fax:614-466-5087\\tO\\n', '\\n', 'Dec\\tO\\n', '10\\tO\\n', \"'98\\tO\\n\", '17:46\\tO\\n', '\\n', 'P.\\tO\\n', '01\\tO\\n', '\\n', 'Attorney\\tO\\n', 'General\\tO\\n', '\\n', 'Betty\\tO\\n', 'D.\\tO\\n', 'Montgomery\\tO\\n', '\\n', 'CONFIDENTIAL\\tB-HEADER\\n', 'FACSIMILE\\tE-HEADER\\n', '\\n', 'TRANSMISSION\\tB-HEADER\\n', 'COVER\\tI-HEADER\\n', 'SHEET\\tE-HEADER\\n', '\\n', 'FAX\\tO\\n', 'NO.\\tO\\n', '(614)\\tO\\n', '466-5087\\tO\\n', '\\n', 'TO:\\tS-QUESTION\\n', '\\n', 'George\\tB-ANSWER\\n', 'Baroody\\tE-ANSWER\\n', '\\n', 'FAX\\tB-ANSWER\\n', 'NUMBER:\\tI-ANSWER\\n', '(336)\\tI-ANSWER\\n', '335-7392\\tE-ANSWER\\n', '\\n', 'PHONE\\tB-QUESTION\\n', 'NUMBER:\\tE-QUESTION\\n', '\\n', '(336)\\tB-ANSWER\\n', '335-7363\\tE-ANSWER\\n', '\\n', 'DATE:\\tS-QUESTION\\n', '\\n', '12/10/98\\tS-ANSWER\\n', '\\n', 'NUMBER\\tB-QUESTION\\n', 'OF\\tI-QUESTION\\n', 'PAGES\\tI-QUESTION\\n', 'INCLUDING\\tI-QUESTION\\n', 'COVER\\tI-QUESTION\\n', 'SHEET:\\tE-QUESTION\\n', '\\n', '3\\tS-ANSWER\\n', '\\n', 'SENDER/PHONE\\tB-ANSWER\\n', 'NUMBER:\\tI-ANSWER\\n', 'June\\tI-ANSWER\\n', 'Flynn\\tI-ANSWER\\n', 'for\\tI-ANSWER\\n', 'Eric\\tI-ANSWER\\n', 'Brown/(614)\\tI-ANSWER\\n', '466-8980\\tE-ANSWER\\n', '\\n', 'SPECIAL\\tO\\n', 'INSTRUCTIONS:\\tO\\n', '\\n', 'IF\\tO\\n', 'YOU\\tO\\n', 'DO\\tO\\n', 'NOT\\tO\\n', 'RECEIVE\\tO\\n', 'ANY\\tO\\n', 'OF\\tO\\n', 'THE\\tO\\n', 'PAGES\\tO\\n', 'PROPERLY,\\tO\\n', '\\n', 'PLEASE\\tO\\n', 'CONTACT\\tO\\n', 'SENDER\\tO\\n', '\\n', 'AS\\tO\\n', 'SOON\\tO\\n', 'AS\\tO\\n', 'POSSIBLE\\tO\\n', '\\n', 'NOTE:\\tO\\n', '\\n', 'THIS\\tO\\n', 'MESSAGE\\tO\\n', 'IS\\tO\\n', 'INTENDED\\tO\\n', 'ONLY\\tO\\n', 'FOR\\tO\\n', 'THE\\tO\\n', 'USE\\tO\\n', 'OF\\tO\\n', 'THE\\tO\\n', 'INDIVIDUAL\\tO\\n', 'OR\\tO\\n', 'ENTITY\\tO\\n', 'TO\\tO\\n', '\\n', 'WHOM\\tO\\n', 'IT\\tO\\n', 'IS\\tO\\n', 'ADDRESSED\\tO\\n', 'AND\\tO\\n', 'MAY\\tO\\n', 'CONTAIN\\tO\\n', 'INFORMATION\\tO\\n', 'THAT\\tO\\n', 'IS\\tO\\n', 'PRIVILEGED,\\tO\\n', '\\n', 'CONFIDENTIAL,\\tO\\n', 'AND\\tO\\n', 'EXEMPT\\tO\\n', 'FROM\\tO\\n', 'DISCLOSURE\\tO\\n', 'UNDER\\tO\\n', 'APPLICABLE\\tO\\n', 'LAW.\\tO\\n', 'If\\tO\\n', 'the\\tO\\n', '\\n', 'reader\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'message\\tO\\n', 'is\\tO\\n', 'not\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'employee\\tO\\n', 'or\\tO\\n', 'agent\\tO\\n', 'responsible\\tO\\n', 'for\\tO\\n', 'delivering\\tO\\n', '\\n', 'the\\tO\\n', 'message\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient,\\tO\\n', 'you\\tO\\n', 'are\\tO\\n', 'hereby\\tO\\n', 'notified\\tO\\n', 'that\\tO\\n', 'any\\tO\\n', 'dissemination,\\tO\\n', 'distribution,\\tO\\n', '\\n', 'copying,\\tO\\n', 'or\\tO\\n', 'conveying\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'communication\\tO\\n', 'in\\tO\\n', 'any\\tO\\n', 'manner\\tO\\n', 'Is\\tO\\n', 'strictly\\tO\\n', 'prohibited.\\tO\\n', 'If\\tO\\n', 'you\\tO\\n', 'have\\tO\\n', '\\n', 'received\\tO\\n', 'this\\tO\\n', 'communication\\tO\\n', 'in\\tO\\n', 'error,\\tO\\n', 'please\\tO\\n', 'notify\\tO\\n', 'us\\tO\\n', 'immediately\\tO\\n', 'by\\tO\\n', 'telephone\\tO\\n', 'and\\tO\\n', 'ream\\tO\\n', 'the\\tO\\n', '\\n', 'original\\tO\\n', 'message\\tO\\n', 'to\\tO\\n', 'us\\tO\\n', 'at\\tO\\n', 'the\\tO\\n', 'address\\tO\\n', 'below\\tO\\n', 'via\\tO\\n', 'the\\tO\\n', 'U.S.\\tO\\n', 'Postal\\tO\\n', 'Service.\\tO\\n', 'Thank\\tO\\n', 'you\\tO\\n', 'for\\tO\\n', 'your\\tO\\n', '\\n', 'cooperation.\\tO\\n', '\\n', '82092117\\tO\\n', '\\n', 'State\\tO\\n', 'Office\\tO\\n', 'Tower\\tO\\n', '/\\tO\\n', '30\\tO\\n', 'East\\tO\\n', 'Broad\\tO\\n', 'Street\\tO\\n', '/\\tO\\n', 'Columbus,\\tO\\n', 'Ohio\\tO\\n', '43215-3428\\tO\\n', '\\n', 'www.ag.state.oh.us\\tO\\n', '\\n', 'An\\tO\\n', 'Equal\\tO\\n', 'Opportunity\\tO\\n', 'Employer\\tO\\n', '\\n', '@Printed\\tO\\n', 'on\\tO\\n', 'Recycled\\tO\\n', 'Paper\\tO\\n', '\\n', 'ATT.\\tO\\n', 'GEN.\\tO\\n', 'ADMIN.\\tO\\n', 'OFFICE\\tO\\n', 'Fax:614-466-5087\\tO\\n', '\\n', 'Dec\\tO\\n', '10\\tO\\n', \"'98\\tO\\n\", '17:46\\tO\\n', '\\n', 'P.\\tO\\n', '01\\tO\\n', '\\n', 'Attorney\\tO\\n', 'General\\tO\\n', '\\n', 'Betty\\tO\\n', 'D.\\tO\\n', 'Montgomery\\tO\\n', '\\n', 'CONFIDENTIAL\\tB-HEADER\\n', 'FACSIMILE\\tE-HEADER\\n', '\\n', 'TRANSMISSION\\tB-HEADER\\n', 'COVER\\tI-HEADER\\n', 'SHEET\\tE-HEADER\\n', '\\n', 'FAX\\tO\\n', 'NO.\\tO\\n', '(614)\\tO\\n', '466-5087\\tO\\n', '\\n', 'TO:\\tS-QUESTION\\n', '\\n', 'George\\tB-ANSWER\\n', 'Baroody\\tE-ANSWER\\n', '\\n', 'FAX\\tB-ANSWER\\n', 'NUMBER:\\tI-ANSWER\\n', '(336)\\tI-ANSWER\\n', '335-7392\\tE-ANSWER\\n', '\\n', 'PHONE\\tB-QUESTION\\n', 'NUMBER:\\tE-QUESTION\\n', '\\n', '(336)\\tB-ANSWER\\n', '335-7363\\tE-ANSWER\\n', '\\n', 'DATE:\\tS-QUESTION\\n', '\\n', '12/10/98\\tS-ANSWER\\n', '\\n', 'NUMBER\\tB-QUESTION\\n', 'OF\\tI-QUESTION\\n', 'PAGES\\tI-QUESTION\\n', 'INCLUDING\\tI-QUESTION\\n', 'COVER\\tI-QUESTION\\n', 'SHEET:\\tE-QUESTION\\n', '\\n', '3\\tS-ANSWER\\n', '\\n', 'SENDER/PHONE\\tB-ANSWER\\n', 'NUMBER:\\tI-ANSWER\\n', 'June\\tI-ANSWER\\n', 'Flynn\\tI-ANSWER\\n', 'for\\tI-ANSWER\\n', 'Eric\\tI-ANSWER\\n', 'Brown/(614)\\tI-ANSWER\\n', '466-8980\\tE-ANSWER\\n', '\\n', 'SPECIAL\\tO\\n', 'INSTRUCTIONS:\\tO\\n', '\\n', 'IF\\tO\\n', 'YOU\\tO\\n', 'DO\\tO\\n', 'NOT\\tO\\n', 'RECEIVE\\tO\\n', 'ANY\\tO\\n', 'OF\\tO\\n', 'THE\\tO\\n', 'PAGES\\tO\\n', 'PROPERLY,\\tO\\n', '\\n', 'PLEASE\\tO\\n', 'CONTACT\\tO\\n', 'SENDER\\tO\\n', '\\n', 'AS\\tO\\n', 'SOON\\tO\\n', 'AS\\tO\\n', 'POSSIBLE\\tO\\n', '\\n', 'NOTE:\\tO\\n', '\\n', 'THIS\\tO\\n', 'MESSAGE\\tO\\n', 'IS\\tO\\n', 'INTENDED\\tO\\n', 'ONLY\\tO\\n', 'FOR\\tO\\n', 'THE\\tO\\n', 'USE\\tO\\n', 'OF\\tO\\n', 'THE\\tO\\n', 'INDIVIDUAL\\tO\\n', 'OR\\tO\\n', 'ENTITY\\tO\\n', 'TO\\tO\\n', '\\n', 'WHOM\\tO\\n', 'IT\\tO\\n', 'IS\\tO\\n', 'ADDRESSED\\tO\\n', 'AND\\tO\\n', 'MAY\\tO\\n', 'CONTAIN\\tO\\n', 'INFORMATION\\tO\\n', 'THAT\\tO\\n', 'IS\\tO\\n', 'PRIVILEGED,\\tO\\n', '\\n', 'CONFIDENTIAL,\\tO\\n', 'AND\\tO\\n', 'EXEMPT\\tO\\n', 'FROM\\tO\\n', 'DISCLOSURE\\tO\\n', 'UNDER\\tO\\n', 'APPLICABLE\\tO\\n', 'LAW.\\tO\\n', 'If\\tO\\n', 'the\\tO\\n', '\\n', 'reader\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'message\\tO\\n', 'is\\tO\\n', 'not\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'employee\\tO\\n', 'or\\tO\\n', 'agent\\tO\\n', 'responsible\\tO\\n', 'for\\tO\\n', 'delivering\\tO\\n', '\\n', 'the\\tO\\n', 'message\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient,\\tO\\n', 'you\\tO\\n', 'are\\tO\\n', 'hereby\\tO\\n', 'notified\\tO\\n', 'that\\tO\\n', 'any\\tO\\n', 'dissemination,\\tO\\n', 'distribution,\\tO\\n', '\\n', 'copying,\\tO\\n', 'or\\tO\\n', 'conveying\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'communication\\tO\\n', 'in\\tO\\n', 'any\\tO\\n', 'manner\\tO\\n', 'Is\\tO\\n', 'strictly\\tO\\n', 'prohibited.\\tO\\n', 'If\\tO\\n', 'you\\tO\\n', 'have\\tO\\n', '\\n', 'received\\tO\\n', 'this\\tO\\n', 'communication\\tO\\n', 'in\\tO\\n', 'error,\\tO\\n', 'please\\tO\\n', 'notify\\tO\\n', 'us\\tO\\n', 'immediately\\tO\\n', 'by\\tO\\n', 'telephone\\tO\\n', 'and\\tO\\n', 'ream\\tO\\n', 'the\\tO\\n', '\\n', 'original\\tO\\n', 'message\\tO\\n', 'to\\tO\\n', 'us\\tO\\n', 'at\\tO\\n', 'the\\tO\\n', 'address\\tO\\n', 'below\\tO\\n', 'via\\tO\\n', 'the\\tO\\n', 'U.S.\\tO\\n', 'Postal\\tO\\n', 'Service.\\tO\\n', 'Thank\\tO\\n', 'you\\tO\\n', 'for\\tO\\n', 'your\\tO\\n', '\\n', 'cooperation.\\tO\\n', '\\n', '82092117\\tO\\n', '\\n', 'State\\tO\\n', 'Office\\tO\\n', 'Tower\\tO\\n', '/\\tO\\n', '30\\tO\\n', 'East\\tO\\n', 'Broad\\tO\\n', 'Street\\tO\\n', '/\\tO\\n', 'Columbus,\\tO\\n', 'Ohio\\tO\\n', '43215-3428\\tO\\n', '\\n', 'www.ag.state.oh.us\\tO\\n', '\\n', 'An\\tO\\n', 'Equal\\tO\\n', 'Opportunity\\tO\\n', 'Employer\\tO\\n', '\\n', '@Printed\\tO\\n', 'on\\tO\\n', 'Recycled\\tO\\n', 'Paper\\tO\\n', '\\n', 'ATT.\\tO\\n', 'GEN.\\tO\\n', 'ADMIN.\\tO\\n', 'OFFICE\\tO\\n', 'Fax:614-466-5087\\tO\\n', '\\n', 'Dec\\tO\\n', '10\\tO\\n', \"'98\\tO\\n\", '17:46\\tO\\n', '\\n', 'P.\\tO\\n', '01\\tO\\n', '\\n', 'Attorney\\tO\\n', 'General\\tO\\n', '\\n', 'Betty\\tO\\n', 'D.\\tO\\n', 'Montgomery\\tO\\n', '\\n', 'CONFIDENTIAL\\tB-HEADER\\n', 'FACSIMILE\\tE-HEADER\\n', '\\n', 'TRANSMISSION\\tB-HEADER\\n', 'COVER\\tI-HEADER\\n', 'SHEET\\tE-HEADER\\n', '\\n', 'FAX\\tO\\n', 'NO.\\tO\\n', '(614)\\tO\\n', '466-5087\\tO\\n', '\\n', 'TO:\\tS-QUESTION\\n', '\\n', 'George\\tB-ANSWER\\n', 'Baroody\\tE-ANSWER\\n', '\\n', 'FAX\\tB-ANSWER\\n', 'NUMBER:\\tI-ANSWER\\n', '(336)\\tI-ANSWER\\n', '335-7392\\tE-ANSWER\\n', '\\n', 'PHONE\\tB-QUESTION\\n', 'NUMBER:\\tE-QUESTION\\n', '\\n', '(336)\\tB-ANSWER\\n', '335-7363\\tE-ANSWER\\n', '\\n', 'DATE:\\tS-QUESTION\\n', '\\n', '12/10/98\\tS-ANSWER\\n', '\\n', 'NUMBER\\tB-QUESTION\\n', 'OF\\tI-QUESTION\\n', 'PAGES\\tI-QUESTION\\n', 'INCLUDING\\tI-QUESTION\\n', 'COVER\\tI-QUESTION\\n', 'SHEET:\\tE-QUESTION\\n', '\\n', '3\\tS-ANSWER\\n', '\\n', 'SENDER/PHONE\\tB-ANSWER\\n', 'NUMBER:\\tI-ANSWER\\n', 'June\\tI-ANSWER\\n', 'Flynn\\tI-ANSWER\\n', 'for\\tI-ANSWER\\n', 'Eric\\tI-ANSWER\\n', 'Brown/(614)\\tI-ANSWER\\n', '466-8980\\tE-ANSWER\\n', '\\n', 'SPECIAL\\tO\\n', 'INSTRUCTIONS:\\tO\\n', '\\n', 'IF\\tO\\n', 'YOU\\tO\\n', 'DO\\tO\\n', 'NOT\\tO\\n', 'RECEIVE\\tO\\n', 'ANY\\tO\\n', 'OF\\tO\\n', 'THE\\tO\\n', 'PAGES\\tO\\n', 'PROPERLY,\\tO\\n', '\\n', 'PLEASE\\tO\\n', 'CONTACT\\tO\\n', 'SENDER\\tO\\n', '\\n', 'AS\\tO\\n', 'SOON\\tO\\n', 'AS\\tO\\n', 'POSSIBLE\\tO\\n', '\\n', 'NOTE:\\tO\\n', '\\n', 'THIS\\tO\\n', 'MESSAGE\\tO\\n', 'IS\\tO\\n', 'INTENDED\\tO\\n', 'ONLY\\tO\\n', 'FOR\\tO\\n', 'THE\\tO\\n', 'USE\\tO\\n', 'OF\\tO\\n', 'THE\\tO\\n', 'INDIVIDUAL\\tO\\n', 'OR\\tO\\n', 'ENTITY\\tO\\n', 'TO\\tO\\n', '\\n', 'WHOM\\tO\\n', 'IT\\tO\\n', 'IS\\tO\\n', 'ADDRESSED\\tO\\n', 'AND\\tO\\n', 'MAY\\tO\\n', 'CONTAIN\\tO\\n', 'INFORMATION\\tO\\n', 'THAT\\tO\\n', 'IS\\tO\\n', 'PRIVILEGED,\\tO\\n', '\\n', 'CONFIDENTIAL,\\tO\\n', 'AND\\tO\\n', 'EXEMPT\\tO\\n', 'FROM\\tO\\n', 'DISCLOSURE\\tO\\n', 'UNDER\\tO\\n', 'APPLICABLE\\tO\\n', 'LAW.\\tO\\n', 'If\\tO\\n', 'the\\tO\\n', '\\n', 'reader\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'message\\tO\\n', 'is\\tO\\n', 'not\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'employee\\tO\\n', 'or\\tO\\n', 'agent\\tO\\n', 'responsible\\tO\\n', 'for\\tO\\n', 'delivering\\tO\\n', '\\n', 'the\\tO\\n', 'message\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient,\\tO\\n', 'you\\tO\\n', 'are\\tO\\n', 'hereby\\tO\\n', 'notified\\tO\\n', 'that\\tO\\n', 'any\\tO\\n', 'dissemination,\\tO\\n', 'distribution,\\tO\\n', '\\n', 'copying,\\tO\\n', 'or\\tO\\n', 'conveying\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'communication\\tO\\n', 'in\\tO\\n', 'any\\tO\\n', 'manner\\tO\\n', 'Is\\tO\\n', 'strictly\\tO\\n', 'prohibited.\\tO\\n', 'If\\tO\\n', 'you\\tO\\n', 'have\\tO\\n', '\\n', 'received\\tO\\n', 'this\\tO\\n', 'communication\\tO\\n', 'in\\tO\\n', 'error,\\tO\\n', 'please\\tO\\n', 'notify\\tO\\n', 'us\\tO\\n', 'immediately\\tO\\n', 'by\\tO\\n', 'telephone\\tO\\n', 'and\\tO\\n', 'ream\\tO\\n', 'the\\tO\\n', '\\n', 'original\\tO\\n', 'message\\tO\\n', 'to\\tO\\n', 'us\\tO\\n', 'at\\tO\\n', 'the\\tO\\n', 'address\\tO\\n', 'below\\tO\\n', 'via\\tO\\n', 'the\\tO\\n', 'U.S.\\tO\\n', 'Postal\\tO\\n', 'Service.\\tO\\n', 'Thank\\tO\\n', 'you\\tO\\n', 'for\\tO\\n', 'your\\tO\\n', '\\n', 'cooperation.\\tO\\n', '\\n', '82092117\\tO\\n', '\\n', 'State\\tO\\n', 'Office\\tO\\n', 'Tower\\tO\\n', '/\\tO\\n', '30\\tO\\n', 'East\\tO\\n', 'Broad\\tO\\n', 'Street\\tO\\n', '/\\tO\\n', 'Columbus,\\tO\\n', 'Ohio\\tO\\n', '43215-3428\\tO\\n', '\\n', 'www.ag.state.oh.us\\tO\\n', '\\n', 'An\\tO\\n', 'Equal\\tO\\n', 'Opportunity\\tO\\n', 'Employer\\tO\\n', '\\n', '@Printed\\tO\\n', 'on\\tO\\n', 'Recycled\\tO\\n', 'Paper\\tO\\n', '\\n', 'ATT.\\tO\\n', 'GEN.\\tO\\n', 'ADMIN.\\tO\\n', 'OFFICE\\tO\\n', 'Fax:614-466-5087\\tO\\n', '\\n', 'Dec\\tO\\n', '10\\tO\\n', \"'98\\tO\\n\", '17:46\\tO\\n', '\\n', 'P.\\tO\\n', '01\\tO\\n', '\\n', 'Attorney\\tO\\n', 'General\\tO\\n', '\\n', 'Betty\\tO\\n', 'D.\\tO\\n', 'Montgomery\\tO\\n', '\\n', 'CONFIDENTIAL\\tB-HEADER\\n', 'FACSIMILE\\tE-HEADER\\n', '\\n', 'TRANSMISSION\\tB-HEADER\\n', 'COVER\\tI-HEADER\\n', 'SHEET\\tE-HEADER\\n', '\\n', 'FAX\\tO\\n', 'NO.\\tO\\n', '(614)\\tO\\n', '466-5087\\tO\\n', '\\n', 'TO:\\tS-QUESTION\\n', '\\n', 'George\\tB-ANSWER\\n', 'Baroody\\tE-ANSWER\\n', '\\n', 'FAX\\tB-ANSWER\\n', 'NUMBER:\\tI-ANSWER\\n', '(336)\\tI-ANSWER\\n', '335-7392\\tE-ANSWER\\n', '\\n', 'PHONE\\tB-QUESTION\\n', 'NUMBER:\\tE-QUESTION\\n', '\\n', '(336)\\tB-ANSWER\\n', '335-7363\\tE-ANSWER\\n', '\\n', 'DATE:\\tS-QUESTION\\n', '\\n', '12/10/98\\tS-ANSWER\\n', '\\n', 'NUMBER\\tB-QUESTION\\n', 'OF\\tI-QUESTION\\n', 'PAGES\\tI-QUESTION\\n', 'INCLUDING\\tI-QUESTION\\n', 'COVER\\tI-QUESTION\\n', 'SHEET:\\tE-QUESTION\\n', '\\n', '3\\tS-ANSWER\\n', '\\n', 'SENDER/PHONE\\tB-ANSWER\\n', 'NUMBER:\\tI-ANSWER\\n', 'June\\tI-ANSWER\\n', 'Flynn\\tI-ANSWER\\n', 'for\\tI-ANSWER\\n', 'Eric\\tI-ANSWER\\n', 'Brown/(614)\\tI-ANSWER\\n', '466-8980\\tE-ANSWER\\n', '\\n', 'SPECIAL\\tO\\n', 'INSTRUCTIONS:\\tO\\n', '\\n', 'IF\\tO\\n', 'YOU\\tO\\n', 'DO\\tO\\n', 'NOT\\tO\\n', 'RECEIVE\\tO\\n', 'ANY\\tO\\n', 'OF\\tO\\n', 'THE\\tO\\n', 'PAGES\\tO\\n', 'PROPERLY,\\tO\\n', '\\n', 'PLEASE\\tO\\n', 'CONTACT\\tO\\n', 'SENDER\\tO\\n', '\\n', 'AS\\tO\\n', 'SOON\\tO\\n', 'AS\\tO\\n', 'POSSIBLE\\tO\\n', '\\n', 'NOTE:\\tO\\n', '\\n', 'THIS\\tO\\n', 'MESSAGE\\tO\\n', 'IS\\tO\\n', 'INTENDED\\tO\\n', 'ONLY\\tO\\n', 'FOR\\tO\\n', 'THE\\tO\\n', 'USE\\tO\\n', 'OF\\tO\\n', 'THE\\tO\\n', 'INDIVIDUAL\\tO\\n', 'OR\\tO\\n', 'ENTITY\\tO\\n', 'TO\\tO\\n', '\\n', 'WHOM\\tO\\n', 'IT\\tO\\n', 'IS\\tO\\n', 'ADDRESSED\\tO\\n', 'AND\\tO\\n', 'MAY\\tO\\n', 'CONTAIN\\tO\\n', 'INFORMATION\\tO\\n', 'THAT\\tO\\n', 'IS\\tO\\n', 'PRIVILEGED,\\tO\\n', '\\n', 'CONFIDENTIAL,\\tO\\n', 'AND\\tO\\n', 'EXEMPT\\tO\\n', 'FROM\\tO\\n', 'DISCLOSURE\\tO\\n', 'UNDER\\tO\\n', 'APPLICABLE\\tO\\n', 'LAW.\\tO\\n', 'If\\tO\\n', 'the\\tO\\n', '\\n', 'reader\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'message\\tO\\n', 'is\\tO\\n', 'not\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'employee\\tO\\n', 'or\\tO\\n', 'agent\\tO\\n', 'responsible\\tO\\n', 'for\\tO\\n', 'delivering\\tO\\n', '\\n', 'the\\tO\\n', 'message\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient,\\tO\\n', 'you\\tO\\n', 'are\\tO\\n', 'hereby\\tO\\n', 'notified\\tO\\n', 'that\\tO\\n', 'any\\tO\\n', 'dissemination,\\tO\\n', 'distribution,\\tO\\n', '\\n', 'copying,\\tO\\n', 'or\\tO\\n', 'conveying\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'communication\\tO\\n', 'in\\tO\\n', 'any\\tO\\n', 'manner\\tO\\n', 'Is\\tO\\n', 'strictly\\tO\\n', 'prohibited.\\tO\\n', 'If\\tO\\n', 'you\\tO\\n', 'have\\tO\\n', '\\n', 'received\\tO\\n', 'this\\tO\\n', 'communication\\tO\\n', 'in\\tO\\n', 'error,\\tO\\n', 'please\\tO\\n', 'notify\\tO\\n', 'us\\tO\\n', 'immediately\\tO\\n', 'by\\tO\\n', 'telephone\\tO\\n', 'and\\tO\\n', 'ream\\tO\\n', 'the\\tO\\n', '\\n', 'original\\tO\\n', 'message\\tO\\n', 'to\\tO\\n', 'us\\tO\\n', 'at\\tO\\n', 'the\\tO\\n', 'address\\tO\\n', 'below\\tO\\n', 'via\\tO\\n', 'the\\tO\\n', 'U.S.\\tO\\n', 'Postal\\tO\\n', 'Service.\\tO\\n', 'Thank\\tO\\n', 'you\\tO\\n', 'for\\tO\\n', 'your\\tO\\n', '\\n', 'cooperation.\\tO\\n', '\\n', '82092117\\tO\\n', '\\n', 'State\\tO\\n', 'Office\\tO\\n', 'Tower\\tO\\n', '/\\tO\\n', '30\\tO\\n', 'East\\tO\\n', 'Broad\\tO\\n', 'Street\\tO\\n', '/\\tO\\n', 'Columbus,\\tO\\n', 'Ohio\\tO\\n', '43215-3428\\tO\\n', '\\n', 'www.ag.state.oh.us\\tO\\n', '\\n', 'An\\tO\\n', 'Equal\\tO\\n', 'Opportunity\\tO\\n', 'Employer\\tO\\n', '\\n', '@Printed\\tO\\n', 'on\\tO\\n', 'Recycled\\tO\\n', 'Paper\\tO\\n', '\\n', 'LORILLARD\\tO\\n', 'RGN#11\\tO\\n', '\\n', '5001/004\\tO\\n', '\\n', '*\\tO\\n', '$05/01/00\\tO\\n', '\\n', '10:21\\tO\\n', '\\n', '27612\\tO\\n', '894\\tO\\n', '9890\\tO\\n', '\\n', 'Lorillard\\tO\\n', '\\n', 'Fax\\tS-HEADER\\n', '\\n', 'To:\\tS-QUESTION\\n', '\\n', 'Mr.\\tB-ANSWER\\n', 'Randy\\tI-ANSWER\\n', 'Spell\\tE-ANSWER\\n', '\\n', 'From:\\tB-ANSWER\\n', 'Rick\\tI-ANSWER\\n', 'Redfield\\tE-ANSWER\\n', '\\n', 'Fax:\\tO\\n', '\\n', 'Date:\\tS-QUESTION\\n', '\\n', 'May\\tB-ANSWER\\n', '1,\\tI-ANSWER\\n', '2000\\tE-ANSWER\\n', '\\n', 'CC:\\tS-QUESTION\\n', '\\n', 'Mr.\\tB-ANSWER\\n', 'Fred\\tI-ANSWER\\n', 'Paternostro\\tE-ANSWER\\n', '\\n', 'Fax:\\tB-ANSWER\\n', '952\\tI-ANSWER\\n', '894-9690\\tE-ANSWER\\n', '\\n', 'Re:\\tS-QUESTION\\n', '\\n', 'Legal\\tO\\n', '-\\tO\\n', 'lowa\\tO\\n', '\\n', 'Pages:\\tO\\n', '\\n', '[]Urgent\\tO\\n', 'For\\tO\\n', 'Review\\tO\\n', 'Please\\tO\\n', 'Comment\\tO\\n', 'Please\\tO\\n', 'Reply\\tO\\n', '[Please\\tO\\n', 'Recycle\\tO\\n', '\\n', 'Comments:\\tO\\n', '\\n', 'Attached\\tO\\n', 'is\\tO\\n', 'the\\tO\\n', 'lowa\\tO\\n', 'Department\\tO\\n', 'of\\tO\\n', 'Revenue\\tO\\n', 'and\\tO\\n', 'Finance\\tO\\n', 'Minimum\\tO\\n', 'Legal\\tO\\n', 'Prices\\tO\\n', '\\n', 'on\\tO\\n', 'Cigarettes\\tO\\n', 'as\\tO\\n', 'of\\tO\\n', 'April\\tO\\n', '3,\\tO\\n', '2000.\\tO\\n', 'I\\tO\\n', 'am\\tO\\n', 'attempting\\tO\\n', 'to\\tO\\n', 'obtain\\tO\\n', 'a\\tO\\n', 'copy\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'actual\\tO\\n', '\\n', 'Fair\\tO\\n', 'Trade\\tO\\n', 'Law\\tO\\n', 'and\\tO\\n', 'will\\tO\\n', 'forward\\tO\\n', 'as\\tO\\n', 'soon\\tO\\n', 'as\\tO\\n', 'obtained.\\tO\\n', '\\n', 'Rick\\tO\\n', '\\n', 'CONFIDENTIALITY\\tO\\n', 'NOTE\\tO\\n', '\\n', 'This\\tO\\n', 'facsimile\\tO\\n', 'may\\tO\\n', 'contain\\tO\\n', 'privileged\\tO\\n', 'and\\tO\\n', 'confidential\\tO\\n', 'information\\tO\\n', 'intended\\tO\\n', 'only\\tO\\n', 'for\\tO\\n', 'the\\tO\\n', 'use\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'individual\\tO\\n', 'or\\tO\\n', 'entity\\tO\\n', 'named\\tO\\n', '\\n', 'above.\\tO\\n', 'If\\tO\\n', 'the\\tO\\n', 'reader\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'facsimile\\tO\\n', 'is\\tO\\n', 'not\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient\\tO\\n', 'or\\tO\\n', 'employee\\tO\\n', 'or\\tO\\n', 'agent\\tO\\n', 'responsible\\tO\\n', 'for\\tO\\n', 'delivering\\tO\\n', 'it\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', '\\n', 'intended\\tO\\n', 'recipient,\\tO\\n', 'you\\tO\\n', 'are\\tO\\n', 'hereby\\tO\\n', 'notified\\tO\\n', 'that\\tO\\n', 'any\\tO\\n', 'dissemination\\tO\\n', 'or\\tO\\n', 'copying\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'facsimile\\tO\\n', 'is\\tO\\n', 'strictly\\tO\\n', 'prohibited.\\tO\\n', 'If\\tO\\n', 'you\\tO\\n', 'have\\tO\\n', '\\n', 'received\\tO\\n', 'this\\tO\\n', 'facsimile\\tO\\n', 'in\\tO\\n', 'error,\\tO\\n', 'please\\tO\\n', 'nollly\\tO\\n', 'us\\tO\\n', 'immediately\\tO\\n', 'by\\tO\\n', 'tolephone\\tO\\n', '(number\\tO\\n', 'indicated\\tO\\n', 'below)\\tO\\n', 'and\\tO\\n', 'return\\tO\\n', 'the\\tO\\n', 'original\\tO\\n', '\\n', '82562350\\tO\\n', '\\n', 'facsimile\\tO\\n', 'to\\tO\\n', 'us\\tO\\n', 'at\\tO\\n', 'the\\tO\\n', 'address\\tO\\n', 'below\\tO\\n', 'via\\tO\\n', 'the\\tO\\n', 'U.\\tO\\n', 'S.\\tO\\n', 'Postal\\tO\\n', 'Service.\\tO\\n', '\\n', '205\\tO\\n', 'River\\tO\\n', 'Ridge\\tO\\n', 'Circle\\tO\\n', '\\n', 'P.O.\\tO\\n', 'Box\\tO\\n', '1589\\tO\\n', '\\n', 'Bumsville,\\tO\\n', 'MN\\tO\\n', '55337\\tO\\n', '\\n', 'Telephone:\\tO\\n', '(952)\\tO\\n', '894-2238\\tO\\n', '\\n', 'LORILLARD\\tO\\n', 'RGN#11\\tO\\n', '\\n', '5001/004\\tO\\n', '\\n', '*\\tO\\n', '$05/01/00\\tO\\n', '\\n', '10:21\\tO\\n', '\\n', '27612\\tO\\n', '894\\tO\\n', '9890\\tO\\n', '\\n', 'Lorillard\\tO\\n', '\\n', 'Fax\\tS-HEADER\\n', '\\n', 'To:\\tS-QUESTION\\n', '\\n', 'Mr.\\tB-ANSWER\\n', 'Randy\\tI-ANSWER\\n', 'Spell\\tE-ANSWER\\n', '\\n', 'From:\\tB-ANSWER\\n', 'Rick\\tI-ANSWER\\n', 'Redfield\\tE-ANSWER\\n', '\\n', 'Fax:\\tO\\n', '\\n', 'Date:\\tS-QUESTION\\n', '\\n', 'May\\tB-ANSWER\\n', '1,\\tI-ANSWER\\n', '2000\\tE-ANSWER\\n', '\\n', 'CC:\\tS-QUESTION\\n', '\\n', 'Mr.\\tB-ANSWER\\n', 'Fred\\tI-ANSWER\\n', 'Paternostro\\tE-ANSWER\\n', '\\n', 'Fax:\\tB-ANSWER\\n', '952\\tI-ANSWER\\n', '894-9690\\tE-ANSWER\\n', '\\n', 'Re:\\tS-QUESTION\\n', '\\n', 'Legal\\tO\\n', '-\\tO\\n', 'lowa\\tO\\n', '\\n', 'Pages:\\tO\\n', '\\n', '[]Urgent\\tO\\n', 'For\\tO\\n', 'Review\\tO\\n', 'Please\\tO\\n', 'Comment\\tO\\n', 'Please\\tO\\n', 'Reply\\tO\\n', '[Please\\tO\\n', 'Recycle\\tO\\n', '\\n', 'Comments:\\tO\\n', '\\n', 'Attached\\tO\\n', 'is\\tO\\n', 'the\\tO\\n', 'lowa\\tO\\n', 'Department\\tO\\n', 'of\\tO\\n', 'Revenue\\tO\\n', 'and\\tO\\n', 'Finance\\tO\\n', 'Minimum\\tO\\n', 'Legal\\tO\\n', 'Prices\\tO\\n', '\\n', 'on\\tO\\n', 'Cigarettes\\tO\\n', 'as\\tO\\n', 'of\\tO\\n', 'April\\tO\\n', '3,\\tO\\n', '2000.\\tO\\n', 'I\\tO\\n', 'am\\tO\\n', 'attempting\\tO\\n', 'to\\tO\\n', 'obtain\\tO\\n', 'a\\tO\\n', 'copy\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'actual\\tO\\n', '\\n', 'Fair\\tO\\n', 'Trade\\tO\\n', 'Law\\tO\\n', 'and\\tO\\n', 'will\\tO\\n', 'forward\\tO\\n', 'as\\tO\\n', 'soon\\tO\\n', 'as\\tO\\n', 'obtained.\\tO\\n', '\\n', 'Rick\\tO\\n', '\\n', 'CONFIDENTIALITY\\tO\\n', 'NOTE\\tO\\n', '\\n', 'This\\tO\\n', 'facsimile\\tO\\n', 'may\\tO\\n', 'contain\\tO\\n', 'privileged\\tO\\n', 'and\\tO\\n', 'confidential\\tO\\n', 'information\\tO\\n', 'intended\\tO\\n', 'only\\tO\\n', 'for\\tO\\n', 'the\\tO\\n', 'use\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'individual\\tO\\n', 'or\\tO\\n', 'entity\\tO\\n', 'named\\tO\\n', '\\n', 'above.\\tO\\n', 'If\\tO\\n', 'the\\tO\\n', 'reader\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'facsimile\\tO\\n', 'is\\tO\\n', 'not\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient\\tO\\n', 'or\\tO\\n', 'employee\\tO\\n', 'or\\tO\\n', 'agent\\tO\\n', 'responsible\\tO\\n', 'for\\tO\\n', 'delivering\\tO\\n', 'it\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', '\\n', 'intended\\tO\\n', 'recipient,\\tO\\n', 'you\\tO\\n', 'are\\tO\\n', 'hereby\\tO\\n', 'notified\\tO\\n', 'that\\tO\\n', 'any\\tO\\n', 'dissemination\\tO\\n', 'or\\tO\\n', 'copying\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'facsimile\\tO\\n', 'is\\tO\\n', 'strictly\\tO\\n', 'prohibited.\\tO\\n', 'If\\tO\\n', 'you\\tO\\n', 'have\\tO\\n', '\\n', 'received\\tO\\n', 'this\\tO\\n', 'facsimile\\tO\\n', 'in\\tO\\n', 'error,\\tO\\n', 'please\\tO\\n', 'nollly\\tO\\n', 'us\\tO\\n', 'immediately\\tO\\n', 'by\\tO\\n', 'tolephone\\tO\\n', '(number\\tO\\n', 'indicated\\tO\\n', 'below)\\tO\\n', 'and\\tO\\n', 'return\\tO\\n', 'the\\tO\\n', 'original\\tO\\n', '\\n', '82562350\\tO\\n', '\\n', 'facsimile\\tO\\n', 'to\\tO\\n', 'us\\tO\\n', 'at\\tO\\n', 'the\\tO\\n', 'address\\tO\\n', 'below\\tO\\n', 'via\\tO\\n', 'the\\tO\\n', 'U.\\tO\\n', 'S.\\tO\\n', 'Postal\\tO\\n', 'Service.\\tO\\n', '\\n', '205\\tO\\n', 'River\\tO\\n', 'Ridge\\tO\\n', 'Circle\\tO\\n', '\\n', 'P.O.\\tO\\n', 'Box\\tO\\n', '1589\\tO\\n', '\\n', 'Bumsville,\\tO\\n', 'MN\\tO\\n', '55337\\tO\\n', '\\n', 'Telephone:\\tO\\n', '(952)\\tO\\n', '894-2238\\tO\\n', '\\n', 'LORILLARD\\tO\\n', 'RGN#11\\tO\\n', '\\n', '5001/004\\tO\\n', '\\n', '*\\tO\\n', '$05/01/00\\tO\\n', '\\n', '10:21\\tO\\n', '\\n', '27612\\tO\\n', '894\\tO\\n', '9890\\tO\\n', '\\n', 'Lorillard\\tO\\n', '\\n', 'Fax\\tS-HEADER\\n', '\\n', 'To:\\tS-QUESTION\\n', '\\n', 'Mr.\\tB-ANSWER\\n', 'Randy\\tI-ANSWER\\n', 'Spell\\tE-ANSWER\\n', '\\n', 'From:\\tB-ANSWER\\n', 'Rick\\tI-ANSWER\\n', 'Redfield\\tE-ANSWER\\n', '\\n', 'Fax:\\tO\\n', '\\n', 'Date:\\tS-QUESTION\\n', '\\n', 'May\\tB-ANSWER\\n', '1,\\tI-ANSWER\\n', '2000\\tE-ANSWER\\n', '\\n', 'CC:\\tS-QUESTION\\n', '\\n', 'Mr.\\tB-ANSWER\\n', 'Fred\\tI-ANSWER\\n', 'Paternostro\\tE-ANSWER\\n', '\\n', 'Fax:\\tB-ANSWER\\n', '952\\tI-ANSWER\\n', '894-9690\\tE-ANSWER\\n', '\\n', 'Re:\\tS-QUESTION\\n', '\\n', 'Legal\\tO\\n', '-\\tO\\n', 'lowa\\tO\\n', '\\n', 'Pages:\\tO\\n', '\\n', '[]Urgent\\tO\\n', 'For\\tO\\n', 'Review\\tO\\n', 'Please\\tO\\n', 'Comment\\tO\\n', 'Please\\tO\\n', 'Reply\\tO\\n', '[Please\\tO\\n', 'Recycle\\tO\\n', '\\n', 'Comments:\\tO\\n', '\\n', 'Attached\\tO\\n', 'is\\tO\\n', 'the\\tO\\n', 'lowa\\tO\\n', 'Department\\tO\\n', 'of\\tO\\n', 'Revenue\\tO\\n', 'and\\tO\\n', 'Finance\\tO\\n', 'Minimum\\tO\\n', 'Legal\\tO\\n', 'Prices\\tO\\n', '\\n', 'on\\tO\\n', 'Cigarettes\\tO\\n', 'as\\tO\\n', 'of\\tO\\n', 'April\\tO\\n', '3,\\tO\\n', '2000.\\tO\\n', 'I\\tO\\n', 'am\\tO\\n', 'attempting\\tO\\n', 'to\\tO\\n', 'obtain\\tO\\n', 'a\\tO\\n', 'copy\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'actual\\tO\\n', '\\n', 'Fair\\tO\\n', 'Trade\\tO\\n', 'Law\\tO\\n', 'and\\tO\\n', 'will\\tO\\n', 'forward\\tO\\n', 'as\\tO\\n', 'soon\\tO\\n', 'as\\tO\\n', 'obtained.\\tO\\n', '\\n', 'Rick\\tO\\n', '\\n', 'CONFIDENTIALITY\\tO\\n', 'NOTE\\tO\\n', '\\n', 'This\\tO\\n', 'facsimile\\tO\\n', 'may\\tO\\n', 'contain\\tO\\n', 'privileged\\tO\\n', 'and\\tO\\n', 'confidential\\tO\\n', 'information\\tO\\n', 'intended\\tO\\n', 'only\\tO\\n', 'for\\tO\\n', 'the\\tO\\n', 'use\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'individual\\tO\\n', 'or\\tO\\n', 'entity\\tO\\n', 'named\\tO\\n', '\\n', 'above.\\tO\\n', 'If\\tO\\n', 'the\\tO\\n', 'reader\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'facsimile\\tO\\n', 'is\\tO\\n', 'not\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient\\tO\\n', 'or\\tO\\n', 'employee\\tO\\n', 'or\\tO\\n', 'agent\\tO\\n', 'responsible\\tO\\n', 'for\\tO\\n', 'delivering\\tO\\n', 'it\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', '\\n', 'intended\\tO\\n', 'recipient,\\tO\\n', 'you\\tO\\n', 'are\\tO\\n', 'hereby\\tO\\n', 'notified\\tO\\n', 'that\\tO\\n', 'any\\tO\\n', 'dissemination\\tO\\n', 'or\\tO\\n', 'copying\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'facsimile\\tO\\n', 'is\\tO\\n', 'strictly\\tO\\n', 'prohibited.\\tO\\n', 'If\\tO\\n', 'you\\tO\\n', 'have\\tO\\n', '\\n', 'received\\tO\\n', 'this\\tO\\n', 'facsimile\\tO\\n', 'in\\tO\\n', 'error,\\tO\\n', 'please\\tO\\n', 'nollly\\tO\\n', 'us\\tO\\n', 'immediately\\tO\\n', 'by\\tO\\n', 'tolephone\\tO\\n', '(number\\tO\\n', 'indicated\\tO\\n', 'below)\\tO\\n', 'and\\tO\\n', 'return\\tO\\n', 'the\\tO\\n', 'original\\tO\\n', '\\n', '82562350\\tO\\n', '\\n', 'facsimile\\tO\\n', 'to\\tO\\n', 'us\\tO\\n', 'at\\tO\\n', 'the\\tO\\n', 'address\\tO\\n', 'below\\tO\\n', 'via\\tO\\n', 'the\\tO\\n', 'U.\\tO\\n', 'S.\\tO\\n', 'Postal\\tO\\n', 'Service.\\tO\\n', '\\n', '205\\tO\\n', 'River\\tO\\n', 'Ridge\\tO\\n', 'Circle\\tO\\n', '\\n', 'P.O.\\tO\\n', 'Box\\tO\\n', '1589\\tO\\n', '\\n', 'Bumsville,\\tO\\n', 'MN\\tO\\n', '55337\\tO\\n', '\\n', 'Telephone:\\tO\\n', '(952)\\tO\\n', '894-2238\\tO\\n', '\\n', 'LORILLARD\\tO\\n', 'RGN#11\\tO\\n', '\\n', '5001/004\\tO\\n', '\\n', '*\\tO\\n', '$05/01/00\\tO\\n', '\\n', '10:21\\tO\\n', '\\n', '27612\\tO\\n', '894\\tO\\n', '9890\\tO\\n', '\\n', 'Lorillard\\tO\\n', '\\n', 'Fax\\tS-HEADER\\n', '\\n', 'To:\\tS-QUESTION\\n', '\\n', 'Mr.\\tB-ANSWER\\n', 'Randy\\tI-ANSWER\\n', 'Spell\\tE-ANSWER\\n', '\\n', 'From:\\tB-ANSWER\\n', 'Rick\\tI-ANSWER\\n', 'Redfield\\tE-ANSWER\\n', '\\n', 'Fax:\\tO\\n', '\\n', 'Date:\\tS-QUESTION\\n', '\\n', 'May\\tB-ANSWER\\n', '1,\\tI-ANSWER\\n', '2000\\tE-ANSWER\\n', '\\n', 'CC:\\tS-QUESTION\\n', '\\n', 'Mr.\\tB-ANSWER\\n', 'Fred\\tI-ANSWER\\n', 'Paternostro\\tE-ANSWER\\n', '\\n', 'Fax:\\tB-ANSWER\\n', '952\\tI-ANSWER\\n', '894-9690\\tE-ANSWER\\n', '\\n', 'Re:\\tS-QUESTION\\n', '\\n', 'Legal\\tO\\n', '-\\tO\\n', 'lowa\\tO\\n', '\\n', 'Pages:\\tO\\n', '\\n', '[]Urgent\\tO\\n', 'For\\tO\\n', 'Review\\tO\\n', 'Please\\tO\\n', 'Comment\\tO\\n', 'Please\\tO\\n', 'Reply\\tO\\n', '[Please\\tO\\n', 'Recycle\\tO\\n', '\\n', 'Comments:\\tO\\n', '\\n', 'Attached\\tO\\n', 'is\\tO\\n', 'the\\tO\\n', 'lowa\\tO\\n', 'Department\\tO\\n', 'of\\tO\\n', 'Revenue\\tO\\n', 'and\\tO\\n', 'Finance\\tO\\n', 'Minimum\\tO\\n', 'Legal\\tO\\n', 'Prices\\tO\\n', '\\n', 'on\\tO\\n', 'Cigarettes\\tO\\n', 'as\\tO\\n', 'of\\tO\\n', 'April\\tO\\n', '3,\\tO\\n', '2000.\\tO\\n', 'I\\tO\\n', 'am\\tO\\n', 'attempting\\tO\\n', 'to\\tO\\n', 'obtain\\tO\\n', 'a\\tO\\n', 'copy\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'actual\\tO\\n', '\\n', 'Fair\\tO\\n', 'Trade\\tO\\n', 'Law\\tO\\n', 'and\\tO\\n', 'will\\tO\\n', 'forward\\tO\\n', 'as\\tO\\n', 'soon\\tO\\n', 'as\\tO\\n', 'obtained.\\tO\\n', '\\n', 'Rick\\tO\\n', '\\n', 'CONFIDENTIALITY\\tO\\n', 'NOTE\\tO\\n', '\\n', 'This\\tO\\n', 'facsimile\\tO\\n', 'may\\tO\\n', 'contain\\tO\\n', 'privileged\\tO\\n', 'and\\tO\\n', 'confidential\\tO\\n', 'information\\tO\\n', 'intended\\tO\\n', 'only\\tO\\n', 'for\\tO\\n', 'the\\tO\\n', 'use\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'individual\\tO\\n', 'or\\tO\\n', 'entity\\tO\\n', 'named\\tO\\n', '\\n', 'above.\\tO\\n', 'If\\tO\\n', 'the\\tO\\n', 'reader\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'facsimile\\tO\\n', 'is\\tO\\n', 'not\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient\\tO\\n', 'or\\tO\\n', 'employee\\tO\\n', 'or\\tO\\n', 'agent\\tO\\n', 'responsible\\tO\\n', 'for\\tO\\n', 'delivering\\tO\\n', 'it\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', '\\n', 'intended\\tO\\n', 'recipient,\\tO\\n', 'you\\tO\\n', 'are\\tO\\n', 'hereby\\tO\\n', 'notified\\tO\\n', 'that\\tO\\n', 'any\\tO\\n', 'dissemination\\tO\\n', 'or\\tO\\n', 'copying\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'facsimile\\tO\\n', 'is\\tO\\n', 'strictly\\tO\\n', 'prohibited.\\tO\\n', 'If\\tO\\n', 'you\\tO\\n', 'have\\tO\\n', '\\n', 'received\\tO\\n', 'this\\tO\\n', 'facsimile\\tO\\n', 'in\\tO\\n', 'error,\\tO\\n', 'please\\tO\\n', 'nollly\\tO\\n', 'us\\tO\\n', 'immediately\\tO\\n', 'by\\tO\\n', 'tolephone\\tO\\n', '(number\\tO\\n', 'indicated\\tO\\n', 'below)\\tO\\n', 'and\\tO\\n', 'return\\tO\\n', 'the\\tO\\n', 'original\\tO\\n', '\\n', '82562350\\tO\\n', '\\n', 'facsimile\\tO\\n', 'to\\tO\\n', 'us\\tO\\n', 'at\\tO\\n', 'the\\tO\\n', 'address\\tO\\n', 'below\\tO\\n', 'via\\tO\\n', 'the\\tO\\n', 'U.\\tO\\n', 'S.\\tO\\n', 'Postal\\tO\\n', 'Service.\\tO\\n', '\\n', '205\\tO\\n', 'River\\tO\\n', 'Ridge\\tO\\n', 'Circle\\tO\\n', '\\n', 'P.O.\\tO\\n', 'Box\\tO\\n', '1589\\tO\\n', '\\n', 'Bumsville,\\tO\\n', 'MN\\tO\\n', '55337\\tO\\n', '\\n', 'Telephone:\\tO\\n', '(952)\\tO\\n', '894-2238\\tO\\n', '\\n', 'LORILLARD\\tO\\n', 'RGN#11\\tO\\n', '\\n', '5001/004\\tO\\n', '\\n', '*\\tO\\n', '$05/01/00\\tO\\n', '\\n', '10:21\\tO\\n', '\\n', '27612\\tO\\n', '894\\tO\\n', '9890\\tO\\n', '\\n', 'Lorillard\\tO\\n', '\\n', 'Fax\\tS-HEADER\\n', '\\n', 'To:\\tS-QUESTION\\n', '\\n', 'Mr.\\tB-ANSWER\\n', 'Randy\\tI-ANSWER\\n', 'Spell\\tE-ANSWER\\n', '\\n', 'From:\\tB-ANSWER\\n', 'Rick\\tI-ANSWER\\n', 'Redfield\\tE-ANSWER\\n', '\\n', 'Fax:\\tO\\n', '\\n', 'Date:\\tS-QUESTION\\n', '\\n', 'May\\tB-ANSWER\\n', '1,\\tI-ANSWER\\n', '2000\\tE-ANSWER\\n', '\\n', 'CC:\\tS-QUESTION\\n', '\\n', 'Mr.\\tB-ANSWER\\n', 'Fred\\tI-ANSWER\\n', 'Paternostro\\tE-ANSWER\\n', '\\n', 'Fax:\\tB-ANSWER\\n', '952\\tI-ANSWER\\n', '894-9690\\tE-ANSWER\\n', '\\n', 'Re:\\tS-QUESTION\\n', '\\n', 'Legal\\tO\\n', '-\\tO\\n', 'lowa\\tO\\n', '\\n', 'Pages:\\tO\\n', '\\n', '[]Urgent\\tO\\n', 'For\\tO\\n', 'Review\\tO\\n', 'Please\\tO\\n', 'Comment\\tO\\n', 'Please\\tO\\n', 'Reply\\tO\\n', '[Please\\tO\\n', 'Recycle\\tO\\n', '\\n', 'Comments:\\tO\\n', '\\n', 'Attached\\tO\\n', 'is\\tO\\n', 'the\\tO\\n', 'lowa\\tO\\n', 'Department\\tO\\n', 'of\\tO\\n', 'Revenue\\tO\\n', 'and\\tO\\n', 'Finance\\tO\\n', 'Minimum\\tO\\n', 'Legal\\tO\\n', 'Prices\\tO\\n', '\\n', 'on\\tO\\n', 'Cigarettes\\tO\\n', 'as\\tO\\n', 'of\\tO\\n', 'April\\tO\\n', '3,\\tO\\n', '2000.\\tO\\n', 'I\\tO\\n', 'am\\tO\\n', 'attempting\\tO\\n', 'to\\tO\\n', 'obtain\\tO\\n', 'a\\tO\\n', 'copy\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'actual\\tO\\n', '\\n', 'Fair\\tO\\n', 'Trade\\tO\\n', 'Law\\tO\\n', 'and\\tO\\n', 'will\\tO\\n', 'forward\\tO\\n', 'as\\tO\\n', 'soon\\tO\\n', 'as\\tO\\n', 'obtained.\\tO\\n', '\\n', 'Rick\\tO\\n', '\\n', 'CONFIDENTIALITY\\tO\\n', 'NOTE\\tO\\n', '\\n', 'This\\tO\\n', 'facsimile\\tO\\n', 'may\\tO\\n', 'contain\\tO\\n', 'privileged\\tO\\n', 'and\\tO\\n', 'confidential\\tO\\n', 'information\\tO\\n', 'intended\\tO\\n', 'only\\tO\\n', 'for\\tO\\n', 'the\\tO\\n', 'use\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'individual\\tO\\n', 'or\\tO\\n', 'entity\\tO\\n', 'named\\tO\\n', '\\n', 'above.\\tO\\n', 'If\\tO\\n', 'the\\tO\\n', 'reader\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'facsimile\\tO\\n', 'is\\tO\\n', 'not\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient\\tO\\n', 'or\\tO\\n', 'employee\\tO\\n', 'or\\tO\\n', 'agent\\tO\\n', 'responsible\\tO\\n', 'for\\tO\\n', 'delivering\\tO\\n', 'it\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', '\\n', 'intended\\tO\\n', 'recipient,\\tO\\n', 'you\\tO\\n', 'are\\tO\\n', 'hereby\\tO\\n', 'notified\\tO\\n', 'that\\tO\\n', 'any\\tO\\n', 'dissemination\\tO\\n', 'or\\tO\\n', 'copying\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'facsimile\\tO\\n', 'is\\tO\\n', 'strictly\\tO\\n', 'prohibited.\\tO\\n', 'If\\tO\\n', 'you\\tO\\n', 'have\\tO\\n', '\\n', 'received\\tO\\n', 'this\\tO\\n', 'facsimile\\tO\\n', 'in\\tO\\n', 'error,\\tO\\n', 'please\\tO\\n', 'nollly\\tO\\n', 'us\\tO\\n', 'immediately\\tO\\n', 'by\\tO\\n', 'tolephone\\tO\\n', '(number\\tO\\n', 'indicated\\tO\\n', 'below)\\tO\\n', 'and\\tO\\n', 'return\\tO\\n', 'the\\tO\\n', 'original\\tO\\n', '\\n', '82562350\\tO\\n', '\\n', 'facsimile\\tO\\n', 'to\\tO\\n', 'us\\tO\\n', 'at\\tO\\n', 'the\\tO\\n', 'address\\tO\\n', 'below\\tO\\n', 'via\\tO\\n', 'the\\tO\\n', 'U.\\tO\\n', 'S.\\tO\\n', 'Postal\\tO\\n', 'Service.\\tO\\n', '\\n', '205\\tO\\n', 'River\\tO\\n', 'Ridge\\tO\\n', 'Circle\\tO\\n', '\\n', 'P.O.\\tO\\n', 'Box\\tO\\n', '1589\\tO\\n', '\\n', 'Bumsville,\\tO\\n', 'MN\\tO\\n', '55337\\tO\\n', '\\n', 'Telephone:\\tO\\n', '(952)\\tO\\n', '894-2238\\tO\\n', '\\n', 'LORILLARD\\tO\\n', 'RGN#11\\tO\\n', '\\n', '5001/004\\tO\\n', '\\n', '*\\tO\\n', '$05/01/00\\tO\\n', '\\n', '10:21\\tO\\n', '\\n', '27612\\tO\\n', '894\\tO\\n', '9890\\tO\\n', '\\n', 'Lorillard\\tO\\n', '\\n', 'Fax\\tS-HEADER\\n', '\\n', 'To:\\tS-QUESTION\\n', '\\n', 'Mr.\\tB-ANSWER\\n', 'Randy\\tI-ANSWER\\n', 'Spell\\tE-ANSWER\\n', '\\n', 'From:\\tB-ANSWER\\n', 'Rick\\tI-ANSWER\\n', 'Redfield\\tE-ANSWER\\n', '\\n', 'Fax:\\tO\\n', '\\n', 'Date:\\tS-QUESTION\\n', '\\n', 'May\\tB-ANSWER\\n', '1,\\tI-ANSWER\\n', '2000\\tE-ANSWER\\n', '\\n', 'CC:\\tS-QUESTION\\n', '\\n', 'Mr.\\tB-ANSWER\\n', 'Fred\\tI-ANSWER\\n', 'Paternostro\\tE-ANSWER\\n', '\\n', 'Fax:\\tB-ANSWER\\n', '952\\tI-ANSWER\\n', '894-9690\\tE-ANSWER\\n', '\\n', 'Re:\\tS-QUESTION\\n', '\\n', 'Legal\\tO\\n', '-\\tO\\n', 'lowa\\tO\\n', '\\n', 'Pages:\\tO\\n', '\\n', '[]Urgent\\tO\\n', 'For\\tO\\n', 'Review\\tO\\n', 'Please\\tO\\n', 'Comment\\tO\\n', 'Please\\tO\\n', 'Reply\\tO\\n', '[Please\\tO\\n', 'Recycle\\tO\\n', '\\n', 'Comments:\\tO\\n', '\\n', 'Attached\\tO\\n', 'is\\tO\\n', 'the\\tO\\n', 'lowa\\tO\\n', 'Department\\tO\\n', 'of\\tO\\n', 'Revenue\\tO\\n', 'and\\tO\\n', 'Finance\\tO\\n', 'Minimum\\tO\\n', 'Legal\\tO\\n', 'Prices\\tO\\n', '\\n', 'on\\tO\\n', 'Cigarettes\\tO\\n', 'as\\tO\\n', 'of\\tO\\n', 'April\\tO\\n', '3,\\tO\\n', '2000.\\tO\\n', 'I\\tO\\n', 'am\\tO\\n', 'attempting\\tO\\n', 'to\\tO\\n', 'obtain\\tO\\n', 'a\\tO\\n', 'copy\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'actual\\tO\\n', '\\n', 'Fair\\tO\\n', 'Trade\\tO\\n', 'Law\\tO\\n', 'and\\tO\\n', 'will\\tO\\n', 'forward\\tO\\n', 'as\\tO\\n', 'soon\\tO\\n', 'as\\tO\\n', 'obtained.\\tO\\n', '\\n', 'Rick\\tO\\n', '\\n', 'CONFIDENTIALITY\\tO\\n', 'NOTE\\tO\\n', '\\n', 'This\\tO\\n', 'facsimile\\tO\\n', 'may\\tO\\n', 'contain\\tO\\n', 'privileged\\tO\\n', 'and\\tO\\n', 'confidential\\tO\\n', 'information\\tO\\n', 'intended\\tO\\n', 'only\\tO\\n', 'for\\tO\\n', 'the\\tO\\n', 'use\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'individual\\tO\\n', 'or\\tO\\n', 'entity\\tO\\n', 'named\\tO\\n', '\\n', 'above.\\tO\\n', 'If\\tO\\n', 'the\\tO\\n', 'reader\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'facsimile\\tO\\n', 'is\\tO\\n', 'not\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient\\tO\\n', 'or\\tO\\n', 'employee\\tO\\n', 'or\\tO\\n', 'agent\\tO\\n', 'responsible\\tO\\n', 'for\\tO\\n', 'delivering\\tO\\n', 'it\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', '\\n', 'intended\\tO\\n', 'recipient,\\tO\\n', 'you\\tO\\n', 'are\\tO\\n', 'hereby\\tO\\n', 'notified\\tO\\n', 'that\\tO\\n', 'any\\tO\\n', 'dissemination\\tO\\n', 'or\\tO\\n', 'copying\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'facsimile\\tO\\n', 'is\\tO\\n', 'strictly\\tO\\n', 'prohibited.\\tO\\n', 'If\\tO\\n', 'you\\tO\\n', 'have\\tO\\n', '\\n', 'received\\tO\\n', 'this\\tO\\n', 'facsimile\\tO\\n', 'in\\tO\\n', 'error,\\tO\\n', 'please\\tO\\n', 'nollly\\tO\\n', 'us\\tO\\n', 'immediately\\tO\\n', 'by\\tO\\n', 'tolephone\\tO\\n', '(number\\tO\\n', 'indicated\\tO\\n', 'below)\\tO\\n', 'and\\tO\\n', 'return\\tO\\n', 'the\\tO\\n', 'original\\tO\\n', '\\n', '82562350\\tO\\n', '\\n', 'facsimile\\tO\\n', 'to\\tO\\n', 'us\\tO\\n', 'at\\tO\\n', 'the\\tO\\n', 'address\\tO\\n', 'below\\tO\\n', 'via\\tO\\n', 'the\\tO\\n', 'U.\\tO\\n', 'S.\\tO\\n', 'Postal\\tO\\n', 'Service.\\tO\\n', '\\n', '205\\tO\\n', 'River\\tO\\n', 'Ridge\\tO\\n', 'Circle\\tO\\n', '\\n', 'P.O.\\tO\\n', 'Box\\tO\\n', '1589\\tO\\n', '\\n', 'Bumsville,\\tO\\n', 'MN\\tO\\n', '55337\\tO\\n', '\\n', 'Telephone:\\tO\\n', '(952)\\tO\\n', '894-2238\\tO\\n', '\\n', 'LORILLARD\\tO\\n', 'RGN#11\\tO\\n', '\\n', '5001/004\\tO\\n', '\\n', '*\\tO\\n', '$05/01/00\\tO\\n', '\\n', '10:21\\tO\\n', '\\n', '27612\\tO\\n', '894\\tO\\n', '9890\\tO\\n', '\\n', 'Lorillard\\tO\\n', '\\n', 'Fax\\tS-HEADER\\n', '\\n', 'To:\\tS-QUESTION\\n', '\\n', 'Mr.\\tB-ANSWER\\n', 'Randy\\tI-ANSWER\\n', 'Spell\\tE-ANSWER\\n', '\\n', 'From:\\tB-ANSWER\\n', 'Rick\\tI-ANSWER\\n', 'Redfield\\tE-ANSWER\\n', '\\n', 'Fax:\\tO\\n', '\\n', 'Date:\\tS-QUESTION\\n', '\\n', 'May\\tB-ANSWER\\n', '1,\\tI-ANSWER\\n', '2000\\tE-ANSWER\\n', '\\n', 'CC:\\tS-QUESTION\\n', '\\n', 'Mr.\\tB-ANSWER\\n', 'Fred\\tI-ANSWER\\n', 'Paternostro\\tE-ANSWER\\n', '\\n', 'Fax:\\tB-ANSWER\\n', '952\\tI-ANSWER\\n', '894-9690\\tE-ANSWER\\n', '\\n', 'Re:\\tS-QUESTION\\n', '\\n', 'Legal\\tO\\n', '-\\tO\\n', 'lowa\\tO\\n', '\\n', 'Pages:\\tO\\n', '\\n', '[]Urgent\\tO\\n', 'For\\tO\\n', 'Review\\tO\\n', 'Please\\tO\\n', 'Comment\\tO\\n', 'Please\\tO\\n', 'Reply\\tO\\n', '[Please\\tO\\n', 'Recycle\\tO\\n', '\\n', 'Comments:\\tO\\n', '\\n', 'Attached\\tO\\n', 'is\\tO\\n', 'the\\tO\\n', 'lowa\\tO\\n', 'Department\\tO\\n', 'of\\tO\\n', 'Revenue\\tO\\n', 'and\\tO\\n', 'Finance\\tO\\n', 'Minimum\\tO\\n', 'Legal\\tO\\n', 'Prices\\tO\\n', '\\n', 'on\\tO\\n', 'Cigarettes\\tO\\n', 'as\\tO\\n', 'of\\tO\\n', 'April\\tO\\n', '3,\\tO\\n', '2000.\\tO\\n', 'I\\tO\\n', 'am\\tO\\n', 'attempting\\tO\\n', 'to\\tO\\n', 'obtain\\tO\\n', 'a\\tO\\n', 'copy\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'actual\\tO\\n', '\\n', 'Fair\\tO\\n', 'Trade\\tO\\n', 'Law\\tO\\n', 'and\\tO\\n', 'will\\tO\\n', 'forward\\tO\\n', 'as\\tO\\n', 'soon\\tO\\n', 'as\\tO\\n', 'obtained.\\tO\\n', '\\n', 'Rick\\tO\\n', '\\n', 'CONFIDENTIALITY\\tO\\n', 'NOTE\\tO\\n', '\\n', 'This\\tO\\n', 'facsimile\\tO\\n', 'may\\tO\\n', 'contain\\tO\\n', 'privileged\\tO\\n', 'and\\tO\\n', 'confidential\\tO\\n', 'information\\tO\\n', 'intended\\tO\\n', 'only\\tO\\n', 'for\\tO\\n', 'the\\tO\\n', 'use\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'individual\\tO\\n', 'or\\tO\\n', 'entity\\tO\\n', 'named\\tO\\n', '\\n', 'above.\\tO\\n', 'If\\tO\\n', 'the\\tO\\n', 'reader\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'facsimile\\tO\\n', 'is\\tO\\n', 'not\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient\\tO\\n', 'or\\tO\\n', 'employee\\tO\\n', 'or\\tO\\n', 'agent\\tO\\n', 'responsible\\tO\\n', 'for\\tO\\n', 'delivering\\tO\\n', 'it\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', '\\n', 'intended\\tO\\n', 'recipient,\\tO\\n', 'you\\tO\\n', 'are\\tO\\n', 'hereby\\tO\\n', 'notified\\tO\\n', 'that\\tO\\n', 'any\\tO\\n', 'dissemination\\tO\\n', 'or\\tO\\n', 'copying\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'facsimile\\tO\\n', 'is\\tO\\n', 'strictly\\tO\\n', 'prohibited.\\tO\\n', 'If\\tO\\n', 'you\\tO\\n', 'have\\tO\\n', '\\n', 'received\\tO\\n', 'this\\tO\\n', 'facsimile\\tO\\n', 'in\\tO\\n', 'error,\\tO\\n', 'please\\tO\\n', 'nollly\\tO\\n', 'us\\tO\\n', 'immediately\\tO\\n', 'by\\tO\\n', 'tolephone\\tO\\n', '(number\\tO\\n', 'indicated\\tO\\n', 'below)\\tO\\n', 'and\\tO\\n', 'return\\tO\\n', 'the\\tO\\n', 'original\\tO\\n', '\\n', '82562350\\tO\\n', '\\n', 'facsimile\\tO\\n', 'to\\tO\\n', 'us\\tO\\n', 'at\\tO\\n', 'the\\tO\\n', 'address\\tO\\n', 'below\\tO\\n', 'via\\tO\\n', 'the\\tO\\n', 'U.\\tO\\n', 'S.\\tO\\n', 'Postal\\tO\\n', 'Service.\\tO\\n', '\\n', '205\\tO\\n', 'River\\tO\\n', 'Ridge\\tO\\n', 'Circle\\tO\\n', '\\n', 'P.O.\\tO\\n', 'Box\\tO\\n', '1589\\tO\\n', '\\n', 'Bumsville,\\tO\\n', 'MN\\tO\\n', '55337\\tO\\n', '\\n', 'Telephone:\\tO\\n', '(952)\\tO\\n', '894-2238\\tO\\n', '\\n', 'LORILLARD\\tO\\n', 'RGN#11\\tO\\n', '\\n', '5001/004\\tO\\n', '\\n', '*\\tO\\n', '$05/01/00\\tO\\n', '\\n', '10:21\\tO\\n', '\\n', '27612\\tO\\n', '894\\tO\\n', '9890\\tO\\n', '\\n', 'Lorillard\\tO\\n', '\\n', 'Fax\\tS-HEADER\\n', '\\n', 'To:\\tS-QUESTION\\n', '\\n', 'Mr.\\tB-ANSWER\\n', 'Randy\\tI-ANSWER\\n', 'Spell\\tE-ANSWER\\n', '\\n', 'From:\\tB-ANSWER\\n', 'Rick\\tI-ANSWER\\n', 'Redfield\\tE-ANSWER\\n', '\\n', 'Fax:\\tO\\n', '\\n', 'Date:\\tS-QUESTION\\n', '\\n', 'May\\tB-ANSWER\\n', '1,\\tI-ANSWER\\n', '2000\\tE-ANSWER\\n', '\\n', 'CC:\\tS-QUESTION\\n', '\\n', 'Mr.\\tB-ANSWER\\n', 'Fred\\tI-ANSWER\\n', 'Paternostro\\tE-ANSWER\\n', '\\n', 'Fax:\\tB-ANSWER\\n', '952\\tI-ANSWER\\n', '894-9690\\tE-ANSWER\\n', '\\n', 'Re:\\tS-QUESTION\\n', '\\n', 'Legal\\tO\\n', '-\\tO\\n', 'lowa\\tO\\n', '\\n', 'Pages:\\tO\\n', '\\n', '[]Urgent\\tO\\n', 'For\\tO\\n', 'Review\\tO\\n', 'Please\\tO\\n', 'Comment\\tO\\n', 'Please\\tO\\n', 'Reply\\tO\\n', '[Please\\tO\\n', 'Recycle\\tO\\n', '\\n', 'Comments:\\tO\\n', '\\n', 'Attached\\tO\\n', 'is\\tO\\n', 'the\\tO\\n', 'lowa\\tO\\n', 'Department\\tO\\n', 'of\\tO\\n', 'Revenue\\tO\\n', 'and\\tO\\n', 'Finance\\tO\\n', 'Minimum\\tO\\n', 'Legal\\tO\\n', 'Prices\\tO\\n', '\\n', 'on\\tO\\n', 'Cigarettes\\tO\\n', 'as\\tO\\n', 'of\\tO\\n', 'April\\tO\\n', '3,\\tO\\n', '2000.\\tO\\n', 'I\\tO\\n', 'am\\tO\\n', 'attempting\\tO\\n', 'to\\tO\\n', 'obtain\\tO\\n', 'a\\tO\\n', 'copy\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'actual\\tO\\n', '\\n', 'Fair\\tO\\n', 'Trade\\tO\\n', 'Law\\tO\\n', 'and\\tO\\n', 'will\\tO\\n', 'forward\\tO\\n', 'as\\tO\\n', 'soon\\tO\\n', 'as\\tO\\n', 'obtained.\\tO\\n', '\\n', 'Rick\\tO\\n', '\\n', 'CONFIDENTIALITY\\tO\\n', 'NOTE\\tO\\n', '\\n', 'This\\tO\\n', 'facsimile\\tO\\n', 'may\\tO\\n', 'contain\\tO\\n', 'privileged\\tO\\n', 'and\\tO\\n', 'confidential\\tO\\n', 'information\\tO\\n', 'intended\\tO\\n', 'only\\tO\\n', 'for\\tO\\n', 'the\\tO\\n', 'use\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'individual\\tO\\n', 'or\\tO\\n', 'entity\\tO\\n', 'named\\tO\\n', '\\n', 'above.\\tO\\n', 'If\\tO\\n', 'the\\tO\\n', 'reader\\tO\\n', 'of\\tO\\n', 'the\\tO\\n', 'facsimile\\tO\\n', 'is\\tO\\n', 'not\\tO\\n', 'the\\tO\\n', 'intended\\tO\\n', 'recipient\\tO\\n', 'or\\tO\\n', 'employee\\tO\\n', 'or\\tO\\n', 'agent\\tO\\n', 'responsible\\tO\\n', 'for\\tO\\n', 'delivering\\tO\\n', 'it\\tO\\n', 'to\\tO\\n', 'the\\tO\\n', '\\n', 'intended\\tO\\n', 'recipient,\\tO\\n', 'you\\tO\\n', 'are\\tO\\n', 'hereby\\tO\\n', 'notified\\tO\\n', 'that\\tO\\n', 'any\\tO\\n', 'dissemination\\tO\\n', 'or\\tO\\n', 'copying\\tO\\n', 'of\\tO\\n', 'this\\tO\\n', 'facsimile\\tO\\n', 'is\\tO\\n', 'strictly\\tO\\n', 'prohibited.\\tO\\n', 'If\\tO\\n', 'you\\tO\\n', 'have\\tO\\n', '\\n', 'received\\tO\\n', 'this\\tO\\n', 'facsimile\\tO\\n', 'in\\tO\\n', 'error,\\tO\\n', 'please\\tO\\n', 'nollly\\tO\\n', 'us\\tO\\n', 'immediately\\tO\\n', 'by\\tO\\n', 'tolephone\\tO\\n', '(number\\tO\\n', 'indicated\\tO\\n', 'below)\\tO\\n', 'and\\tO\\n', 'return\\tO\\n', 'the\\tO\\n', 'original\\tO\\n', '\\n', '82562350\\tO\\n', '\\n', 'facsimile\\tO\\n', 'to\\tO\\n', 'us\\tO\\n', 'at\\tO\\n', 'the\\tO\\n', 'address\\tO\\n', 'below\\tO\\n', 'via\\tO\\n', 'the\\tO\\n', 'U.\\tO\\n', 'S.\\tO\\n', 'Postal\\tO\\n', 'Service.\\tO\\n', '\\n', '205\\tO\\n', 'River\\tO\\n', 'Ridge\\tO\\n', 'Circle\\tO\\n', '\\n', 'P.O.\\tO\\n', 'Box\\tO\\n', '1589\\tO\\n', '\\n', 'Bumsville,\\tO\\n', 'MN\\tO\\n', '55337\\tO\\n', '\\n', 'Telephone:\\tO\\n', '(952)\\tO\\n', '894-2238\\tO\\n', '\\n']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"87fS2fXJV1cn","colab_type":"code","colab":{}},"source":["from re import search\n","import pandas as pd\n","\n","\n","question_table = []\n","answer_table = []\n","\n","labels = []\n","words = []\n","#extracting question and answer labels and their relative words\n","for line1 in text:\n","  if(len(line1.split(\"\\t\"))==2):\n","    if((line1.split(\"\\t\")[1][0] != \"O\")):\n","      if((search(\"HEADER\", line1.split(\"\\t\")[1]) == None)):\n","        #label\n","        #print(line1.split(\"\\t\")[1])\n","        labels.append(line1.split(\"\\t\")[1])\n","        #text\n","        #print(line1.split(\"\\t\")[0])\n","        words.append(line1.split(\"\\t\")[0])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SIrzZ1uPV2sV","colab_type":"code","colab":{}},"source":["from re import search\n","import pandas as pd\n","\n","\n","question_table = []\n","answer_table = []\n","\n","labels = []\n","words = []\n","#extracting question and answer labels and their relative words\n","for line1 in text:\n","  if(len(line1.split(\"\\t\"))==2):\n","    if((line1.split(\"\\t\")[1][0] != \"O\")):\n","      if((search(\"HEADER\", line1.split(\"\\t\")[1]) == None)):\n","        #label\n","        #print(line1.split(\"\\t\")[1])\n","        labels.append(line1.split(\"\\t\")[1])\n","        #text\n","        #print(line1.split(\"\\t\")[0])\n","        words.append(line1.split(\"\\t\")[0])\n","\n","#creating table which contains questions and answers\n","current_index = 0\n","num_labels = 0\n","while num_labels < len(labels):\n","  #for question label\n","  if search(\"QUESTION\", labels[num_labels][2:-1]) != None:\n","    #checking with side labels in the list.\n","    q_else_check = False\n","    inner_num_labels = num_labels\n","    question_string = str(words[num_labels])\n","    while inner_num_labels < len(labels)-1:\n","      if labels[inner_num_labels+1][2:-1] == labels[num_labels][2:-1]:\n","        current_index = inner_num_labels+1\n","        #assigning question\n","        question_string = question_string+\" \"+str(words[inner_num_labels+1])\n","        inner_num_labels+=1\n","      else:\n","        q_else_check = True\n","        current_index+=1\n","        break\n","    #appending the complete question into the original list\n","    question_table.append(question_string)\n","    #updating the original index\n","    if q_else_check == True:\n","      num_labels = current_index\n","    else:\n","      num_labels = current_index+1\n","  \n","  elif search(\"ANSWER\", labels[num_labels][2:-1]) != None:\n","    q_else_check = False\n","    inner_num_labels = num_labels\n","    answer_string = str(words[num_labels])\n","    while inner_num_labels < len(labels)-1:\n","      if labels[inner_num_labels+1][2:-1] == labels[num_labels][2:-1]:\n","        current_index = inner_num_labels+1\n","        #assigning question\n","        answer_string = answer_string+\" \"+str(words[inner_num_labels+1])\n","        inner_num_labels+=1\n","      else:\n","        q_else_check = True\n","        current_index+=1\n","        break\n","    #appending the complete question into the original list\n","    answer_table.append(answer_string)\n","    #updating the original index\n","    if q_else_check == True:\n","      num_labels = current_index\n","    else:\n","      num_labels = current_index+1\n","  \n","  else:\n","    print(\"something wrong\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C93LDkYzV4I0","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597834246891,"user_tz":-330,"elapsed":2788,"user":{"displayName":"Mohit Tuli","photoUrl":"","userId":"05250823286326681737"}},"outputId":"3350338d-eb5e-4452-dd51-29aa16f011e8"},"source":["len(question_table[:-1])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["92"]},"metadata":{"tags":[]},"execution_count":50}]},{"cell_type":"code","metadata":{"id":"Afm0D6OrV697","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1597834251097,"user_tz":-330,"elapsed":1918,"user":{"displayName":"Mohit Tuli","photoUrl":"","userId":"05250823286326681737"}},"outputId":"2e5d5741-fb47-4a05-812e-1650b8afd3ee"},"source":["len(answer_table)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["92"]},"metadata":{"tags":[]},"execution_count":51}]},{"cell_type":"code","metadata":{"id":"GQIUz2qdV8M-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1597834257123,"user_tz":-330,"elapsed":1783,"user":{"displayName":"Mohit Tuli","photoUrl":"","userId":"05250823286326681737"}},"outputId":"ab310f38-8d5e-4c12-8f08-19708653c3d8"},"source":["pd.DataFrame({\"Question\":question_table[:-1],\"Answers\":answer_table}).head(5)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Question</th>\n","      <th>Answers</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Registration No.</td>\n","      <td>533</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Date</td>\n","      <td>March 18, 1968</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>AUTHORS</td>\n","      <td>Andrew G. Kallianos, Richard E. Means, James D...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TITLE</td>\n","      <td>\"Effect of Nitrates in Tobacco on the Catechol...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Registration No.</td>\n","      <td>533</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["           Question                                            Answers\n","0  Registration No.                                                533\n","1              Date                                     March 18, 1968\n","2           AUTHORS  Andrew G. Kallianos, Richard E. Means, James D...\n","3             TITLE  \"Effect of Nitrates in Tobacco on the Catechol...\n","4  Registration No.                                                533"]},"metadata":{"tags":[]},"execution_count":52}]},{"cell_type":"code","metadata":{"id":"fY86mMvlV9tj","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}