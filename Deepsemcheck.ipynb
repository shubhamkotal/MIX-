{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Deepsemcheck.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNdEinxQcnNyfn9kZ3TCMoO"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rhOVTNnTYtDy","executionInfo":{"elapsed":31443,"status":"ok","timestamp":1620295048112,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"},"user_tz":-330},"outputId":"9504c56e-ca9f-4187-b7fc-4673cc331d3f"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')\n","%cd '/content/gdrive/MyDrive/Deeplobe GIT/deeplobe-ai-master'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/Deeplobe GIT/deeplobe-ai-master\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vwvpf3TsZrrn","executionInfo":{"elapsed":19623,"status":"ok","timestamp":1620295058429,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"},"user_tz":-330},"outputId":"203df201-8cda-4161-f81f-afdb251b3286"},"source":["!pip install pytorch-lightning"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting pytorch-lightning\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/07/0c/e2d52147ac12a77ee4e7fd7deb4b5f334cfb335af9133a0f2780c8bb9a2c/pytorch_lightning-1.2.10-py3-none-any.whl (841kB)\n","\u001b[K     |████████████████████████████████| 849kB 9.1MB/s \n","\u001b[?25hCollecting future>=0.17.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n","\u001b[K     |████████████████████████████████| 829kB 27.9MB/s \n","\u001b[?25hCollecting PyYAML!=5.4.*,>=5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n","\u001b[K     |████████████████████████████████| 276kB 54.5MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.41.1)\n","Collecting torchmetrics==0.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/42/d984612cabf005a265aa99c8d4ab2958e37b753aafb12f31c81df38751c8/torchmetrics-0.2.0-py3-none-any.whl (176kB)\n","\u001b[K     |████████████████████████████████| 184kB 53.5MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (20.9)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n","Requirement already satisfied: tensorboard!=2.5.0,>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.4.1)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.8.1+cu101)\n","Collecting fsspec[http]>=0.8.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n","\u001b[K     |████████████████████████████████| 112kB 52.7MB/s \n","\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->pytorch-lightning) (2.4.7)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.3.4)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.15.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.4)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (2.23.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (56.0.0)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.32.0)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.12.4)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.28.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.0.1)\n","Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.12.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.36.2)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.8.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch-lightning) (3.7.4.3)\n","Collecting aiohttp; extra == \"http\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 43.9MB/s \n","\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.10.1)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.3.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (2020.12.5)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.7.2)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (4.2.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.2.8)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (20.3.0)\n","Collecting yarl<2.0,>=1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n","\u001b[K     |████████████████████████████████| 296kB 57.7MB/s \n","\u001b[?25hCollecting async-timeout<4.0,>=3.0\n","  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n","Collecting multidict<7.0,>=4.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n","\u001b[K     |████████████████████████████████| 143kB 58.5MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.4.1)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (3.1.0)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard!=2.5.0,>=2.2.0->pytorch-lightning) (0.4.8)\n","Building wheels for collected packages: future, PyYAML\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=5ce0a704ba2e94aca978c62f6bebaf546ff6bbd0e03602162a08c00b0b8297d4\n","  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n","  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=16502032a96e402c6e6ccadc66557d8450ba982b5cb50c91a983980a50c6130f\n","  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n","Successfully built future PyYAML\n","Installing collected packages: future, PyYAML, torchmetrics, multidict, yarl, async-timeout, aiohttp, fsspec, pytorch-lightning\n","  Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-5.3.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-2021.4.0 future-0.18.2 multidict-5.1.0 pytorch-lightning-1.2.10 torchmetrics-0.2.0 yarl-1.6.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Otbr642LoiDY"},"source":["#documentation\n","from pathlib import Path\n","import random\n","from PIL import Image\n","import torch\n","import torch.nn.functional as F \n","import torch.nn as nn\n","# To get MNIST data and transforms \n","from torchvision import datasets, transforms \n","# To get the optimizer for our model \n","import cv2\n","from torch.utils.data import random_split, DataLoader \n","from torch.utils.data import DataLoader\n","from torch.utils.data import TensorDataset\n","from torchvision.models.segmentation.fcn import FCNHead\n","from torchvision import models\n","import torchvision.transforms as T\n","# Plot the input image, ground truth and the predicted output\n","from matplotlib import pyplot as plt\n","from PIL import *\n","import numpy as np\n","#!pip install pytorch-lightning\n","import pytorch_lightning as pl \n","from pytorch_lightning.callbacks import ModelCheckpoint\n","\n","def return_files(path):\n","  files = []\n","  extensions = [\"jpg\",'jpeg','png','PNG']\n","  for ext in extensions:\n","    files.extend(path.glob('*.'+ext))\n","  return files\n","def process_data(loc):\n","  # Data directory\n","  path = Path(loc)\n","  path_lbl = path/'Masks'\n","  #path_lbl.to_globa\n","  path_img = path/'Images'\n","  mask_definitions = path/'mask_definitions.json'\n","  img_files = return_files(path_img)\n","  train_imgs = [img for img in img_files]\n","  with open(mask_definitions) as md:\n","      mask_def = eval(md.read())\n","  mask_info = {}\n","  label_dict = {}\n","  for image in mask_def['masks']:\n","      mask=mask_def['masks'][image]['mask']\n","      mask = mask.lstrip('masks/')\n","      colors = mask_def['masks'][image]['color_categories']\n","      rgb = colors.keys()\n","      categories = [i['super_category'] for i in colors.values()]\n","      color_map = dict(zip(rgb,categories))\n","      mask_info.update({mask:color_map})\n","  for v in mask_info.values():\n","    label_dict.update(v.items())\n","  #create an rgb dict with colors as keys and labels as values\n","  label_dict['(0, 0, 0)'] = 'background'\n","  name2id = {v:k for k,v in enumerate(sorted(label_dict.values()))}\n","  class_dict = {eval(v):k for k,v in enumerate(sorted(label_dict.keys()))}\n","  return class_dict, train_imgs, path_lbl\n","class CustomDataset():\n","    def __init__(self,class_dict,path_lbl ,img_files, img_transform=None,mask_transform=None):\n","        self.class_dict=class_dict\n","        self.img_files = img_files\n","        self.img_transform = img_transform\n","        self.mask_transform = mask_transform\n","        self.path_lbl=path_lbl\n","    #helper function to convert rbg mask to label encoded masks\n","    def mask_to_class(self,mask):\n","        target = torch.from_numpy(mask)\n","        h,w = target.shape[0],target.shape[1]\n","        masks = torch.zeros(h, w, dtype=torch.long)\n","        colors = torch.unique(target.view(-1,target.size(2)),dim=0).numpy()\n","        target = target.permute(2, 0, 1).contiguous()\n","        mapping = self.class_dict #{tuple(c): t for c, t in zip(colors.tolist(), range(len(colors)))}\n","        for k in mapping:\n","            idx = (target==torch.tensor(k, dtype=torch.uint8).unsqueeze(1).unsqueeze(2))\n","            validx = (idx.sum(0) == 3) \n","            masks[validx] = torch.tensor(mapping[k], dtype=torch.long)\n","        return masks\n","    def __getitem__(self, idx):\n","        image_file = self.img_files[idx]\n","        mask_file = self.path_lbl/image_file.name      \n","        image = Image.open(image_file)\n","        mask = Image.open(mask_file)         \n","        #apply tranforms\n","        if self.img_transform:\n","          image = self.img_transform(image)\n","        if self.mask_transform:\n","          mask = self.mask_transform(mask)\n","        #encode RGB mask to class_labels\n","        mask = np.array(mask)\n","        mask = self.mask_to_class(mask)\n","        return image, mask\n","    def __len__(self):  # return count of sample we have\n","        return len(self.img_files)\n","def get_img_transform(train=True):\n","    transforms = []\n","    # converts the image, a PIL image, into a PyTorch Tensor\n","    transforms.append(T.Resize(256))\n","    transforms.append(T.CenterCrop(224))\n","    transforms.append(T.ToTensor())\n","    transforms.append(T.Normalize(mean=[0.485,0.456,0.406],\n","                                std=[0.229,0.224,0.225]))\n","    return T.Compose(transforms)\n","def get_mask_transform(train=True):\n","    transforms = []\n","    # apply same resize and cropping on masks\n","    transforms.append(T.Resize(256))\n","    transforms.append(T.CenterCrop(224))    \n","    #transforms.append(T.ToTensor())\n","    return T.Compose(transforms)\n","\n","def get_class(loc):\n","  path = Path(loc)\n","  mask_definitions = path/'mask_definitions.json'\n","  with open(mask_definitions) as md:\n","      mask_def = eval(md.read())\n","  mask_info = {}\n","  label_dict = {}\n","  for image in mask_def['masks']:\n","      mask=mask_def['masks'][image]['mask']\n","      mask = mask.lstrip('masks/')\n","      colors = mask_def['masks'][image]['color_categories']\n","      rgb = colors.keys()\n","      categories = [i['super_category'] for i in colors.values()]\n","      color_map = dict(zip(rgb,categories))\n","      mask_info.update({mask:color_map})\n","  for v in mask_info.values():\n","    label_dict.update(v.items())\n","  label_dict['(0, 0, 0)'] = 'background'\n","  name2id = {v:k for k,v in enumerate(sorted(label_dict.values()))}\n","  class_dict = {eval(v):k for k,v in enumerate(sorted(label_dict.keys()))}\n","  return class_dict\n","\n","class DataModulesemantic(pl.LightningDataModule):\n","  def __init__(self,class_dict,train_imgs,path_lbl):\n","    super(DataModulesemantic,self).__init__()\n","    self.class_dict=class_dict\n","    self.train_imgs=train_imgs\n","    self.path_lbl=path_lbl\n","    self.train_dataset = CustomDataset(self.class_dict,self.path_lbl,self.train_imgs,img_transform=get_img_transform(train=True),mask_transform=get_mask_transform(train=True))\n","    #self.valid_dataset = CustomDataset(valid_imgs,img_transform=get_img_transform(train=False),mask_transform=get_mask_transform(train=False))\n","    length=len(self.train_dataset)\n","    trl,vel,tel= int(length*0.8), int(length*0.1), int(length*0.1)\n","    self.train_data, self.val_data, self.test_data = random_split(self.train_dataset,[trl,vel,tel])\n","    self.bs = 1\n","  def train_dataloader(self):\n","    return DataLoader(self.train_data,batch_size=self.bs)\n","  def val_dataloader(self):\n","    return DataLoader(self.val_data,batch_size=self.bs)\n","  def test_dataloader(self):\n","    return DataLoader(self.test_data,batch_size=self.bs)  \n","\n","class FCNHead_model(pl.LightningModule):\n","  def __init__(self,path):\n","    super(FCNHead_model,self).__init__()\n","    classes=get_class(path)\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    model = models.segmentation.fcn_resnet101(pretrained=True)\n","    model.classifier = FCNHead(2048,len(classes))\n","    model.aux_classifier = FCNHead(1024,len(classes))\n","    model.to(device)\n","    self.model=model\n","    # self.model = build_model(model,'FCN',3)\n","    self.criterion = nn.CrossEntropyLoss()\n","    self.params = [p for p in self.model.parameters() if p.requires_grad]\n","    #self.optimizer = torch.optim.Adadelta(params, lr=1e-4)\n","\n","  def forward(self,x):\n","    output = self.model(x)\n","    return output\n","\n","  def configure_optimizers(self):\n","    return torch.optim.Adadelta(self.params, lr=1e-4)\n","  def training_step(self, train_batch, batch_idx):\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    x, y = train_batch\n","    x=x.to(device)\n","    y=y.to(device) \n","    logits = self.model(x) \n","    loss = self.criterion(logits['out'], y)\n","    self.log('Training Loss', loss, on_step=True, on_epoch=True, sync_dist=True) \n","    return loss \n","  def validation_step(self, valid_batch, batch_idx):\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    x, y = valid_batch \n","    x=x.to(device)\n","    y=y.to(device) \n","    logits = self.model(x) \n","    loss = self.criterion(logits['out'], y)\n","    self.log('Validation Loss', loss, on_step=True, on_epoch=True, sync_dist=True) \n","    return loss\n","  def test_step(self, test_batch, batch_idx):\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    x, y = test_batch \n","    x=x.to(device)\n","    y=y.to(device) \n","    logits = self.model(x) \n","    loss = self.criterion(logits['out'], y)\n","    self.log('Testing Loss', loss, on_step=True, on_epoch=True, sync_dist=True) \n","    return loss\n","\n","def train_model(epochs,path):\n","  max_epoc= 10 if epochs is None else epochs\n","  classes, images, path1 = process_data(path)\n","  data_module = DataModulesemantic(classes,images,path1) \n","  model_module = FCNHead_model(path)\n","  gpu=1 if torch.cuda.is_available() else 0\n","  checkpoint_callback = ModelCheckpoint(monitor = 'Validation Loss',dirpath = path)\n","  trainer = pl.Trainer(max_epochs=max_epoc,gpus=gpu,default_root_dir = path,callbacks = [checkpoint_callback])\n","  trainer.fit(model_module, data_module)\n","  trainer.test()\n","\n","from PIL import Image\n","from matplotlib import cm\n","\n","def predict_img(img,model_weight,annotation_file_path,output_file_name):\n","    model=FCNHead_model(annotation_file_path)\n","    model=model.load_from_checkpoint(model_weight,path=annotation_file_path)\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    img = Image.open(img).convert('RGB')\n","    transform = transforms.Compose([transforms.Resize(256),\n","                                    transforms.CenterCrop(224),\n","                                    transforms.ToTensor(),\n","                                    transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])])\n","    t_img = transform(img).unsqueeze(0)\n","    t_img=t_img.to(device)\n","    out = model(t_img)\n","    y_pred_final=out['out']\n","    pred = y_pred_final.cpu().detach().numpy()\n","    pred = np.argmax(pred[0], axis = 0)\n","    plt.imshow(pred)\n","    plt.show()\n","    filename = output_file_name + '.jpg'\n","    cv2.imwrite(filename, pred*255)\n","\n","\n","def video_segmentaton(in_path,model_weight,annotation_file_path,out_path):\n","  model=FCNHead_model(annotation_file_path)\n","  model=model.load_from_checkpoint(model_weight,path=annotation_file_path)\n","  video_capture = cv2.VideoCapture(in_path)\n","  palette = torch.tensor([2 ** 25 - 1, 2 ** 15 - 1, 2 ** 21 - 1])\n","  frame_width = int(video_capture.get(3))\n","  frame_height = int(video_capture.get(4))\n","  #print('width: ',frame_width, 'height: ',frame_height)\n","  colors = torch.as_tensor([i for i in range(classes)])[:, None] * palette\n","  colors = (colors % 255).numpy().astype(\"uint8\")\n","  colors = np.array(list(class_dict.keys()),dtype='uint8')\n","  # Define the codec and create VideoWriter object.The output is stored in 'output.avi' file.\n","  fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n","  out = cv2.VideoWriter(out_path,fourcc, 30, (120,120),True)\n","  model = model.to(device)\n","  a = 0\n","  while True:\n","      a = a+1\n","      #print('frame number', a)\n","      ret, frame = video_capture.read()\n","      if ret == False:\n","        break \n","      rgb_frame = cv2.cvtColor(np.array(frame), cv2.COLOR_BGR2RGB)\n","      img = Image.fromarray(rgb_frame)\n","      input_size = img.size\n","      img = T.Resize(128)(img)\n","      img = T.CenterCrop(120)(img)\n","      img = T.ToTensor()(img)\n","      img = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(img)    \n","      frames = torch.tensor(img,dtype=torch.float).cuda()\n","      output = model(frames.unsqueeze(0))['out'][0]\n","      output_predictions = output.argmax(0)\n","      r = Image.fromarray(output_predictions.byte().cpu().numpy())#.resize(input_size)\n","      r.putpalette(colors)\n","      out_frame = cv2.cvtColor(np.array(r.convert('RGB')),cv2.COLOR_RGB2BGR)\n","      out.write(out_frame)\n","  out.release()\n","  video_capture.release()\n","\n","class Semantic():\n","  def load_data(self,path):\n","    self.path=path\n","    self.classes,self.images,self.path1=process_data(self.path)\n","  def train(self,epochs):\n","    self.epochs=epochs\n","    self.model=train_model(self.epochs,self.path)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":551,"referenced_widgets":["59e26042a0d4485fb4d4842539d56367","0ffd499e64c143efbabb50db1c0c35f0","d10a96061f6a4093a8e67839f987e0d5","ba1f667ad583426bbb5da27aea959df2"]},"id":"ptzghqgOrsVJ","executionInfo":{"elapsed":417348,"status":"ok","timestamp":1620295483740,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"},"user_tz":-330},"outputId":"409e9136-352f-47cd-c204-824ca3db469c"},"source":["semseg_model = Semantic()\n","semseg_model.load_data(\"/content/gdrive/MyDrive/Deeplobe GIT/deeplobe-ai-master/Datasets/semantic\")\n","semseg_model.train(epochs = 1) # default epochs = 10 if not mentioned"],"execution_count":null,"outputs":[{"output_type":"stream","text":["GPU available: False, used: False\n","TPU available: False, using: 0 TPU cores\n","\n","  | Name      | Type             | Params\n","-----------------------------------------------\n","0 | model     | FCN              | 54.3 M\n","1 | criterion | CrossEntropyLoss | 0     \n","-----------------------------------------------\n","54.3 M    Trainable params\n","0         Non-trainable params\n","54.3 M    Total params\n","217.202   Total estimated model params size (MB)\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"59e26042a0d4485fb4d4842539d56367","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0ffd499e64c143efbabb50db1c0c35f0","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d10a96061f6a4093a8e67839f987e0d5","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validating', layout=Layout(flex='2'), m…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ba1f667ad583426bbb5da27aea959df2","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Testing', layout=Layout(flex='2'), max=…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{'Testing Loss': 2.2919704914093018, 'Testing Loss_epoch': 13.172649383544922}\n","--------------------------------------------------------------------------------\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"76RHwhM5Iy4K","executionInfo":{"elapsed":1894,"status":"ok","timestamp":1620295498937,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"},"user_tz":-330},"outputId":"e4e305ea-0df7-4f8b-e3dc-8260fe7b65ed"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mDatasets\u001b[0m/    \u001b[01;34mlightning_logs\u001b[0m/        output_file_name4.jpg\n","DATASETS.md  output_file_name1.jpg  output_file_name5.jpg\n","\u001b[01;34mdeeplobeai\u001b[0m/  output_file_name2.jpg  README.md\n","LICENSE      output_file_name3.png  setup.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fjX5I_lzGbAs","executionInfo":{"status":"ok","timestamp":1620298006402,"user_tz":-330,"elapsed":4435,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"1720dc47-0640-4497-8de7-81762390ee0d"},"source":["def predict_img(img,model_weight,annotation_file_path,output_file_name):\n","    model=FCNHead_model(annotation_file_path)\n","    model=model.load_from_checkpoint(model_weight,path=annotation_file_path)\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    img = Image.open(img).convert('RGB')\n","    transform = transforms.Compose([transforms.Resize(256),\n","                                    transforms.CenterCrop(224),\n","                                    transforms.ToTensor(),\n","                                    transforms.Normalize(mean=[0.485,0.456,0.406],std=[0.229,0.224,0.225])])\n","    t_img = transform(img).unsqueeze(0)\n","    t_img=t_img.to(device)\n","    out = model(t_img)\n","    print('keys',out.keys())\n","    y_pred_final=out['out']\n","    pred = y_pred_final.cpu().detach().numpy()\n","    print('pred',pred)\n","    pred = np.argmax(pred[0], axis = 0)\n","    print('pred',pred)\n","    # plt.imshow(pred)\n","    # plt.show()\n","    # filename = output_file_name + '.jpg'\n","    # cv2.imwrite(filename, pred*255)\n","predict_img('/content/gdrive/MyDrive/Deeplobe GIT/deeplobe-ai-master/Datasets/semantic/Images/00000000.png',\n","            '/content/gdrive/MyDrive/Deeplobe GIT/deeplobe-ai-master/Datasets/semantic/epoch=0-step=239.ckpt',\n","            '/content/gdrive/MyDrive/Deeplobe GIT/deeplobe-ai-master/Datasets/semantic',\n","            'output_file_name5')"],"execution_count":26,"outputs":[{"output_type":"stream","text":["keys odict_keys(['out', 'aux'])\n","pred [[[[ 1.75845265e-01  1.75845265e-01  1.75845265e-01 ... -6.27923369e-01\n","    -6.27923369e-01 -6.27923369e-01]\n","   [ 1.75845265e-01  1.75845265e-01  1.75845265e-01 ... -6.27923369e-01\n","    -6.27923369e-01 -6.27923369e-01]\n","   [ 1.75845265e-01  1.75845265e-01  1.75845265e-01 ... -6.27923369e-01\n","    -6.27923369e-01 -6.27923369e-01]\n","   ...\n","   [ 8.77804220e-01  8.77804220e-01  8.77804220e-01 ...  9.47703421e-02\n","     9.47703421e-02  9.47703421e-02]\n","   [ 8.77804220e-01  8.77804220e-01  8.77804220e-01 ...  9.47703421e-02\n","     9.47703421e-02  9.47703421e-02]\n","   [ 8.77804220e-01  8.77804220e-01  8.77804220e-01 ...  9.47703421e-02\n","     9.47703421e-02  9.47703421e-02]]\n","\n","  [[ 1.16258845e-01  1.16258845e-01  1.16258845e-01 ... -7.27648847e-04\n","    -7.27648847e-04 -7.27648847e-04]\n","   [ 1.16258845e-01  1.16258845e-01  1.16258845e-01 ... -7.27648847e-04\n","    -7.27648847e-04 -7.27648847e-04]\n","   [ 1.16258845e-01  1.16258845e-01  1.16258845e-01 ... -7.27648847e-04\n","    -7.27648847e-04 -7.27648847e-04]\n","   ...\n","   [-2.00531170e-01 -2.00531170e-01 -2.00531170e-01 ... -3.13363522e-01\n","    -3.13363522e-01 -3.13363522e-01]\n","   [-2.00531170e-01 -2.00531170e-01 -2.00531170e-01 ... -3.13363522e-01\n","    -3.13363522e-01 -3.13363522e-01]\n","   [-2.00531170e-01 -2.00531170e-01 -2.00531170e-01 ... -3.13363522e-01\n","    -3.13363522e-01 -3.13363522e-01]]\n","\n","  [[-6.91377819e-01 -6.91377819e-01 -6.91377819e-01 ...  2.47583702e-01\n","     2.47583702e-01  2.47583702e-01]\n","   [-6.91377819e-01 -6.91377819e-01 -6.91377819e-01 ...  2.47583702e-01\n","     2.47583702e-01  2.47583702e-01]\n","   [-6.91377819e-01 -6.91377819e-01 -6.91377819e-01 ...  2.47583702e-01\n","     2.47583702e-01  2.47583702e-01]\n","   ...\n","   [ 1.99087918e-01  1.99087918e-01  1.99087918e-01 ... -1.18667200e-01\n","    -1.18667200e-01 -1.18667200e-01]\n","   [ 1.99087918e-01  1.99087918e-01  1.99087918e-01 ... -1.18667200e-01\n","    -1.18667200e-01 -1.18667200e-01]\n","   [ 1.99087918e-01  1.99087918e-01  1.99087918e-01 ... -1.18667200e-01\n","    -1.18667200e-01 -1.18667200e-01]]]]\n","pred [[0 0 0 ... 2 2 2]\n"," [0 0 0 ... 2 2 2]\n"," [0 0 0 ... 2 2 2]\n"," ...\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]\n"," [0 0 0 ... 0 0 0]]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"yWj62-XZSW20"},"source":[""],"execution_count":null,"outputs":[]}]}