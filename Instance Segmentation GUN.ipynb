{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Instance Segmentation GUN.ipynb","provenance":[],"authorship_tag":"ABX9TyNOS6hurzdCe1T+7MXoYLD4"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"id":"7vor1Pe0MyaX","executionInfo":{"status":"error","timestamp":1620359815303,"user_tz":-330,"elapsed":829469,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"084f336d-fa3d-4a9a-b623-a041e91149ab"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')  #mounting"],"execution_count":1,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    728\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 729\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    730\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    802\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 803\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    804\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    582\u001b[0m         \"\"\"\n\u001b[0;32m--> 583\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv\u001b[0;34m()\u001b[0m\n","\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc\u001b[0;34m()\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: ","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-dcf1c35fd314>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#mounting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, use_metadata_server)\u001b[0m\n\u001b[1;32m    258\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dfs-auth-dance'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfifo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfifo_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m           \u001b[0mfifo_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_prompt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m       \u001b[0mwrote_to_fifo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 704\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    705\u001b[0m         )\n\u001b[1;32m    706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    732\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QM63wHyKNJQQ","executionInfo":{"status":"ok","timestamp":1613649619581,"user_tz":-330,"elapsed":2100,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"32b2f26d-1b40-48a4-a433-74fc4c905a57"},"source":["%cd '/content/gdrive/MyDrive/DEEPLOBE/segmentation'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/MyDrive/DEEPLOBE/segmentation\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HOo3Ys11Axxj","executionInfo":{"status":"ok","timestamp":1613649647179,"user_tz":-330,"elapsed":1213,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"271e71c9-b85d-4fa5-d00a-d4542226c029"},"source":["cd 'maskrcnn_pistol_data'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/DEEPLOBE/segmentation/maskrcnn_pistol_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fNLQ3YwGA5wL","executionInfo":{"status":"ok","timestamp":1613649667919,"user_tz":-330,"elapsed":1357,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"f723f8b5-d8dd-4dbc-e29f-6f824157ea3d"},"source":["ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["coco_instances.json  \u001b[0m\u001b[01;34mimages\u001b[0m/  \u001b[01;34mmasks\u001b[0m/  visualize.py\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Xvqw7zmbQJ7H"},"source":["#!unzip 'maskrcnn_pistol_data.zip'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":381},"id":"sPK5PoqAQPWp","executionInfo":{"status":"error","timestamp":1613649675976,"user_tz":-330,"elapsed":4965,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"56776ea7-a843-49e4-987c-ecd639e3366c"},"source":["import os\n","import random\n","import torch\n","import torchvision\n","from PIL import Image\n","from pycocotools.coco import COCO\n","from torchvision import transforms as T\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","from models.detection_pytorch_repo import utils\n","from models.detection_pytorch_repo import engine\n","\n","class UserDataset(torch.utils.data.Dataset):\n","    \"\"\"\n","    Custom class inheriting from Pytorch's Dataset utility class\n","    that allows applying custom transformations on user-datasets\n","    \n","    It returns transformed images and masks in an iterator object \n","    that can be indexed according to the batch sizes\n","    in the data loading phase for passing to model\n","    \"\"\"\n","\n","    \n","    def __init__(self, root, annotation, transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        self.coco = COCO(annotation)\n","        self.ids = list(sorted(self.coco.imgs.keys()))\n","\n","    def __getitem__(self, index):\n","        # coco file\n","        coco = self.coco\n","        # Image ID\n","        img_id = self.ids[index]\n","        # List: get annotation id from coco\n","        ann_ids = coco.getAnnIds(imgIds=img_id)\n","        # Dictionary: target coco_annotation file for an image\n","        coco_annotation = coco.loadAnns(ann_ids)\n","        # path for input image\n","        path = coco.loadImgs(img_id)[0]['file_name']\n","        # open the input image\n","        img = Image.open(os.path.join(self.root, 'images',path))\n","        \n","        ## mask ##\n","        mask_file_path = os.path.splitext(path)[0] + '.jpg'\n","        mask = Image.open(os.path.join(self.root,'masks',mask_file_path))\n","\n","        # number of objects in the image\n","        num_objs = len(coco_annotation)\n","\n","        # Bounding boxes for objects\n","        # In coco format, bbox = [xmin, ymin, width, height]\n","        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n","        boxes = []\n","        for i in range(num_objs):\n","            xmin = coco_annotation[i]['bbox'][0]\n","            ymin = coco_annotation[i]['bbox'][1]\n","            xmax = xmin + coco_annotation[i]['bbox'][2]\n","            ymax = ymin + coco_annotation[i]['bbox'][3]\n","            boxes.append([xmin, ymin, xmax, ymax])\n","            \n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","\n","        # Tensorise img_id\n","        img_id = torch.tensor([img_id])\n","        # Size of bbox (Rectangular)\n","        areas = []\n","        iscrowd = []\n","        labels = []\n","\n","        for i in range(num_objs):\n","            areas.append(coco_annotation[i]['area'])\n","            iscrowd.append(coco_annotation[i]['iscrowd'])\n","            labels.append(coco_annotation[i]['category_id'])\n","\n","\n","        areas,iscrowd,labels = map(torch.tensor, [areas,iscrowd,labels])\n","\n","        if self.transforms is not None:\n","            img = self.transforms(img)\n","            mask = self.transforms(mask)\n","        mask = mask.numpy().reshape(mask.shape,order='F')\n","        mask = torch.as_tensor(mask, dtype=torch.uint8)\n","\n","\n","        # Annotation is in dictionary format\n","        my_annotation = {}\n","        my_annotation[\"boxes\"] = boxes\n","        my_annotation[\"labels\"] = labels\n","        my_annotation[\"image_id\"] = img_id\n","        my_annotation[\"area\"] = areas\n","        my_annotation[\"iscrowd\"] = iscrowd\n","        my_annotation[\"masks\"] = mask\n","\n","        return img, my_annotation\n","\n","    def __len__(self):\n","        return len(self.ids)\n","\n","\n","class PreProcessing(object):\n","    \"\"\"\n","    For loading imagefiles, preprocessing like train-test split based on a specified \n","    validation set size.\n","    \n","    Passing data directory and transforms for creating a dataset iterator\n","    \n","    Creates a dataset instance from the UserDataset class by passing data directory path.\n","    The data directory should have the following structure:\n","                    data_dir\n","             ------Image\n","             ---------Image1\n","             ---------Image2\n","             ---------\n","             ---------\n","                   \n","             ---------ImageN\n","             ------Mask\n","             ---------Mask1\n","             ---------Mask2\n","             ---------\n","             ---------\n","             ---------MaskN\n","    This class also calls the CustomDataset class above for creating \n","    a dataset iterator and creates dataloaders using torch.utils.Dataloaders class\n","    \n","    \"\"\"\n","    def __init__(self,valid_pct=0.2,batch_size=4):\n","        self.data_dir = os.path.join(r'.\\Uploads',\n","                                     \"segmentation\",\n","                                     'instance_segmentation')\n","        self.coco_instances = os.path.join(r'.\\Uploads',\n","                                           \"segmentation\",\n","                                           'instance_segmentation',\n","                                           \"annotation\",\n","                                           \"coco_instances.json\")\n","        self.batch_size = batch_size\n","        self.valid_pct = valid_pct\n","        self.transform = T.Compose([T.ToTensor()])\n","\n","    def get_class_dict(self):\n","        #Category dictinoray for output tagging\n","        instances = COCO(self.coco_instances)\n","        categories = instances.loadCats(instances.getCatIds())\n","        class_dict = {d['id']:d['name'] for d in categories}\n","    \n","        # Number of categories and a background\n","        num_classes = 1 + len(instances.getCatIds())\n","        \n","        return num_classes,class_dict\n","\n","        \n","    def train_test_split(self):\n","        \"\"\"\n","        Creates a dataset instance from the UserDataset class. It requires the \n","        annotation JSON file in the coco format: http://cocodataset.org/#format-data\n","        \n","        This method carries out train-val split and also creates dataloaders\n","        required for model building\n","        \n","        \"\"\"\n","        # create own Dataset\n","        user_dataset = UserDataset(root=self.data_dir,\n","                                annotation=self.coco_instances,\n","                                transforms=self.transform)\n","        #train-val split\n","        samples = len(user_dataset)\n","        test_counts = int(samples*self.valid_pct)\n","        train_counts = samples-test_counts\n","        train_set,val_set = torch.utils.data.random_split(user_dataset, [train_counts, test_counts])\n","\n","        # Train DataLoader\n","        train_loader = torch.utils.data.DataLoader(train_set,\n","                                                  batch_size=self.batch_size,\n","                                                  shuffle=True,\n","                                                  collate_fn = utils.collate_fn)\n","        # Validation DataLoader\n","        val_loader = torch.utils.data.DataLoader(val_set,\n","                                                  batch_size=self.batch_size,\n","                                                  shuffle=False,\n","                                                  collate_fn = utils.collate_fn)\n","        return train_loader,val_loader\n","\n","\n","   \n","class InstanceSegment():\n","    \"\"\"\n","    creates a device object based on GPU availability\n","    downloads pretrained models from torchvision library\n","    changes the final layer based on the target class count\n","    trains the model and saves the trained model weights in the .\\weights folder\n","    arguments: valid_pct - percentage of images to go into the validation set\n","                batch_size - no. images in a batch for training\n","    \n","    \"\"\"\n","    def __init__(self,valid_pct=0.2,batch_size=4):\n","        self.valid_pct = valid_pct\n","        self.batch_size = batch_size\n","        self.preprocessing = PreProcessing(self.valid_pct,\n","                                           self.batch_size)\n","\n","        self.num_classes,self.class_dict = self.preprocessing.get_class_dict()\n","        \n","        # select device (whether GPU or CPU)\n","        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","\n","    def build(self):\n","        \n","        #download pretrained models from torchvision library    \n","        # load an instance segmentation model pre-trained pre-trained on COCO\n","        self.model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n","\n","        # get number of input features for the classifier\n","        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n","        # replace the pre-trained head with a new one\n","        self.model.roi_heads.box_predictor = FastRCNNPredictor(in_features, self.num_classes)\n","        # now get the number of input features for the mask classifier\n","        in_features_mask = self.model.roi_heads.mask_predictor.conv5_mask.in_channels\n","        hidden_layer = 256\n","        # and replace the mask predictor with a new one\n","        self.model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,hidden_layer,self.num_classes)\n","\n","    def train_model(self,num_epochs=20,demo = False):\n","        \"\"\"trains and validates the model on the train images for the given no. of epochs\n","            arguments: num_epochs - no. of epochs to run\n","                        demo - False, by default. Set to True only during demo and testing\n","        \"\"\"\n","        train_loader,val_loader = self.preprocessing.train_test_split()\n","        \n","        # number of epochs\n","        num_epochs = num_epochs    \n","        self.model.to(self.device)\n","    \n","        # parameters and optimizer\n","        params = [p for p in self.model.parameters() if p.requires_grad]\n","        optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.01)\n","        \n","        self.total_batch_count = len(train_loader)\n","\n","        print('Training the instance segmentation model..')\n","        for epoch in range(num_epochs):\n","            print(f'Epoch no: {epoch}')\n","            self.model.train()\n","            i = 1\n","            for imgs, annotations in train_loader:\n","                imgs = list(img.to(self.device) for img in imgs)\n","                annotations = [{key: value.to(self.device) for key, value in annotation.items()} for annotation in annotations]\n","                loss_dict = self.model(imgs, annotations)\n","                losses = sum(loss for loss in loss_dict.values())\n","        \n","                optimizer.zero_grad()\n","                losses.backward()\n","                optimizer.step()\n","                print(f'Epoch no: {epoch}/{num_epochs-1}, Batch no: {i}/{self.total_batch_count}, Loss: {losses}')\n","                i+=1\n","        #### to be removed after demo ###\n","                if demo == True:\n","                    break\n","            if demo == True:\n","                break\n","        #### to be removed after demo ###\n","\n","        print('Model training completed')\n","        #Validation\n","        print('Validating on the held-out set...')\n","        #engine.evaluate(self.model, val_loader, device=self.device)\n","        print('validation completed')\n","\n","        #saving the model\n","        torch.save(self.model.state_dict(),'./weights/instance_segment.pt')\n","\n","    #Inference\n","    def predict_instance_masks(self,img,demo=False):\n","        \"\"\"\n","        Loads the image and predicts bounding boxes using the model saved in weights folder\n","        Overlays predicted boxes on the image along with predicted tags\n","        Saves the tagged image in the Uploads\\tagging\\test directory\n","        \"\"\"\n","        import numpy as np\n","        import cv2\n","\n","        img = Image.open(img)\n","        #transforms\n","        test_image = T.ToTensor()(img)\n","\n","        model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=False)\n","        # get number of input features for the classifier\n","        in_features = model.roi_heads.box_predictor.cls_score.in_features\n","        # replace the pre-trained head with a new one\n","        model.roi_heads.box_predictor = FastRCNNPredictor(in_features, self.num_classes)\n","        # now get the number of input features for the mask classifier\n","        in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","        hidden_layer = 256\n","        # and replace the mask predictor with a new one\n","        model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,hidden_layer,self.num_classes)\n","        \n","        #fill the model object with the trained weights in state_dict\n","        model.load_state_dict(torch.load('./weights/instance_segment.pt'))\n","        model.eval()\n","\n","        #Passing the test image to model\n","        test_image = test_image.to(self.device)\n","        pred = model([test_image])\n","\n","        threshold = 0.1 if demo == True else 0.9\n","        \n","        rect_th=1\n","        text_size=.3\n","        text_th=1\n","        \n","        #Extracting score, masks, classes and predicted bboxes from output\n","        \n","        pred_score = list(pred[0]['scores'].detach().to('cpu').numpy())\n","        if demo: \n","            pred_t = 3\n","        else: pred_t = [pred_score.index(x) for x in pred_score if x>threshold][-1]\n","        masks = (pred[0]['masks']>0).squeeze().detach().to('cpu').numpy()\n","        pred_class = [self.class_dict[i] for i in list(pred[0]['labels'].detach().to('cpu').numpy())]\n","        \n","        pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().to('cpu').numpy())]\n","        masks = masks[:pred_t+1]\n","        pred_boxes = pred_boxes[:pred_t+1]\n","        pred_class = pred_class[:pred_t+1]\n","\n","        img = np.array(img)\n","        #img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        \n","        #Overlaying the masks and predicted bounding boxes on the image\n","        \n","        for i in range(len(masks)):\n","            colours = [[0, 255, 0],[0, 0, 255],[255, 0, 0],[0, 255, 255],[255, 255, 0],[255, 0, 255],[80, 70, 180],[250, 80, 190],[245, 145, 50],[70, 150, 250],[50, 190, 190]]\n","            r = np.zeros_like(masks[i]).astype(np.uint8)\n","            g = np.zeros_like(masks[i]).astype(np.uint8)\n","            b = np.zeros_like(masks[i]).astype(np.uint8)\n","            r[masks[i] == 1], g[masks[i] == 1], b[masks[i] == 1] = colours[random.randrange(0,10)]\n","            rgb_mask = np.stack([r, g, b], axis=2)\n","            img = cv2.addWeighted(img, 1, rgb_mask, 0.5, 0)\n","            cv2.rectangle(img, pred_boxes[i][0], pred_boxes[i][1],color=(0, 255, 0), thickness=rect_th)\n","            cv2.putText(img,pred_class[i], pred_boxes[i][0], cv2.FONT_HERSHEY_SIMPLEX, text_size, (0,255,0),thickness=text_th)\n","        \n","        inst_mask_image_loc = os.path.join(r'.\\Uploads',\n","                                           \"segmentation\",\n","                                           \"instance_segmentation\",\n","                                           \"test\")\n","        inst_mask_image = os.path.join(inst_mask_image_loc,\"inst_masked.jpg\")\n","        \n","        cv2.imwrite(inst_mask_image,img)\n","        \n","        return inst_mask_image\n"],"execution_count":null,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-9835bddab3f1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmask_rcnn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMaskRCNNPredictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection_pytorch_repo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection_pytorch_repo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"id":"ai2nbR5IVf15"},"source":[""],"execution_count":null,"outputs":[]}]}