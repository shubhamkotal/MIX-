{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Deepinstcheck.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOxZ60Sppnn8NkMnr+7iPAs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"6f2610dd08914427aab32f5c73d1f710":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_dc75669aa7fb4249911500c81c701be7","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0331046fd49440a0a53e6516cc7a5faa","IPY_MODEL_4a1232f27e4a48e890425808fec22cb7"]}},"dc75669aa7fb4249911500c81c701be7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"row wrap","width":"100%","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"inline-flex","left":null}},"0331046fd49440a0a53e6516cc7a5faa":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_12739faae9b94e54b11d27e47f2e92a7","_dom_classes":[],"description":"Validation sanity check: ","_model_name":"FloatProgressModel","bar_style":"info","max":0,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2942125c07fe47bfae36384d9c14daf1"}},"4a1232f27e4a48e890425808fec22cb7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_e5f378bb543f49cdba59fd9150af1eda","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/0 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_71b84456953f431680d59b64c69e55e9"}},"12739faae9b94e54b11d27e47f2e92a7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"2942125c07fe47bfae36384d9c14daf1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":"2","_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e5f378bb543f49cdba59fd9150af1eda":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"71b84456953f431680d59b64c69e55e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"6dc7b7c6b6524fd0b7c362898d8104fb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_c724af3c7b4e418082637f1f6e301273","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_d818b16e78fe4741a54c8d4dcdb84e04","IPY_MODEL_c31a1fc8659b4eb99ab2013c2e4f1cb1"]}},"c724af3c7b4e418082637f1f6e301273":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"row wrap","width":"100%","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"inline-flex","left":null}},"d818b16e78fe4741a54c8d4dcdb84e04":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6797cd754c794481a31b0f4c49cd447d","_dom_classes":[],"description":"Epoch 0: 100%","_model_name":"FloatProgressModel","bar_style":"success","max":80,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":80,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e4cdb9ca7563423b9e8d312a3042a73f"}},"c31a1fc8659b4eb99ab2013c2e4f1cb1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ab8abd2672a24a53bae70dc1c7843d6f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 80/80 [22:49&lt;00:00, 17.12s/it, loss=0.606, v_num=3]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0e19843ccca94bb58d82258091dc4fa1"}},"6797cd754c794481a31b0f4c49cd447d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e4cdb9ca7563423b9e8d312a3042a73f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":"2","_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ab8abd2672a24a53bae70dc1c7843d6f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0e19843ccca94bb58d82258091dc4fa1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BRm6imlSaSMt","executionInfo":{"status":"ok","timestamp":1618806965761,"user_tz":-330,"elapsed":10938,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"6a47a74c-ee75-4cb8-8c7c-f845be5177c6"},"source":["!pip install pytorch-lightning"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Collecting pytorch-lightning\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c4/99/68da5c6ca999de560036d98c492e507d17996f5eeb7e76ba64acd4bbb142/pytorch_lightning-1.2.8-py3-none-any.whl (841kB)\n","\u001b[K     |████████████████████████████████| 849kB 19.3MB/s \n","\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (2.4.1)\n","Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.8.1+cu101)\n","Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (1.19.5)\n","Collecting future>=0.17.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/0b/38b06fd9b92dc2b68d58b75f900e97884c45bedd2ff83203d933cf5851c9/future-0.18.2.tar.gz (829kB)\n","\u001b[K     |████████████████████████████████| 829kB 53.2MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-lightning) (4.41.1)\n","Collecting fsspec[http]>=0.8.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n","\u001b[K     |████████████████████████████████| 112kB 54.0MB/s \n","\u001b[?25hCollecting PyYAML!=5.4.*,>=5.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n","\u001b[K     |████████████████████████████████| 276kB 50.8MB/s \n","\u001b[?25hCollecting torchmetrics>=0.2.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3a/42/d984612cabf005a265aa99c8d4ab2958e37b753aafb12f31c81df38751c8/torchmetrics-0.2.0-py3-none-any.whl (176kB)\n","\u001b[K     |████████████████████████████████| 184kB 42.8MB/s \n","\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.12.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (2.23.0)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (54.2.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.15.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.3.4)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.8.0)\n","Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.36.2)\n","Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.28.1)\n","Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.32.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (0.4.4)\n","Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (3.12.4)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch-lightning) (1.0.1)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->pytorch-lightning) (3.7.4.3)\n","Collecting aiohttp; extra == \"http\"\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/c0/5890b4c8b04a79b7360e8fe4490feb0bb3ab179743f199f0e6220cebd568/aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3MB)\n","\u001b[K     |████████████████████████████████| 1.3MB 54.6MB/s \n","\u001b[?25hRequirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard>=2.2.0->pytorch-lightning) (2.10)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.10.1)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.2.1)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (4.7.2)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (1.3.0)\n","Collecting yarl<2.0,>=1.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f1/62/046834c5fc998c88ab2ef722f5d42122230a632212c8afa76418324f53ff/yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294kB)\n","\u001b[K     |████████████████████████████████| 296kB 51.0MB/s \n","\u001b[?25hCollecting multidict<7.0,>=4.5\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7c/a6/4123b8165acbe773d1a8dc8e3f0d1edea16d29f7de018eda769abb56bd30/multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142kB)\n","\u001b[K     |████████████████████████████████| 143kB 55.0MB/s \n","\u001b[?25hCollecting async-timeout<4.0,>=3.0\n","  Downloading https://files.pythonhosted.org/packages/e1/1e/5a4441be21b0726c4464f3f23c8b19628372f606755a9d2e46c187e65ec4/async_timeout-3.0.1-py3-none-any.whl\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp; extra == \"http\"->fsspec[http]>=0.8.1->pytorch-lightning) (20.3.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=2.2.0->pytorch-lightning) (3.4.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=2.2.0->pytorch-lightning) (0.4.8)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch-lightning) (3.1.0)\n","Building wheels for collected packages: future, PyYAML\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-cp37-none-any.whl size=491058 sha256=ed87201b12abe6d3fe25f6968218992c5bae72b4349e0f9e4a05d8a39179b33b\n","  Stored in directory: /root/.cache/pip/wheels/8b/99/a0/81daf51dcd359a9377b110a8a886b3895921802d2fc1b2397e\n","  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44620 sha256=eee195c200d2e635da1e06a07d5248b8d84924760f0defe13f827169eca86b61\n","  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n","Successfully built future PyYAML\n","Installing collected packages: future, multidict, yarl, async-timeout, aiohttp, fsspec, PyYAML, torchmetrics, pytorch-lightning\n","  Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","  Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed PyYAML-5.3.1 aiohttp-3.7.4.post0 async-timeout-3.0.1 fsspec-2021.4.0 future-0.18.2 multidict-5.1.0 pytorch-lightning-1.2.8 torchmetrics-0.2.0 yarl-1.6.3\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZSXCUpYEUGaC","executionInfo":{"status":"ok","timestamp":1618807020078,"user_tz":-330,"elapsed":45087,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"b6897d4f-1802-45eb-c761-c8523bf5c27d"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')  #mounting\n","%cd '/content/gdrive/MyDrive/DEEPLOBE/segmentation'\n"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n","/content/gdrive/MyDrive/DEEPLOBE/segmentation\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YuZ4lU3Iak8b","executionInfo":{"status":"ok","timestamp":1618807043873,"user_tz":-330,"elapsed":1053,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"97dffa4d-3ce9-4f80-c1e1-423d8f42e7f7"},"source":["cd 'maskrcnn_pistol_data'"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/gdrive/My Drive/DEEPLOBE/segmentation/maskrcnn_pistol_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BHcDR8iJUYNd","executionInfo":{"status":"ok","timestamp":1618816508455,"user_tz":-330,"elapsed":1908,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["#importing dependent libraries\n","import os\n","import torch\n","import torch.utils.data\n","import torchvision\n","import numpy as np\n","from PIL import Image\n","from pycocotools.coco import COCO\n","from torchvision import transforms as T\n","\n","import matplotlib.pyplot as plt\n","import cv2\n","import random\n","from pathlib import Path\n","\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n","\n","import pytorch_lightning as pl  \n","from pytorch_lightning.callbacks import ModelCheckpoint\n","\n","\n","import torch.nn as nn\n","from torch.utils.data import DataLoader\n","\n","class MyOwnDataset(torch.utils.data.Dataset):\n","    \"\"\"\n","    Custom class inheriting from Pytorch's Dataset utility class\n","    that allows applying custom transformations on user-datasets\n","    \n","    It returns transformed images and masks in an iterator object \n","    that can be indexed according to the batch sizes\n","    in the data loading phase for passing to model\n","    \"\"\"\n","    def __init__(self, root, annotation, transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        self.coco = COCO(annotation)\n","        self.ids = list(sorted(self.coco.imgs.keys()))\n","\n","    def __getitem__(self, index):\n","        # Own coco file\n","        coco = self.coco\n","        # Image ID\n","        img_id = self.ids[index]\n","        # List: get annotation id from coco\n","        ann_ids = coco.getAnnIds(imgIds=img_id)\n","        # Dictionary: target coco_annotation file for an image\n","        coco_annotation = coco.loadAnns(ann_ids)\n","        # filename for input image\n","        path = coco.loadImgs(img_id)[0]['file_name']\n","        # filename of input mask corresponding to the above image file\n","        mask_file_path = os.path.splitext(path)[0] + '.png'\n","        # open the input image\n","        img = Image.open(os.path.join(self.root, 'images',path)).convert('RGB')\n","        ## mask ##\n","        mask = Image.open(os.path.join(self.root,'masks',mask_file_path))\n","        \n","        # number of objects in the image\n","        num_objs = len(coco_annotation)\n","\n","        # Bounding boxes for objects\n","        # In coco format, bbox = [xmin, ymin, width, height]\n","        # In pytorch, the input should be [xmin, ymin, xmax, ymax]\n","        boxes = []\n","        for i in range(num_objs):\n","            xmin = coco_annotation[i]['bbox'][0]\n","            ymin = coco_annotation[i]['bbox'][1]\n","            xmax = xmin + coco_annotation[i]['bbox'][2]\n","            ymax = ymin + coco_annotation[i]['bbox'][3]\n","            boxes.append([xmin, ymin, xmax, ymax])\n","            \n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","\n","        # Tensorise img_id\n","        img_id = torch.tensor([img_id])\n","        # Size of bbox (Rectangular)\n","        areas = []\n","        iscrowd = []\n","        labels = []\n","\n","        for i in range(num_objs):\n","            areas.append(coco_annotation[i]['area'])\n","            iscrowd.append(coco_annotation[i]['iscrowd'])\n","            labels.append(coco_annotation[i]['category_id'])\n","\n","        areas,iscrowd,labels = map(torch.tensor, [areas,iscrowd,labels])\n","        \n","        if self.transforms is not None:\n","            img = self.transforms(img)\n","            mask = self.transforms(mask)\n","        mask = mask.numpy().reshape(mask.shape,order='F')\n","        mask = torch.as_tensor(mask, dtype=torch.uint8)\n","\n","        # Annotation is in dictionary format\n","        my_annotation = {}\n","        my_annotation[\"boxes\"] = boxes\n","        my_annotation[\"labels\"] = labels\n","        my_annotation[\"image_id\"] = img_id\n","        my_annotation[\"area\"] = areas\n","        my_annotation[\"iscrowd\"] = iscrowd\n","        my_annotation[\"masks\"] = mask\n","        \n","        return img, my_annotation\n","\n","    def __len__(self):\n","        return len(self.ids)\n","\n","\n","\n","def get_transform(train):\n","    custom_transforms = []\n","    custom_transforms.append(T.ToTensor())\n","    return T.Compose(custom_transforms)\n","\n","\n","def process_data(loc):\n","  \"\"\"\n","   For loading imagefiles, preprocessing like train-test split based on a specified \n","    validation set size.\n","    \n","    Passing data directory and transforms for creating a dataset iterator\n","    \n","    Creates a dataset instance from the UserDataset class by passing data directory path.\n","    The data directory should have the following structure:\n","\n","                    data_dir\n","             ------Image\n","             ---------Image1\n","             ---------Image2\n","             ---------\n","             ---------\n","                   \n","             ---------ImageN\n","             ------Mask\n","             ---------Mask1\n","             ---------Mask2\n","             ---------\n","             ---------\n","             ---------MaskN\n","  \"\"\"\n","  path = Path(loc)\n","  data_dir = path\n","  coco_instances = path/'coco_instances.json'\n","  # create own Dataset\n","  my_dataset = MyOwnDataset(root=data_dir,\n","                            annotation=coco_instances,\n","                            transforms=get_transform(train=True)) \n","  #Category dictinoray for output tagging\n","  instances = COCO(coco_instances)\n","  categories = instances.loadCats(instances.getCatIds())\n","  class_dict = {d['id']:d['name'] for d in categories}  \n","  return my_dataset, class_dict, instances\n","\n","def get_class(loc):\n","  path = Path(loc)\n","  data_dir = path\n","  coco_instances = path/'coco_instances.json'\n","  instances = COCO(coco_instances)\n","  return instances\n","\n","\n","class DataModuleInstance(pl.LightningDataModule):\n","  \"\"\"\n","  This class also calls the CustomDataset class above for creating \n","  a dataset iterator and creates dataloaders using torch.utils.Dataloaders class\n","  \"\"\"\n","  def __init__(self,my_dataset):\n","    super(DataModuleInstance,self).__init__()\n","    self.bs=10\n","    self.my_dataset=my_dataset\n","    samples = len(my_dataset)\n","    test_counts = int(samples*.2)\n","    train_counts = samples-test_counts\n","    self.train_set, self.test_set = torch.utils.data.random_split(self.my_dataset, [train_counts, test_counts])\n","    # loading data using Dataloader: Train, val\n","    def collate_fn(batch):\n","      return tuple(zip(*batch))\n","    self.collate_fn=collate_fn  \n","  def train_dataloader(self):\n","    return DataLoader(self.train_set,batch_size=self.bs,shuffle=False,collate_fn=self.collate_fn)\n","  def test_dataloader(self):\n","    return DataLoader(self.test_set,batch_size=self.bs,shuffle=False,collate_fn=self.collate_fn)\n","\n","\n","\n","class InstanceSegment(pl.LightningModule):\n","  \"\"\"\n","   \n","    calls a device object based on GPU availability\n","    with a downloaded pretrained model from torchvision library\n","    changes the final layer based on the target class count\n","    trains the model\n","   \n","    \n","    \n","  \"\"\"\n","  def __init__(self,path):\n","    super(InstanceSegment,self).__init__()\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","    # add model\n","    instance=get_class(path)\n","    num_classes = 1 + len(instance.getCatIds())\n","    model = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n","    in_features = model.roi_heads.box_predictor.cls_score.in_features\n","    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n","    hidden_layer = 256\n","    model.roi_heads.mask_predictor = MaskRCNNPredictor(in_features_mask,hidden_layer,num_classes)\n","\n","    self.model=model \n","    self.model.to(device)\n","    self.params = [p for p in self.model.parameters() if p.requires_grad]\n","\n","  def forward(self,x):\n","    output = self.model(x)\n","    return output\n","\n","\n","  def configure_optimizers(self): # specifying optimizer and learning rate\n","    return torch.optim.SGD(self.params, lr=0.005, momentum=0.08)\n","\n","\n","    # Training , validating phase with lightening method\n","  def training_step(self, train_batch,batch_idx):\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    imgs, annotations  = train_batch\n","    imgs = list(img.to(device) for img in imgs)\n","    annotations = [{key: value.to(device) for key, value in annotation.items()} for annotation in annotations]\n","    loss_dict = self.model(imgs, annotations)\n","    losses = sum(loss for loss in loss_dict.values())\n","    self.log('Training Loss', losses, on_step=True, on_epoch=True, sync_dist=True) \n","    return losses \n","  def validation_step(self, valid_batch, batch_idx):\n","    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","    imgs, annotations  = valid_batch\n","    imgs = list(img.to(device) for img in imgs)\n","    annotations = [{key: value.to(device) for key, value in annotation.items()} for annotation in annotations]\n","    loss_dict = self.model(imgs, annotations)\n","    losses = sum(loss for loss in loss_dict.values())\n","    self.log('Validation Loss', losses, on_step=True, on_epoch=True, sync_dist=True) \n","    return losses \n","\n","\n","def train_model(epochs, my_dataset, path):\n","     \"\"\"\n","     creating the model\n","     \"\"\"\n","     max_epoc= 10 if epochs is None else epochs\n","     data_module = DataModuleInstance(my_dataset) \n","     model_module = InstanceSegment(path) # change\n","     gpu=1 if torch.cuda.is_available() else 0 # setting gpu based on availability\n","     checkpoint_callback = ModelCheckpoint(monitor ='Training Loss',dirpath = path)\n","     trainer = pl.Trainer(max_epochs=max_epoc,gpus=gpu,default_root_dir = path,callbacks = [checkpoint_callback])\n","     trainer.fit(model_module, data_module) # fit model\n","\n","\n","def random_colour_masks(image):\n","    colours = [[0, 255, 0],[0, 0, 255],[255, 0, 0],[0, 255, 255],[255, 255, 0],[255, 0, 255],[80, 70, 180],[250, 80, 190],[245, 145, 50],[70, 150, 250],[50, 190, 190]]\n","    r = np.zeros_like(image).astype(np.uint8)\n","    g = np.zeros_like(image).astype(np.uint8)\n","    b = np.zeros_like(image).astype(np.uint8)\n","    r[image == 1], g[image == 1], b[image == 1] = colours[random.randrange(0,10)]\n","    coloured_mask = np.stack([r, g, b], axis=2)\n","    return coloured_mask\n","\n","def get_video_prediction(img_file, model_weight,annotation_file_path,threshold):\n","    model=InstanceSegment(annotation_file_path)\n","    model=model.load_from_checkpoint(model_weight,path=annotation_file_path)\n","    instances=get_class(annotation_file_path)\n","    categories = instances.loadCats(instances.getCatIds())\n","    class_dict = {d['id']:d['name'] for d in categories} \n","    img = Image.open(img_file)\n","    transform = T.Compose([T.ToTensor()])\n","    img = transform(img)\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    img = img.to(device) #cuda()\n","    model=model.to(device)\n","    model=model.eval()\n","    pred = model([img])\n","    pred_score = list(pred[0]['scores'].detach().to('cpu').numpy())\n","    pred_t = [pred_score.index(x) for x in pred_score if x>threshold][-1]    \n","    masks = (pred[0]['masks']>0.5).squeeze().detach().to('cpu').numpy()\n","    pred_class = [class_dict[i] for i in list(pred[0]['labels'].detach().to('cpu').numpy())]\n","    pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().to('cpu').numpy())]\n","    masks = masks[:pred_t+1]\n","    boxes = pred_boxes[:pred_t+1]\n","    pred_cls = pred_class[:pred_t+1]\n","    img1 = cv2.imread(img_file)\n","    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n","    for i in range(len(masks)):\n","        rgb_mask = random_colour_masks(masks[i])\n","        img1 = cv2.addWeighted(img1, 1, rgb_mask, 0.5, 0)\n","        cv2.rectangle(img1, boxes[i][0], boxes[i][1],color=(0, 255, 0), thickness=3)\n","        cv2.putText(img1,pred_cls[i], boxes[i][0], cv2.FONT_HERSHEY_SIMPLEX, 3, (0,255,0),thickness=3)\n","    return img1\n","    \n","def video_prediction(input_video,output_video,model_weight,annotation_file_path, threshold_value):\n","  video_capture = cv2.VideoCapture(input_video)\n","  fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n","  out = cv2.VideoWriter(output_video,fourcc, 30, (1280,720),True)\n","  a=0\n","  while True:\n","      cap,frame = video_capture.read()\n","      if cap == False:\n","        break\n","      a+=1\n","      print('frame count: ',a)\n","      rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","      newframe = get_video_prediction(rgb_frame, model_weight,annotation_file_path,threshold)\n","      out.write(newframe)\n","  out.release()\n","  video_capture.release()\n","\n","def image_prediction(img_file, model_weight,annotation_file_path,threshold,output_file_name):\n","    model=InstanceSegment(annotation_file_path)\n","    model=model.load_from_checkpoint(model_weight,path=annotation_file_path)\n","    instances=get_class(annotation_file_path)\n","    categories = instances.loadCats(instances.getCatIds())\n","    class_dict = {d['id']:d['name'] for d in categories} \n","    img = Image.open(img_file)\n","    transform = T.Compose([T.ToTensor()])\n","    img = transform(img)\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","    img = img.to(device) #cuda()\n","    model=model.to(device)\n","    model=model.eval()\n","    pred = model([img])\n","    pred_score = list(pred[0]['scores'].detach().to('cpu').numpy())\n","    pred_t = [pred_score.index(x) for x in pred_score if x>threshold][-1]    \n","    masks = (pred[0]['masks']>0.5).squeeze().detach().to('cpu').numpy()\n","    pred_class = [class_dict[i] for i in list(pred[0]['labels'].detach().to('cpu').numpy())]\n","    pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().to('cpu').numpy())]\n","    masks = masks[:pred_t+1]\n","    boxes = pred_boxes[:pred_t+1]\n","    pred_cls = pred_class[:pred_t+1]\n","    img1 = cv2.imread(img_file)\n","    img1 = cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n","    for i in range(len(masks)):\n","        rgb_mask = random_colour_masks(masks[i])\n","        img1 = cv2.addWeighted(img1, 1, rgb_mask, 0.5, 0)\n","        cv2.rectangle(img1, boxes[i][0], boxes[i][1],color=(0, 255, 0), thickness=3)\n","        cv2.putText(img1,pred_cls[i], boxes[i][0], cv2.FONT_HERSHEY_SIMPLEX, 3, (0,255,0),thickness=3)\n","    plt.figure(figsize=(20,30))\n","    plt.imshow(img1)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.show()\n","    filename = output_file_name + '.jpg'\n","    cv2.imwrite(filename, img1)\n","\n","\n","class Instance():\n","    def load_data(self,path):\n","      self.path=path\n","      self.my_dataset, self.class_dict, self.instances = process_data(self.path) # process data get the files needed\n","    def train(self,epochs):\n","      self.epochs=epochs\n","      self.model=train_model(self.epochs,self.my_dataset, self.path )\n","    "],"execution_count":64,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TcIl0giGb7m3","executionInfo":{"status":"ok","timestamp":1618810142901,"user_tz":-330,"elapsed":1060,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"a8be3629-87e3-41c3-a8a3-a2129ef7ad85"},"source":["ls"],"execution_count":33,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mdeeplobe-ai-master\u001b[0m/  deeplobe-ai-master.zip  \u001b[01;34mInstance\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1yrJ3JFMajXp","executionInfo":{"status":"ok","timestamp":1618811396874,"user_tz":-330,"elapsed":1577,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"84f6445d-e3b1-4e5d-bfa1-7e7eb4d16144"},"source":["instanceseg_model = Instance()\n","instanceseg_model.load_data('/content/gdrive/MyDrive/Deeplobe GIT/Instance')\n","# instanceseg_model.train(epochs = 1) #default epochs = 10 if not mentioned\n","# instanceseg_model.predict(\"sampleimage\",threshold = 0.1) #threshold ranges from 0 to 1, default = 0.1 if not mentioned\n","# instanceseg_model.predict_on_video(\"sample_video.mp4\",\"output_video.avi\",threshold = 0.7)"],"execution_count":40,"outputs":[{"output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.05s)\n","creating index...\n","index created!\n","loading annotations into memory...\n","Done (t=0.05s)\n","creating index...\n","index created!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":403,"referenced_widgets":["6f2610dd08914427aab32f5c73d1f710","dc75669aa7fb4249911500c81c701be7","0331046fd49440a0a53e6516cc7a5faa","4a1232f27e4a48e890425808fec22cb7","12739faae9b94e54b11d27e47f2e92a7","2942125c07fe47bfae36384d9c14daf1","e5f378bb543f49cdba59fd9150af1eda","71b84456953f431680d59b64c69e55e9","6dc7b7c6b6524fd0b7c362898d8104fb","c724af3c7b4e418082637f1f6e301273","d818b16e78fe4741a54c8d4dcdb84e04","c31a1fc8659b4eb99ab2013c2e4f1cb1","6797cd754c794481a31b0f4c49cd447d","e4cdb9ca7563423b9e8d312a3042a73f","ab8abd2672a24a53bae70dc1c7843d6f","0e19843ccca94bb58d82258091dc4fa1"]},"id":"FeTKGcJmb1aL","executionInfo":{"status":"ok","timestamp":1618811636033,"user_tz":-330,"elapsed":240312,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"7dbbe116-a390-4719-d412-1c435f1995de"},"source":["instanceseg_model.train(epochs = 1) #default epochs = 10 if not mentioned"],"execution_count":41,"outputs":[{"output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.06s)\n","creating index...\n","index created!\n"],"name":"stdout"},{"output_type":"stream","text":["GPU available: True, used: True\n","TPU available: False, using: 0 TPU cores\n","/usr/local/lib/python3.7/dist-packages/pytorch_lightning/utilities/distributed.py:68: UserWarning: you defined a validation_step but have no val_dataloader. Skipping validation loop\n","  warnings.warn(*args, **kwargs)\n","LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n","\n","  | Name  | Type     | Params\n","-----------------------------------\n","0 | model | MaskRCNN | 44.4 M\n","-----------------------------------\n","44.2 M    Trainable params\n","222 K     Non-trainable params\n","44.4 M    Total params\n","177.515   Total estimated model params size (MB)\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6f2610dd08914427aab32f5c73d1f710","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Validation sanity check', layout=Layout…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\r"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6dc7b7c6b6524fd0b7c362898d8104fb","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tkbUi4MuoJKJ","executionInfo":{"status":"ok","timestamp":1618811751415,"user_tz":-330,"elapsed":3346,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"2122f25f-70ae-4eca-c7b5-94b24edd1f70"},"source":["  model=InstanceSegment('/content/gdrive/MyDrive/Deeplobe GIT/Instance')\n","  model=model.load_from_checkpoint('/content/gdrive/MyDrive/Deeplobe GIT/Instance/epoch=0-step=79.ckpt',path='/content/gdrive/MyDrive/Deeplobe GIT/Instance')"],"execution_count":42,"outputs":[{"output_type":"stream","text":["loading annotations into memory...\n","Done (t=0.06s)\n","creating index...\n","index created!\n","loading annotations into memory...\n","Done (t=0.05s)\n","creating index...\n","index created!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5S8BtQWB65Ln"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":990,"output_embedded_package_id":"1ULKp-0gBc303HeREVODk21d4Fvf2dvEN"},"id":"nrPgjho7wZzl","executionInfo":{"status":"ok","timestamp":1618816527186,"user_tz":-330,"elapsed":8408,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"41647360-bcc6-4f4f-897a-c519e8a9dd96"},"source":["image_prediction('/content/gdrive/MyDrive/Deeplobe GIT/Instance/images/00000000.png', '/content/gdrive/MyDrive/Deeplobe GIT/Instance/epoch=0-step=79.ckpt',\n","               '/content/gdrive/MyDrive/Deeplobe GIT/Instance',0.5,'test5')"],"execution_count":65,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"cUfymbjf4yvg"},"source":[""],"execution_count":null,"outputs":[]}]}