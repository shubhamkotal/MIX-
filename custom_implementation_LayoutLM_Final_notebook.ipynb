{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"custom_implementation_LayoutLM_Final_notebook.ipynb","provenance":[{"file_id":"1OmL70CxYX2fGqjsyOlWu90unYEqpJqM-","timestamp":1599203969818},{"file_id":"1Q9Xg6TrpMRB5SX--TnBoBkT2PJH3TtDE","timestamp":1597216091956}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"sE3Oy-IAOHTh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599480738665,"user_tz":-330,"elapsed":1063,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"aa9f7368-c10d-4094-dbe5-b212aec07588"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PTkyimHpOcAT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599480742674,"user_tz":-330,"elapsed":1280,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"432af533-1a23-4c00-96a8-6b2c96643771"},"source":["%cd /content/gdrive/My Drive/unilm"],"execution_count":2,"outputs":[{"output_type":"stream","text":["/content/gdrive/.shortcut-targets-by-id/19rY6aGdoyGH5C22rAwdx79e5pwrlXHil/unilm\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zIoHzVydOzJm","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":471},"executionInfo":{"status":"ok","timestamp":1599480747478,"user_tz":-330,"elapsed":4232,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"cb6aa098-e2cb-4011-e491-648dce69d245"},"source":["!pip install seqeval transformers==2.9.0 tensorboardX"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: seqeval in /usr/local/lib/python3.6/dist-packages (0.0.12)\n","Requirement already satisfied: transformers==2.9.0 in /usr/local/lib/python3.6/dist-packages (2.9.0)\n","Requirement already satisfied: tensorboardX in /usr/local/lib/python3.6/dist-packages (2.1)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.18.5)\n","Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.4.3)\n","Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (0.7.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (2019.12.20)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (4.41.1)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (0.1.91)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (2.23.0)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (0.7)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (0.0.43)\n","Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (3.12.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorboardX) (1.15.0)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.10.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (2020.6.20)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (1.24.3)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.0) (0.16.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.0) (7.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.8.0->tensorboardX) (49.6.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"tWE3YkBgR8VG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1599480747480,"user_tz":-330,"elapsed":2557,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"a4083762-1cbb-46d0-cdc0-e81f45841dfc"},"source":["%cd \"/content/gdrive/My Drive/unilm/layoutlm/examples/seq_labeling/\""],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/gdrive/.shortcut-targets-by-id/19rY6aGdoyGH5C22rAwdx79e5pwrlXHil/unilm/layoutlm/examples/seq_labeling\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"adKoTEKBRT_4","colab_type":"text"},"source":["*preprocessing* as per funsd\n"]},{"cell_type":"code","metadata":{"id":"8ZTbnZ0CQ5zb","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599480752818,"user_tz":-330,"elapsed":5485,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["import argparse\n","import json\n","import os\n","\n","from PIL import Image\n","from transformers import AutoTokenizer\n","\n","\n","def bbox_string(box, width, length):\n","    return (\n","        str(int(1000 * (int(box[0]) / width)))\n","        + \" \"\n","        + str(int(1000 * (int(box[1])/ length)))\n","        + \" \"\n","        + str(int(1000 * (int(box[2]) / width)))\n","        + \" \"\n","        + str(int(1000 * (int(box[3]) / length)))\n","    )\n","\n","\n","def actual_bbox_string(box, width, length):\n","    return (\n","        str(box[0])\n","        + \" \"\n","        + str(box[1])\n","        + \" \"\n","        + str(box[2])\n","        + \" \"\n","        + str(box[3])\n","        + \"\\t\"\n","        + str(width)\n","        + \" \"\n","        + str(length)\n","    )\n","\n","path='data/dataset/training_data/new_annotations/'\n","\n","def convert(args):\n","    with open(\n","        os.path.join(args.output_dir, args.data_split + \".txt.tmp\"),\n","        \"w\",\n","        encoding=\"utf8\",\n","    ) as fw, open(\n","        os.path.join(args.output_dir, args.data_split + \"_box.txt.tmp\"),\n","        \"w\",\n","        encoding=\"utf8\",\n","    ) as fbw, open(\n","        os.path.join(args.output_dir, args.data_split + \"_image.txt.tmp\"),\n","        \"w\",\n","        encoding=\"utf8\",\n","    ) as fiw:\n","        for file in os.listdir(path):\n","          with open(path + file,encoding='utf8') as f: \n","            data=f.readlines()\n","            other=[]\n","            rest=[]\n","          for i in data:\n","            if 'other' in i or 'total' in i or 'date' in i:\n","              other.append(i)\n","            else:\n","              rest.append(i) \n","          company=[]\n","          address=[]\n","          heading=[]\n","\n","          for i in rest:\n","            if 'company' in i:\n","              a=i.split(',')\n","              b=[a[0], a[1], a[2] ,a[3]]\n","              c= {'text': a[4] , 'box' : b}\n","              company.append(c)\n","            if 'address' in i:\n","              a=i.split(',') \n","              b=[a[0], a[1], a[2] ,a[3]]\n","              c= {'text': a[4] , 'box' : b}\n","              address.append(c)\n","         #   if 'heading' in i:\n","          #    a=i.split(',') \n","           #   b=[a[0], a[1], a[2] ,a[3]]\n","            #  c= {'text': a[4] , 'box' : b}\n","             # heading.append(c)\n","    \n","\n","          ls1=[]\n","          ls2=[]\n","          for i in other:\n","            text=str(i.split(\",\")[4:-1]).replace(\"'\",\"\").replace(\"[\",\"\").replace(\"]\",\"\").replace('\\\\n',' ')\n","            apple=[{'box':i.split(\",\")[0:4],'text':text}]\n","            ball=str(i.split(\",\")[-1]).replace('\\n',' ')\n","            ls1.append(apple)\n","            ls2.append(ball)\n","\n","          ls1.append(company)\n","          ls1.append(address)\n","       #   ls1.append(heading)\n","\n","          ls2.append('company')\n","          ls2.append('address')\n","      #    ls2.append('heading')\n","\n","\n","\n","\n","          words=ls1.copy()\n","          label=ls2.copy()\n","\n","          image_path='data/dataset/training_data/new_image/X00016469619.jpg'\n","\n","\n","     \n","          file_name = os.path.basename(image_path)\n","          image = Image.open(image_path)\n","          width, length = image.size\n","\n","       \n","  \n","          for i,j in zip(words,label):\n","            words = [w for w in i if w[\"text\"].strip() != \"\"]\n","            if len(words) == 0:\n","              continue\n","            label=j  \n","            if label == \"other\":\n","              for w in words:\n","                fw.write(w[\"text\"] + \"\\tO\\n\")\n","                fbw.write(w[\"text\"]\n","                          + \"\\t\"\n","                          + bbox_string(w[\"box\"], width, length)\n","                          + \"\\n\")\n","                fiw.write(w[\"text\"]+\n","                          \"\\t\"\n","                          + actual_bbox_string(w[\"box\"], width, length)\n","                          + \"\\t\"\n","                          + file_name\n","                          + \"\\n\"\n","                          )\n","            else:\n","              if len(words) == 1:\n","                fw.write(words[0][\"text\"] + \"\\tS-\" + label.upper() + \"\\n\")\n","                fbw.write(words[0][\"text\"]\n","                          + \"\\t\"\n","                          + bbox_string(words[0][\"box\"], width, length)\n","                          + \"\\n\"\n","                          )\n","                fiw.write(words[0][\"text\"]\n","                          + \"\\t\"\n","                          + actual_bbox_string(words[0][\"box\"], width, length)\n","                          + \"\\t\"\n","                          + file_name\n","                          + \"\\n\"\n","                          )\n","              else:\n","                fw.write(words[0][\"text\"] \n","                         + \"\\tB-\" + label.upper() + \"\\n\")\n","                fbw.write(words[0][\"text\"]\n","                          + \"\\t\"\n","                          + bbox_string(words[0][\"box\"], width, length)\n","                          + \"\\n\"\n","                          )\n","                fiw.write(words[0][\"text\"]\n","                          + \"\\t\"\n","                          + actual_bbox_string(words[0][\"box\"], width, length)\n","                          + \"\\t\"\n","                          + file_name\n","                          + \"\\n\"\n","                          )\n","                for w in words[1:-1]:\n","                  fw.write(w[\"text\"] + \"\\tI-\" + label.upper() + \"\\n\")\n","                  fbw.write(w[\"text\"]\n","                            + \"\\t\"\n","                            + bbox_string(w[\"box\"], width, length)\n","                            + \"\\n\")\n","                  fiw.write(w[\"text\"]\n","                            + \"\\t\"\n","                            + actual_bbox_string(w[\"box\"], width, length)\n","                            + \"\\t\"\n","                            + file_name\n","                            + \"\\n\")\n","                  fw.write(words[-1][\"text\"] + \"\\tE-\" + label + \"\\n\")\n","                  fbw.write(words[-1][\"text\"]+ \"\\t\"+ bbox_string(words[-1][\"box\"], width, length)+ \"\\n\")\n","                  fiw.write( words[-1][\"text\"]+ \"\\t\"+ actual_bbox_string(words[-1][\"box\"], width, length) + \"\\t\"+ file_name+ \"\\n\" )\n","                  \n","          fw.write(\"\\n\")\n","          fbw.write(\"\\n\")\n","          fiw.write(\"\\n\")\n","\n","\n","def seg_file(file_path, tokenizer, max_len):\n","    subword_len_counter = 0\n","    output_path = file_path[:-4]\n","    with open(file_path, \"r\", encoding=\"utf8\") as f_p, open(\n","        output_path, \"w\", encoding=\"utf8\"\n","    ) as fw_p:\n","        for line in f_p:\n","            line = line.rstrip()\n","\n","            if not line:\n","                fw_p.write(line + \"\\n\")\n","                subword_len_counter = 0\n","                continue\n","            token = line.split(\"\\t\")[0]\n","\n","            current_subwords_len = len(tokenizer.tokenize(token))\n","\n","            # Token contains strange control characters like \\x96 or \\x95\n","            # Just filter out the complete line\n","            if current_subwords_len == 0:\n","                continue\n","\n","            if (subword_len_counter + current_subwords_len) > max_len:\n","                fw_p.write(\"\\n\" + line + \"\\n\")\n","                subword_len_counter = current_subwords_len\n","                continue\n","\n","            subword_len_counter += current_subwords_len\n","\n","            fw_p.write(line + \"\\n\")\n","\n","\n","def seg(args):\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        args.model_name_or_path, do_lower_case=True\n","    )\n","    seg_file(\n","        os.path.join(args.output_dir, args.data_split + \".txt.tmp\"),\n","        tokenizer,\n","        args.max_len,\n","    )\n","    seg_file(\n","        os.path.join(args.output_dir, args.data_split + \"_box.txt.tmp\"),\n","        tokenizer,\n","        args.max_len,\n","    )\n","    seg_file(\n","        os.path.join(args.output_dir, args.data_split + \"_image.txt.tmp\"),\n","        tokenizer,\n","        args.max_len,\n","    )\n","\n"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"nkLAfmNcvu4_","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599480752822,"user_tz":-330,"elapsed":3881,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":[""],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"B7atOrmv-eI5","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599480752823,"user_tz":-330,"elapsed":3569,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["# initilizer"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"mz4fzziAV19V","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599480752824,"user_tz":-330,"elapsed":3412,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["class preprocessing_data_input:\n","  def __init__(self,data_dir = \"data/training_data/new_annotations\",data_split = \"train\",output_dir=\"data\",model_name_or_path=\"bert-base-uncased\",max_len=510):\n","    self.data_dir = data_dir\n","    self.data_split = data_split\n","    self.output_dir = output_dir\n","    self.model_name_or_path = model_name_or_path\n","    self.max_len = max_len"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"Di7Hh2b29G0Z","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599480752827,"user_tz":-330,"elapsed":3180,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":[""],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"W-AhBI1dCYwr","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599480752828,"user_tz":-330,"elapsed":2970,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":[""],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u7kzdgl9_sXo","colab_type":"text"},"source":["TRAINING"]},{"cell_type":"code","metadata":{"id":"naj9Q2B4YGZC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599480753564,"user_tz":-330,"elapsed":3244,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["\n","args = preprocessing_data_input(\"/content/gdrive/My Drive/unilm/layoutlm/examples/seq_labeling/data/dataset/training_data/new_annotations\",\"train\",\"data\",\"bert-base-uncased\",510)\n","convert(args)\n","seg(args)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"wNfciLPZASIy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599480753569,"user_tz":-330,"elapsed":3052,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":[""],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hKpgikoVbwle","colab_type":"text"},"source":["preparing/preprocessing data for testing"]},{"cell_type":"code","metadata":{"id":"9qf_1tlOYQkj","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599480755019,"user_tz":-330,"elapsed":3349,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["args = preprocessing_data_input(\"/content/gdrive/My Drive/unilm/layoutlm/examples/seq_labeling/data/dataset/testing_data/new_annotations\",\"test\",\"data\",\"bert-base-uncased\",510)\n","convert(args)\n","seg(args)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2ZfZ5fagpqHw","colab_type":"text"},"source":["**creating labels.txt**"]},{"cell_type":"code","metadata":{"id":"sxZ2a2R3p8pl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599480755027,"user_tz":-330,"elapsed":2291,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["!cat data/train.txt | cut -d$'\\t' -f 2 | grep -v \"^$\"| sort | uniq > data/labels.txt\n"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j77A2FlUqyNh","colab_type":"text"},"source":["**train the model**"]},{"cell_type":"markdown","metadata":{"id":"1V63aG5q-LJu","colab_type":"text"},"source":["***layoutlm.py***"]},{"cell_type":"code","metadata":{"id":"ripg2WCb97Zr","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599480757692,"user_tz":-330,"elapsed":1423,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["import logging\n","\n","import torch\n","from torch import nn\n","from torch.nn import CrossEntropyLoss, MSELoss\n","from transformers import BertConfig, BertModel, BertPreTrainedModel\n","from transformers.modeling_bert import BertLayerNorm\n","\n","logger = logging.getLogger(__name__)\n","\n","LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP = {}\n","\n","LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP = {}\n","\n","\n","class LayoutlmConfig(BertConfig):\n","    pretrained_config_archive_map = LAYOUTLM_PRETRAINED_CONFIG_ARCHIVE_MAP\n","    model_type = \"bert\"\n","\n","    def __init__(self, max_2d_position_embeddings=1024, **kwargs):\n","        super().__init__(**kwargs)\n","        self.max_2d_position_embeddings = max_2d_position_embeddings\n","\n","\n","class LayoutlmEmbeddings(nn.Module): \n","    def __init__(self, config):\n","        super(LayoutlmEmbeddings, self).__init__()\n","        self.word_embeddings = nn.Embedding(\n","            config.vocab_size, config.hidden_size, padding_idx=0\n","        )\n","        self.position_embeddings = nn.Embedding(\n","            config.max_position_embeddings, config.hidden_size\n","        )\n","        self.x_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.y_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.h_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.w_position_embeddings = nn.Embedding(\n","            config.max_2d_position_embeddings, config.hidden_size\n","        )\n","        self.token_type_embeddings = nn.Embedding(\n","            config.type_vocab_size, config.hidden_size\n","        )\n","\n","        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n","        # any TensorFlow checkpoint file\n","        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        token_type_ids=None,\n","        position_ids=None,\n","        inputs_embeds=None,\n","    ):\n","        seq_length = input_ids.size(1)\n","        if position_ids is None:\n","            position_ids = torch.arange(\n","                seq_length, dtype=torch.long, device=input_ids.device\n","            )\n","            position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        words_embeddings = self.word_embeddings(input_ids)\n","        position_embeddings = self.position_embeddings(position_ids)\n","        left_position_embeddings = self.x_position_embeddings(bbox[:, :, 0])\n","        upper_position_embeddings = self.y_position_embeddings(bbox[:, :, 1])\n","        right_position_embeddings = self.x_position_embeddings(bbox[:, :, 2])\n","        lower_position_embeddings = self.y_position_embeddings(bbox[:, :, 3])\n","        h_position_embeddings = self.h_position_embeddings(\n","            bbox[:, :, 3] - bbox[:, :, 1]\n","        )\n","       # print(\"\\n bbox shape :- \",bbox.size())\n","       # print(\"\\nbbox[:,:,2] :- \", bbox[:, :, 2])\n","       # print(\"\\nbbox[:,:,2] shape :- \", bbox[:, :, 2].shape)\n","       # print(\"\\nbbox[:, :, 0] :- \", bbox[:, :, 0])\n","       # print(\"\\nbbox[:, :, 0] shape :- \", bbox[:, :, 0].shape)\n","       # print(\"\\n sub of two above matrix :- \",bbox[:, :, 2] - bbox[:, :, 0])\n","       # print(\"\\n printing the w_position_embeddings:- \",self.w_position_embeddings)\n","        bboxshape=bbox[:, :, 2] - bbox[:, :, 0]\n","       # print(\"\\n shape of the resultant sub array :- \",bboxshape.size())\n","        w_position_embeddings = self.w_position_embeddings(\n","            bbox[:, :, 2] - bbox[:, :, 0]\n","        )\n","        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n","\n","        embeddings = (\n","            words_embeddings\n","            + position_embeddings\n","            + left_position_embeddings\n","            + upper_position_embeddings\n","            + right_position_embeddings\n","            + lower_position_embeddings\n","            + h_position_embeddings\n","            + w_position_embeddings\n","            + token_type_embeddings\n","        )\n","        embeddings = self.LayerNorm(embeddings)\n","        embeddings = self.dropout(embeddings)\n","        return embeddings\n","\n","\n","class LayoutlmModel(BertModel):\n","\n","    config_class = LayoutlmConfig\n","    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, config):\n","        super(LayoutlmModel, self).__init__(config)\n","        self.embeddings = LayoutlmEmbeddings(config)\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        encoder_hidden_states=None,\n","        encoder_attention_mask=None,\n","    ):\n","        if attention_mask is None:\n","            attention_mask = torch.ones_like(input_ids)\n","        if token_type_ids is None:\n","            token_type_ids = torch.zeros_like(input_ids)\n","\n","        # We create a 3D attention mask from a 2D tensor mask.\n","        # Sizes are [batch_size, 1, 1, to_seq_length]\n","        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n","        # this attention mask is more simple than the triangular masking of causal attention\n","        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n","        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n","\n","        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n","        # masked positions, this operation will create a tensor which is 0.0 for\n","        # positions we want to attend and -10000.0 for masked positions.\n","        # Since we are adding it to the raw scores before the softmax, this is\n","        # effectively the same as removing these entirely.\n","        extended_attention_mask = extended_attention_mask.to(\n","            dtype=next(self.parameters()).dtype\n","        )  # fp16 compatibility\n","        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","\n","        # Prepare head mask if needed\n","        # 1.0 in head_mask indicate we keep the head\n","        # attention_probs has shape bsz x n_heads x N x N\n","        # input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\n","        # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n","        if head_mask is not None:\n","            if head_mask.dim() == 1:\n","                head_mask = (\n","                    head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n","                )\n","                head_mask = head_mask.expand(\n","                    self.config.num_hidden_layers, -1, -1, -1, -1\n","                )\n","            elif head_mask.dim() == 2:\n","                head_mask = (\n","                    head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n","                )  # We can specify head_mask for each layer\n","            head_mask = head_mask.to(\n","                dtype=next(self.parameters()).dtype\n","            )  # switch to fload if need + fp16 compatibility\n","        else:\n","            head_mask = [None] * self.config.num_hidden_layers\n","\n","        print(\"\\n The arguments in the embedding layer :- \\n\\n\\n\")\n","        print(\"\\n input_ids in embedding layer :- \",input_ids)\n","        print(\"\\n position_ids in embedding layer :- \",position_ids)\n","        print(\"\\n token_type_ids in embedding layer :- \",token_type_ids)\n","        print(\"\\n input_ids in embedding layer :- \\n\\n\")\n","        embedding_output = self.embeddings(\n","            input_ids, bbox, position_ids=position_ids, token_type_ids=token_type_ids\n","        )\n","        encoder_outputs = self.encoder(\n","            embedding_output, extended_attention_mask, head_mask=head_mask\n","        )\n","        sequence_output = encoder_outputs[0]\n","        pooled_output = self.pooler(sequence_output)\n","\n","        outputs = (sequence_output, pooled_output) + encoder_outputs[\n","            1:\n","        ]  # add hidden_states and attentions if they are here\n","        return outputs  # sequence_output, pooled_output, (hidden_states), (attentions)\n","\n","\n","class LayoutlmForTokenClassification(BertPreTrainedModel):\n","    config_class = LayoutlmConfig\n","    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        print('num_labels', self.num_labels)\n","        self.bert = LayoutlmModel(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids,\n","        bbox,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","    ):\n","\n","        outputs = self.bert(\n","            input_ids=input_ids,\n","            bbox=bbox,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","\n","        outputs = (logits,) + outputs[\n","            2:\n","        ]  # add hidden states and attention if they are here\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.num_labels)[active_loss]\n","                active_labels = labels.view(-1)[active_loss]\n","                loss = loss_fct(active_logits, active_labels)\n","            else:\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), scores, (hidden_states), (attentions)\n","\n","\n","class LayoutlmForSequenceClassification(BertPreTrainedModel):\n","    config_class = LayoutlmConfig\n","    pretrained_model_archive_map = LAYOUTLM_PRETRAINED_MODEL_ARCHIVE_MAP\n","    base_model_prefix = \"bert\"\n","\n","    def __init__(self, config):\n","        super(LayoutlmForSequenceClassification, self).__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.bert = LayoutlmModel(config)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        #self.classifier = nn.Linear(config.hidden_size, 10)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        self.init_weights()\n","\n","    def forward(  \n","        self,\n","        input_ids,\n","        bbox,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","    ):\n","\n","        outputs = self.bert(\n","            input_ids=input_ids,\n","            bbox=bbox,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","        )\n","\n","        pooled_output = outputs[1]\n","\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","\n","        outputs = (logits,) + outputs[\n","            2:\n","        ]  # add hidden states and attention if they are here\n","\n","        if labels is not None:\n","            if self.num_labels == 1:\n","                #  We are doing regression\n","                loss_fct = MSELoss()\n","                loss = loss_fct(logits.view(-1), labels.view(-1))\n","            else:\n","                loss_fct = CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            outputs = (loss,) + outputs\n","\n","        return outputs  # (loss), logits, (hidden_states), (attentions)\n"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x61CAAmf-hay","colab_type":"text"},"source":["***funsd.py***"]},{"cell_type":"code","metadata":{"id":"cQsu_uQj-W1l","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599480760454,"user_tz":-330,"elapsed":1741,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["import logging\n","import os\n","\n","import torch\n","from torch.utils.data import Dataset\n","\n","logger = logging.getLogger(__name__)\n","\n","\n","class FunsdDataset(Dataset):\n","    def __init__(self, args, tokenizer, labels, pad_token_label_id, mode):\n","        if args.local_rank not in [-1, 0] and mode == \"train\":\n","            torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","\n","        # Load data features from cache or dataset file\n","        cached_features_file = os.path.join(\n","            args.data_dir,\n","            \"cached_{}_{}_{}\".format(\n","                mode,\n","                list(filter(None, args.model_name_or_path.split(\"/\"))).pop(),\n","                str(args.max_seq_length),\n","            ),\n","        )\n","        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n","            logger.info(\"Loading features from cached file %s\", cached_features_file)\n","            features = torch.load(cached_features_file)\n","        else:\n","            logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n","            examples = read_examples_from_file(args.data_dir, mode)\n","            features = convert_examples_to_features(\n","                examples,\n","                labels,\n","                args.max_seq_length,\n","                tokenizer,\n","                cls_token_at_end=bool(args.model_type in [\"xlnet\"]),\n","                # xlnet has a cls token at the end\n","                cls_token=tokenizer.cls_token,\n","                cls_token_segment_id=2 if args.model_type in [\"xlnet\"] else 0,\n","                sep_token=tokenizer.sep_token,\n","                sep_token_extra=bool(args.model_type in [\"roberta\"]),\n","                # roberta uses an extra separator b/w pairs of sentences, cf. github.com/pytorch/fairseq/commit/1684e166e3da03f5b600dbb7855cb98ddfcd0805\n","                pad_on_left=bool(args.model_type in [\"xlnet\"]),\n","                # pad on the left for xlnet\n","                pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n","                pad_token_segment_id=4 if args.model_type in [\"xlnet\"] else 0,\n","                pad_token_label_id=pad_token_label_id,\n","            )\n","            if args.local_rank in [-1, 0]:\n","                logger.info(\"Saving features into cached file %s\", cached_features_file)\n","                torch.save(features, cached_features_file)\n","\n","        if args.local_rank == 0 and mode == \"train\":\n","            torch.distributed.barrier()  # Make sure only the first process in distributed training process the dataset, and the others will use the cache\n","\n","        self.features = features\n","        # Convert to Tensors and build dataset\n","        self.all_input_ids = torch.tensor(\n","            [f.input_ids for f in features], dtype=torch.long\n","        )\n","        self.all_input_mask = torch.tensor(\n","            [f.input_mask for f in features], dtype=torch.long\n","        )\n","        self.all_segment_ids = torch.tensor(\n","            [f.segment_ids for f in features], dtype=torch.long\n","        )\n","        self.all_label_ids = torch.tensor(\n","            [f.label_ids for f in features], dtype=torch.long\n","        )\n","        self.all_bboxes = torch.tensor([f.boxes for f in features], dtype=torch.long)\n","\n","    def __len__(self):\n","        return len(self.features)\n","\n","    def __getitem__(self, index):\n","        return (\n","            self.all_input_ids[index],\n","            self.all_input_mask[index],\n","            self.all_segment_ids[index],\n","            self.all_label_ids[index],\n","            self.all_bboxes[index],\n","        )\n","\n","\n","class InputExample(object):\n","    \"\"\"A single training/test example for token classification.\"\"\"\n","\n","    def __init__(self, guid, words, labels, boxes, actual_bboxes, file_name, page_size):\n","        \"\"\"Constructs a InputExample.\n","\n","        Args:\n","            guid: Unique id for the example.\n","            words: list. The words of the sequence.\n","            labels: (Optional) list. The labels for each word of the sequence. This should be\n","            specified for train and dev examples, but not for test examples.\n","        \"\"\"\n","        self.guid = guid\n","        self.words = words\n","        self.labels = labels\n","        self.boxes = boxes\n","        self.actual_bboxes = actual_bboxes\n","        self.file_name = file_name\n","        self.page_size = page_size\n","\n","\n","class InputFeatures(object):\n","    \"\"\"A single set of features of data.\"\"\"\n","\n","    def __init__(\n","        self,\n","        input_ids,\n","        input_mask,\n","        segment_ids,\n","        label_ids,\n","        boxes,\n","        actual_bboxes,\n","        file_name,\n","        page_size,\n","    ):\n","        assert (\n","            0 <= all(boxes) <= 1000\n","        ), \"Error with input bbox ({}): the coordinate value is not between 0 and 1000\".format(\n","            boxes\n","        )\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.label_ids = label_ids\n","        self.boxes = boxes\n","        self.actual_bboxes = actual_bboxes\n","        self.file_name = file_name\n","        self.page_size = page_size\n","\n","\n","def read_examples_from_file(data_dir, mode):\n","    file_path = os.path.join(data_dir, \"{}.txt\".format(mode))\n","    box_file_path = os.path.join(data_dir, \"{}_box.txt\".format(mode))\n","    image_file_path = os.path.join(data_dir, \"{}_image.txt\".format(mode))\n","    guid_index = 1\n","    examples = []\n","    with open(file_path, encoding=\"utf-8\") as f, open(\n","        box_file_path, encoding=\"utf-8\"\n","    ) as fb, open(image_file_path, encoding=\"utf-8\") as fi:\n","        words = []\n","        boxes = []\n","        actual_bboxes = []\n","        file_name = None\n","        page_size = None\n","        labels = []\n","        for line, bline, iline in zip(f, fb, fi):\n","            if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n","                if words:\n","                    examples.append(\n","                        InputExample(\n","                            guid=\"{}-{}\".format(mode, guid_index),\n","                            words=words,\n","                            labels=labels,\n","                            boxes=boxes,\n","                            actual_bboxes=actual_bboxes,\n","                            file_name=file_name,\n","                            page_size=page_size,\n","                        )\n","                    )\n","                    guid_index += 1\n","                    words = []\n","                    boxes = []\n","                    actual_bboxes = []\n","                    file_name = None\n","                    page_size = None\n","                    labels = []\n","            else:\n","                splits = line.split(\"\\t\")\n","                bsplits = bline.split(\"\\t\")\n","                isplits = iline.split(\"\\t\")\n","                assert len(splits) == 2\n","                assert len(bsplits) == 2\n","                assert len(isplits) == 4\n","                assert splits[0] == bsplits[0]\n","                words.append(splits[0])\n","                if len(splits) > 1:\n","                    labels.append(splits[-1].replace(\"\\n\", \"\"))\n","                    box = bsplits[-1].replace(\"\\n\", \"\")\n","                    box = [int(b) for b in box.split()]\n","                    boxes.append(box)\n","                    actual_bbox = [int(b) for b in isplits[1].split()]\n","                    actual_bboxes.append(actual_bbox)\n","                    page_size = [int(i) for i in isplits[2].split()]\n","                    file_name = isplits[3].strip()\n","                else:\n","                    # Examples could have no label for mode = \"test\"\n","                    labels.append(\"O\")\n","        if words:\n","            examples.append(\n","                InputExample(\n","                    guid=\"%s-%d\".format(mode, guid_index),\n","                    words=words,\n","                    labels=labels,\n","                    boxes=boxes,\n","                    actual_bboxes=actual_bboxes,\n","                    file_name=file_name,\n","                    page_size=page_size,\n","                )\n","            )\n","    return examples\n","\n","\n","def convert_examples_to_features(\n","    examples,\n","    label_list,\n","    max_seq_length,\n","    tokenizer,\n","    cls_token_at_end=False,\n","    cls_token=\"[CLS]\",\n","    cls_token_segment_id=1,\n","    sep_token=\"[SEP]\",\n","    sep_token_extra=False,\n","    pad_on_left=False,\n","    pad_token=0,\n","    cls_token_box=[0, 0, 0, 0],\n","    sep_token_box=[1000, 1000, 1000, 1000],\n","    pad_token_box=[0, 0, 0, 0],\n","    pad_token_segment_id=0,\n","    pad_token_label_id=-1,\n","    sequence_a_segment_id=0,\n","    mask_padding_with_zero=True,\n","):\n","    \"\"\" Loads a data file into a list of `InputBatch`s\n","        `cls_token_at_end` define the location of the CLS token:\n","            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n","            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n","        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n","    \"\"\"\n","\n","    label_map = {label: i for i, label in enumerate(label_list)}\n","\n","    features = []\n","    for (ex_index, example) in enumerate(examples):\n","        file_name = example.file_name\n","        page_size = example.page_size\n","        width, height = page_size\n","        if ex_index % 10000 == 0:\n","            logger.info(\"Writing example %d of %d\", ex_index, len(examples))\n","\n","        tokens = []\n","        token_boxes = []\n","        actual_bboxes = []\n","        label_ids = []\n","        for word, label, box, actual_bbox in zip(\n","            example.words, example.labels, example.boxes, example.actual_bboxes\n","        ):\n","            word_tokens = tokenizer.tokenize(word)\n","            tokens.extend(word_tokens)\n","            token_boxes.extend([box] * len(word_tokens))\n","            actual_bboxes.extend([actual_bbox] * len(word_tokens))\n","            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n","            label_ids.extend(\n","                [label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1)\n","            )\n","\n","        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n","        special_tokens_count = 3 if sep_token_extra else 2\n","        if len(tokens) > max_seq_length - special_tokens_count:\n","            tokens = tokens[: (max_seq_length - special_tokens_count)]\n","            token_boxes = token_boxes[: (max_seq_length - special_tokens_count)]\n","            actual_bboxes = actual_bboxes[: (max_seq_length - special_tokens_count)]\n","            label_ids = label_ids[: (max_seq_length - special_tokens_count)]\n","\n","        # The convention in BERT is:\n","        # (a) For sequence pairs:\n","        #  tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n","        #  type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n","        # (b) For single sequences:\n","        #  tokens:   [CLS] the dog is hairy . [SEP]\n","        #  type_ids:   0   0   0   0  0     0   0\n","        #\n","        # Where \"type_ids\" are used to indicate whether this is the first\n","        # sequence or the second sequence. The embedding vectors for `type=0` and\n","        # `type=1` were learned during pre-training and are added to the wordpiece\n","        # embedding vector (and position vector). This is not *strictly* necessary\n","        # since the [SEP] token unambiguously separates the sequences, but it makes\n","        # it easier for the model to learn the concept of sequences.\n","        #\n","        # For classification tasks, the first vector (corresponding to [CLS]) is\n","        # used as as the \"sentence vector\". Note that this only makes sense because\n","        # the entire model is fine-tuned.\n","        tokens += [sep_token]\n","        token_boxes += [sep_token_box]\n","        actual_bboxes += [[0, 0, width, height]]\n","        label_ids += [pad_token_label_id]\n","        if sep_token_extra:\n","            # roberta uses an extra separator b/w pairs of sentences\n","            tokens += [sep_token]\n","            token_boxes += [sep_token_box]\n","            actual_bboxes += [[0, 0, width, height]]\n","            label_ids += [pad_token_label_id]\n","        segment_ids = [sequence_a_segment_id] * len(tokens)\n","\n","        if cls_token_at_end:\n","            tokens += [cls_token]\n","            token_boxes += [cls_token_box]\n","            actual_bboxes += [[0, 0, width, height]]\n","            label_ids += [pad_token_label_id]\n","            segment_ids += [cls_token_segment_id]\n","        else:\n","            tokens = [cls_token] + tokens\n","            token_boxes = [cls_token_box] + token_boxes\n","            actual_bboxes = [[0, 0, width, height]] + actual_bboxes\n","            label_ids = [pad_token_label_id] + label_ids\n","            segment_ids = [cls_token_segment_id] + segment_ids\n","\n","        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","\n","        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n","        # tokens are attended to.\n","        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n","\n","        # Zero-pad up to the sequence length.\n","        padding_length = max_seq_length - len(input_ids)\n","        if pad_on_left:\n","            input_ids = ([pad_token] * padding_length) + input_ids\n","            input_mask = (\n","                [0 if mask_padding_with_zero else 1] * padding_length\n","            ) + input_mask\n","            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n","            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n","            token_boxes = ([pad_token_box] * padding_length) + token_boxes\n","        else:\n","            input_ids += [pad_token] * padding_length\n","            input_mask += [0 if mask_padding_with_zero else 1] * padding_length\n","            segment_ids += [pad_token_segment_id] * padding_length\n","            label_ids += [pad_token_label_id] * padding_length\n","            token_boxes += [pad_token_box] * padding_length\n","\n","        assert len(input_ids) == max_seq_length\n","        assert len(input_mask) == max_seq_length\n","        assert len(segment_ids) == max_seq_length\n","        assert len(label_ids) == max_seq_length\n","        assert len(token_boxes) == max_seq_length\n","\n","        if ex_index < 5:\n","            logger.info(\"*** Example ***\")\n","            logger.info(\"guid: %s\", example.guid)\n","            logger.info(\"tokens: %s\", \" \".join([str(x) for x in tokens]))\n","            logger.info(\"input_ids: %s\", \" \".join([str(x) for x in input_ids]))\n","            logger.info(\"input_mask: %s\", \" \".join([str(x) for x in input_mask]))\n","            logger.info(\"segment_ids: %s\", \" \".join([str(x) for x in segment_ids]))\n","            logger.info(\"label_ids: %s\", \" \".join([str(x) for x in label_ids]))\n","            logger.info(\"boxes: %s\", \" \".join([str(x) for x in token_boxes]))\n","            logger.info(\"actual_bboxes: %s\", \" \".join([str(x) for x in actual_bboxes]))\n","\n","        features.append(\n","            InputFeatures(\n","                input_ids=input_ids,\n","                input_mask=input_mask,\n","                segment_ids=segment_ids,\n","                label_ids=label_ids,\n","                boxes=token_boxes,\n","                actual_bboxes=actual_bboxes,\n","                file_name=file_name,\n","                page_size=page_size,\n","            )\n","        )\n","    return features\n"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zVJQt2N0q46e","colab_type":"text"},"source":["****run_sequence_labeling.py****"]},{"cell_type":"code","metadata":{"id":"T5CKUcz8q4ao","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599480763025,"user_tz":-330,"elapsed":2296,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["\"\"\" Fine-tuning the library models for named entity recognition on CoNLL-2003 (Bert or Roberta). \"\"\"\n","\n","from __future__ import absolute_import, division, print_function\n","\n","import argparse\n","import glob\n","import logging\n","import os\n","import random\n","import shutil\n","\n","import numpy as np\n","import torch\n","from seqeval.metrics import (\n","    classification_report,\n","    f1_score,\n","    precision_score,\n","    recall_score,\n",")\n","from tensorboardX import SummaryWriter\n","from torch.nn import CrossEntropyLoss\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from torch.utils.data.distributed import DistributedSampler\n","from tqdm import tqdm, trange\n","from transformers import (\n","    WEIGHTS_NAME,\n","    AdamW,\n","    BertConfig,\n","    BertForTokenClassification,\n","    BertTokenizer,\n","    RobertaConfig,\n","    RobertaForTokenClassification,\n","    RobertaTokenizer,\n","    get_linear_schedule_with_warmup,\n",")\n","'''\n","Other file imports {take only important files for deployment}\n","'''\n","#from layoutlm import FunsdDataset, LayoutlmConfig, LayoutlmForTokenClassification\n","\n","logger = logging.getLogger(__name__)\n","\n","ALL_MODELS = sum(\n","    (\n","        tuple(conf.pretrained_config_archive_map.keys())\n","        for conf in (BertConfig, RobertaConfig, LayoutlmConfig)\n","    ),\n","    (),\n",")\n","\n","MODEL_CLASSES = {\n","    \"bert\": (BertConfig, BertForTokenClassification, BertTokenizer),\n","    \"roberta\": (RobertaConfig, RobertaForTokenClassification, RobertaTokenizer),\n","    \"layoutlm\": (LayoutlmConfig, LayoutlmForTokenClassification, BertTokenizer),\n","}\n","\n","\n","def set_seed(args):\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    if args.n_gpu > 0:\n","        torch.cuda.manual_seed_all(args.seed)\n","\n","\n","def collate_fn(data):\n","    batch = [i for i in zip(*data)]\n","    for i in range(len(batch)):\n","        if i < len(batch) - 2:\n","            batch[i] = torch.stack(batch[i], 0)\n","    return tuple(batch)\n","\n","\n","def get_labels(path):\n","    with open(path, \"r\") as f:\n","        labels = f.read().splitlines()\n","    if \"O\" not in labels:\n","        labels = [\"O\"] + labels\n","    return labels\n","\n","\n","def train(  # noqa C901\n","    args, train_dataset, model, tokenizer, labels, pad_token_label_id\n","):\n","    \"\"\" Train the model \"\"\"\n","    if args.local_rank in [-1, 0]:\n","        tb_writer = SummaryWriter(logdir=\"runs/\" + os.path.basename(args.output_dir))\n","\n","    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n","    train_sampler = (\n","        RandomSampler(train_dataset)\n","        if args.local_rank == -1\n","        else DistributedSampler(train_dataset)\n","    )\n","    train_dataloader = DataLoader(\n","        train_dataset,\n","        sampler=train_sampler,\n","        batch_size=args.train_batch_size,\n","        collate_fn=None,\n","    )\n","\n","    if args.max_steps > 0:\n","        t_total = args.max_steps\n","        args.num_train_epochs = (\n","            args.max_steps\n","            // (len(train_dataloader) // args.gradient_accumulation_steps)\n","            + 1\n","        )\n","    else:\n","        t_total = (\n","            len(train_dataloader)\n","            // args.gradient_accumulation_steps\n","            * args.num_train_epochs\n","        )\n","\n","    # Prepare optimizer and schedule (linear warmup and decay)\n","    no_decay = [\"bias\", \"LayerNorm.weight\"]\n","    optimizer_grouped_parameters = [\n","        {\n","            \"params\": [\n","                p\n","                for n, p in model.named_parameters()\n","                if not any(nd in n for nd in no_decay)\n","            ],\n","            \"weight_decay\": args.weight_decay,\n","        },\n","        {\n","            \"params\": [\n","                p\n","                for n, p in model.named_parameters()\n","                if any(nd in n for nd in no_decay)\n","            ],\n","            \"weight_decay\": 0.0,\n","        },\n","    ]\n","    optimizer = AdamW(\n","        optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon\n","    )\n","    scheduler = get_linear_schedule_with_warmup(\n","        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n","    )\n","    if args.fp16:\n","        try:\n","            from apex import amp\n","        except ImportError:\n","            raise ImportError(\n","                \"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\"\n","            )\n","        model, optimizer = amp.initialize(\n","            model, optimizer, opt_level=args.fp16_opt_level\n","        )\n","\n","    # multi-gpu training (should be after apex fp16 initialization)\n","    if args.n_gpu > 1:\n","        model = torch.nn.DataParallel(model)\n","\n","    # Distributed training (should be after apex fp16 initialization)\n","    if args.local_rank != -1:\n","        model = torch.nn.parallel.DistributedDataParallel(\n","            model,\n","            device_ids=[args.local_rank],\n","            output_device=args.local_rank,\n","            find_unused_parameters=True,\n","        )\n","\n","    # Train!\n","    logger.info(\"***** Running training *****\")\n","    logger.info(\"  Num examples = %d\", len(train_dataset))\n","    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n","    logger.info(\n","        \"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size\n","    )\n","    logger.info(\n","        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n","        args.train_batch_size\n","        * args.gradient_accumulation_steps\n","        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n","    )\n","    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n","    logger.info(\"  Total optimization steps = %d\", t_total)\n","\n","    global_step = 0\n","    tr_loss, logging_loss = 0.0, 0.0\n","    model.zero_grad()\n","    train_iterator = trange(\n","        int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n","    )\n","    set_seed(args)  # Added here for reproductibility (even between python 2 and 3)\n","    for _ in train_iterator:\n","        epoch_iterator = tqdm(\n","            train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0]\n","        )\n","        for step, batch in enumerate(epoch_iterator):\n","            model.train()\n","            inputs = {\n","                \"input_ids\": batch[0].to(args.device),\n","                \"attention_mask\": batch[1].to(args.device),\n","                \"labels\": batch[3].to(args.device),\n","            }\n","            if args.model_type in [\"layoutlm\"]:\n","                inputs[\"bbox\"] = batch[4].to(args.device)\n","            inputs[\"token_type_ids\"] = (\n","                batch[2].to(args.device) if args.model_type in [\"bert\", \"layoutlm\"] else None\n","            )  # RoBERTa don\"t use segment_ids\n","            outputs = model(**inputs)\n","            # model outputs are always tuple in pytorch-transformers (see doc)\n","            loss = outputs[0]\n","\n","            if args.n_gpu > 1:\n","                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n","            if args.gradient_accumulation_steps > 1:\n","                loss = loss / args.gradient_accumulation_steps\n","\n","            if args.fp16:\n","                with amp.scale_loss(loss, optimizer) as scaled_loss:\n","                    scaled_loss.backward()\n","            else:\n","                loss.backward()\n","\n","            tr_loss += loss.item()\n","            if (step + 1) % args.gradient_accumulation_steps == 0:\n","                if args.fp16:\n","                    torch.nn.utils.clip_grad_norm_(\n","                        amp.master_params(optimizer), args.max_grad_norm\n","                    )\n","                else:\n","                    torch.nn.utils.clip_grad_norm_(\n","                        model.parameters(), args.max_grad_norm\n","                    )\n","                optimizer.step()\n","                scheduler.step()  # Update learning rate schedule\n","                model.zero_grad()\n","                global_step += 1\n","\n","                if (\n","                    args.local_rank in [-1, 0]\n","                    and args.logging_steps > 0\n","                    and global_step % args.logging_steps == 0\n","                ):\n","                    # Log metrics\n","                    if (\n","                        args.local_rank in [-1, 0] and args.evaluate_during_training\n","                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n","                        results, _ = evaluate(\n","                            args,\n","                            model,\n","                            tokenizer,\n","                            labels,\n","                            pad_token_label_id,\n","                            mode=\"dev\",\n","                        )\n","                        for key, value in results.items():\n","                            tb_writer.add_scalar(\n","                                \"eval_{}\".format(key), value, global_step\n","                            )\n","                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n","                    tb_writer.add_scalar(\n","                        \"loss\",\n","                        (tr_loss - logging_loss) / args.logging_steps,\n","                        global_step,\n","                    )\n","                    logging_loss = tr_loss\n","\n","                if (\n","                    args.local_rank in [-1, 0]\n","                    and args.save_steps > 0\n","                    and global_step % args.save_steps == 0\n","                ):\n","                    # Save model checkpoint\n","                    output_dir = os.path.join(\n","                        args.output_dir, \"checkpoint-{}\".format(global_step)\n","                    )\n","                    if not os.path.exists(output_dir):\n","                        os.makedirs(output_dir)\n","                    model_to_save = (\n","                        model.module if hasattr(model, \"module\") else model\n","                    )  # Take care of distributed/parallel training\n","                    model_to_save.save_pretrained(output_dir)\n","                    tokenizer.save_pretrained(output_dir)\n","                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n","                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n","\n","            if args.max_steps > 0 and global_step > args.max_steps:\n","                epoch_iterator.close()\n","                break\n","        if args.max_steps > 0 and global_step > args.max_steps:\n","            train_iterator.close()\n","            break\n","\n","    if args.local_rank in [-1, 0]:\n","        tb_writer.close()\n","\n","    return global_step, tr_loss / global_step\n","\n","\n","def evaluate(args, model, tokenizer, labels, pad_token_label_id, mode, prefix=\"\"):\n","    eval_dataset = FunsdDataset(args, tokenizer, labels, pad_token_label_id, mode=mode)\n","\n","    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n","    eval_sampler = SequentialSampler(eval_dataset)\n","    eval_dataloader = DataLoader(\n","        eval_dataset,\n","        sampler=eval_sampler,\n","        batch_size=args.eval_batch_size,\n","        collate_fn=None,\n","    )\n","\n","    # Eval!\n","    logger.info(\"***** Running evaluation %s *****\", prefix)\n","    logger.info(\"  Num examples = %d\", len(eval_dataset))\n","    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n","    eval_loss = 0.0\n","    nb_eval_steps = 0\n","    preds = None\n","    out_label_ids = None\n","    model.eval()\n","    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n","        with torch.no_grad():\n","            inputs = {\n","                \"input_ids\": batch[0].to(args.device),\n","                \"attention_mask\": batch[1].to(args.device),\n","                \"labels\": batch[3].to(args.device),\n","            }\n","            if args.model_type in [\"layoutlm\"]:\n","                inputs[\"bbox\"] = batch[4].to(args.device)\n","            inputs[\"token_type_ids\"] = (\n","                batch[2].to(args.device)\n","                if args.model_type in [\"bert\", \"layoutlm\"]\n","                else None\n","            )  # RoBERTa don\"t use segment_ids\n","           # print(\"\\n\\n lenght :- \",len(inputs))\n","           # print(\"\\n\\n keys :- \",inputs.keys())\n","           # print(\"\\n\\n\\n\\n\",)\n","            outputs = model(**inputs)\n","            tmp_eval_loss, logits = outputs[:2]\n","\n","            if args.n_gpu > 1:\n","                tmp_eval_loss = (\n","                    tmp_eval_loss.mean()\n","                )  # mean() to average on multi-gpu parallel evaluating\n","\n","            eval_loss += tmp_eval_loss.item()\n","        nb_eval_steps += 1\n","        if preds is None:\n","            preds = logits.detach().cpu().numpy()\n","            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n","        else:\n","            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n","            out_label_ids = np.append(\n","                out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0\n","            )\n","\n","    eval_loss = eval_loss / nb_eval_steps\n","    preds = np.argmax(preds, axis=2)\n","\n","    label_map = {i: label for i, label in enumerate(labels)}\n","\n","    out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n","    preds_list = [[] for _ in range(out_label_ids.shape[0])]\n","\n","    for i in range(out_label_ids.shape[0]):\n","        for j in range(out_label_ids.shape[1]):\n","            if out_label_ids[i, j] != pad_token_label_id:\n","                out_label_list[i].append(label_map[out_label_ids[i][j]])\n","                preds_list[i].append(label_map[preds[i][j]])\n","\n","    results = {\n","        \"loss\": eval_loss,\n","        \"precision\": precision_score(out_label_list, preds_list),\n","        \"recall\": recall_score(out_label_list, preds_list),\n","        \"f1\": f1_score(out_label_list, preds_list),\n","    }\n","\n","    report = classification_report(out_label_list, preds_list)\n","    logger.info(\"\\n\" + report)\n","\n","    logger.info(\"***** Eval results %s *****\", prefix)\n","    for key in sorted(results.keys()):\n","        logger.info(\"  %s = %s\", key, str(results[key]))\n","\n","    return results, preds_list"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"AmJKaB49d-rC","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599480764551,"user_tz":-330,"elapsed":1326,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["class sequence_labelling_data_input:\n","  def __init__(self,data_dir = None,model_type = None,model_name_or_path = None,output_dir = None,labels = \"\",config_name = \"\",tokenizer_name=\"\",cache_dir = \"\",max_seq_length = 512,do_train = False,do_eval = False,do_predict = False,evaluate_during_training = False,do_lower_case = False,per_gpu_train_batch_size = 8,per_gpu_eval_batch_size=8,gradient_accumulation_steps = 1,learning_rate = 5e-5,weight_decay = 0.0,adam_epsilon = 1e-8,max_grad_norm = 1.0,num_train_epochs = 3.0,max_steps = -1,warmup_steps = 0,logging_steps = 50,eval_all_checkpoins = None ,no_cuda = False,overwrite_output_dir = False,overwrite_cache = False,seed = 42,fp16 = False,fp16_opt_level = \"01\",local_rank = -1,server_ip = \"\",server_port = \"\",save_steps = -1):\n","    #The input data dir. Should contain the training files for the CoNLL-2003 NER task.\n","    self.data_dir = data_dir\n","    #Model type selected in the list: \n","    self.model_type = model_type\n","    #Path to pre-trained model or shortcut name selected in the list: \n","    self.model_name_or_path = model_name_or_path \n","    #The output directory where the model predictions and checkpoints will be written.\n","    self.output_dir = output_dir\n","    #Path to a file containing all labels. If not specified, CoNLL-2003 labels are used\n","    self.labels = labels\n","    #Pretrained config name or path if not the same as model_name\n","    self.config_name = config_name\n","    #Pretrained tokenizer name or path if not the same as model_name\n","    self.tokenizer_name = tokenizer_name\n","    #Where do you want to store the pre-trained models downloaded from s3\n","    self.cache_dir = cache_dir\n","    #The maximum total input sequence length after tokenization. Sequences longer \"than this will be truncated, sequences shorter will be padded.\"\n","    self.max_seq_length = max_seq_length\n","    #Whether to run training.\n","    self.do_train = do_train\n","    #Whether to run eval on the dev set.\n","    self.do_eval = do_eval\n","    #Whether to run predictions on the test set.\n","    self.do_predict = do_predict\n","    #Whether to run evaluation during training at each logging step.\n","    self.evaluate_during_training = evaluate_during_training\n","    #Set this flag if you are using an uncased model.\n","    self.do_lower_case = do_lower_case\n","    #Batch size per GPU/CPU for training.\n","    self.per_gpu_train_batch_size = per_gpu_train_batch_size\n","    #Batch size per GPU/CPU for evaluation.\n","    self.per_gpu_eval_batch_size = per_gpu_eval_batch_size\n","    #Number of updates steps to accumulate before performing a backward/update pass.\n","    self.gradient_accumulation_steps = gradient_accumulation_steps\n","    #The initial learning rate for Adam.\n","    self.learning_rate = learning_rate\n","    #Weight decay if we apply some.\n","    self.weight_decay = weight_decay\n","    #Epsilon for Adam optimizer.\n","    self.adam_epsilon = adam_epsilon\n","    #Max gradient norm.\n","    self.max_grad_norm = max_grad_norm\n","    #Total number of training epochs to perform.\n","    self.num_train_epochs = num_train_epochs\n","    #If > 0: set total number of training steps to perform. Override num_train_epochs.\n","    self.max_steps = max_steps\n","    #Linear warmup over warmup_steps.\n","    self.warmup_steps = warmup_steps\n","    #Log every X updates steps.\n","    self.logging_steps = logging_steps\n","    #Save checkpoint every X updates steps.\n","    self.save_steps = save_steps\n","    #Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number\n","    self.eval_all_checkpoints = False\n","    #Avoid using CUDA when available\n","    self.no_cuda = False\n","    #Overwrite the content of the output directory\n","    self.overwrite_output_dir =False\n","    #Overwrite the cached training and evaluation sets\n","    self.overwrite_cache = overwrite_cache\n","    #random seed for initialization\n","    self.seed = seed\n","    #Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\n","    self.fp16 = fp16\n","    #For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\"See details at https://nvidia.github.io/apex/amp.html\"\n","    self.fp16_opt_level = fp16_opt_level\n","    #For distributed training: local_rank\n","    self.local_rank = local_rank\n","    #For distant debugging.\n","    self.server_ip = server_ip\n","    #For distant debugging.\n","    self.server_port = server_port\n","\n","#args = parser.parse_args()"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I99OAm9i2llC","colab_type":"text"},"source":["**creating the object for training**"]},{"cell_type":"code","metadata":{"id":"bCftw30m3LGD","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599480775061,"user_tz":-330,"elapsed":1527,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}}},"source":["args=sequence_labelling_data_input(data_dir='data',model_type = \"layoutlm\",model_name_or_path = \"bert-base-uncased\",do_lower_case = True,max_seq_length = 512,do_train = True,num_train_epochs = 1,overwrite_output_dir = True,logging_steps = 1,save_steps = -1,output_dir = 'output_dir96',labels = '/content/gdrive/My Drive/unilm/layoutlm/examples/seq_labeling/data/labels.txt' ,per_gpu_train_batch_size = 1,per_gpu_eval_batch_size = 1)"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"em8XAsSBwT8V","colab_type":"code","colab":{}},"source":["def main(args):  # noqa C901\n","    if (\n","        os.path.exists(args.output_dir)\n","        and os.listdir(args.output_dir)\n","        and args.do_train\n","    ):\n","        if not args.overwrite_output_dir:\n","            raise ValueError(\n","                \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n","                    args.output_dir\n","                )\n","            )\n","        else:\n","            if args.local_rank in [-1, 0]:\n","                shutil.rmtree(args.output_dir)\n","\n","    if not os.path.exists(args.output_dir) and (args.do_eval or args.do_predict):\n","        raise ValueError(\n","            \"Output directory ({}) does not exist. Please train and save the model before inference stage.\".format(\n","                args.output_dir\n","            )\n","        )\n","\n","    if (\n","        not os.path.exists(args.output_dir)\n","        and args.do_train\n","        and args.local_rank in [-1, 0]\n","    ):\n","        os.makedirs(args.output_dir)\n","\n","    # Setup distant debugging if needed\n","    if args.server_ip and args.server_port:\n","        # Distant debugging - see https://code.visualstudio.com/docs/python/debugging#_attach-to-a-local-script\n","        import ptvsd\n","\n","     #   print(\"Waiting for debugger attach\")\n","        ptvsd.enable_attach(\n","            address=(args.server_ip, args.server_port), redirect_output=True\n","        )\n","        ptvsd.wait_for_attach()\n","\n","    # Setup CUDA, GPU & distributed training\n","    if args.local_rank == -1 or args.no_cuda:\n","        device = torch.device(\n","            \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n","        )\n","        args.n_gpu = 0 if args.no_cuda else torch.cuda.device_count()\n","    else:  # Initializes the distributed backend which will take care of sychronizing nodes/GPUs\n","        torch.cuda.set_device(args.local_rank)\n","        device = torch.device(\"cuda\", args.local_rank)\n","        torch.distributed.init_process_group(backend=\"nccl\")\n","        args.n_gpu = 1\n","    args.device = device\n","\n","    # Setup logging\n","    logging.basicConfig(\n","        filename=os.path.join(args.output_dir, \"train.log\")\n","        if args.local_rank in [-1, 0]\n","        else None,\n","        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n","        datefmt=\"%m/%d/%Y %H:%M:%S\",\n","        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n","    )\n","    logger.warning(\n","        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n","        args.local_rank,\n","        device,\n","        args.n_gpu,\n","        bool(args.local_rank != -1),\n","        args.fp16,\n","    )\n","\n","    # Set seed\n","    set_seed(args)\n","\n","    labels = get_labels(args.labels)\n","    num_labels = len(labels)\n","    # Use cross entropy ignore index as padding label id so that only real label ids contribute to the loss later\n","    pad_token_label_id = CrossEntropyLoss().ignore_index\n","\n","    # Load pretrained model and tokenizer\n","    if args.local_rank not in [-1, 0]:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n","\n","    args.model_type = args.model_type.lower()\n","    config_class, model_class, tokenizer_class = MODEL_CLASSES[args.model_type]\n","    config = config_class.from_pretrained(\n","        args.config_name if args.config_name else args.model_name_or_path,\n","        num_labels=num_labels,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    tokenizer = tokenizer_class.from_pretrained(\n","        args.tokenizer_name if args.tokenizer_name else args.model_name_or_path,\n","        do_lower_case=args.do_lower_case,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","    model = model_class.from_pretrained(\n","        args.model_name_or_path,\n","        from_tf=bool(\".ckpt\" in args.model_name_or_path),\n","        config=config,\n","        cache_dir=args.cache_dir if args.cache_dir else None,\n","    )\n","\n","    if args.local_rank == 0:\n","        torch.distributed.barrier()  # Make sure only the first process in distributed training will download model & vocab\n","\n","    model.to(args.device)\n","\n","    logger.info(\"Training/evaluation parameters %s\", args)\n","\n","    \n","    \n","    # Training\n","    if args.do_train:\n","        train_dataset = FunsdDataset(\n","            args, tokenizer, labels, pad_token_label_id, mode=\"train\"\n","        )\n","        global_step, tr_loss = train(\n","            args, train_dataset, model, tokenizer, labels, pad_token_label_id\n","        )\n","        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n","\n","    \n","    \n","    # Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","    if args.do_train and (args.local_rank == -1 or torch.distributed.get_rank() == 0):\n","        # Create output directory if needed\n","        if not os.path.exists(args.output_dir) and args.local_rank in [-1, 0]:\n","            os.makedirs(args.output_dir)\n","\n","        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n","        # Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","        # They can then be reloaded using `from_pretrained()`\n","        model_to_save = (\n","            model.module if hasattr(model, \"module\") else model\n","        )  # Take care of distributed/parallel training\n","        model_to_save.save_pretrained(args.output_dir)\n","        tokenizer.save_pretrained(args.output_dir)\n","\n","        # Good practice: save your training arguments together with the trained model\n","        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n","\n","    \n","    \n","    \n","    # Evaluation\n","    results = {}\n","    if args.do_eval and args.local_rank in [-1, 0]:\n","        tokenizer = tokenizer_class.from_pretrained(\n","            args.output_dir, do_lower_case=args.do_lower_case\n","        )\n","        checkpoints = [args.output_dir]\n","        if args.eval_all_checkpoints:\n","            checkpoints = list(\n","                os.path.dirname(c)\n","                for c in sorted(\n","                    glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True)\n","                )\n","            )\n","            logging.getLogger(\"pytorch_transformers.modeling_utils\").setLevel(\n","                logging.WARN\n","            )  # Reduce logging\n","        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n","        for checkpoint in checkpoints:\n","            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n","            model = model_class.from_pretrained(checkpoint)\n","            model.to(args.device)\n","            result, _ = evaluate(\n","                args,\n","                model,\n","                tokenizer,\n","                labels,\n","                pad_token_label_id,\n","                mode=\"test\",\n","                prefix=global_step,\n","            )\n","            if global_step:\n","                result = {\"{}_{}\".format(global_step, k): v for k, v in result.items()}\n","            results.update(result)\n","        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n","        with open(output_eval_file, \"w\") as writer:\n","            for key in sorted(results.keys()):\n","                writer.write(\"{} = {}\\n\".format(key, str(results[key])))\n","\n","    \n","    \n","    # do predict part\n","    if args.do_predict and args.local_rank in [-1, 0]:\n","        tokenizer = tokenizer_class.from_pretrained(\n","            args.model_name_or_path, do_lower_case=args.do_lower_case\n","        )\n","        model = model_class.from_pretrained(args.output_dir)\n","        model.to(args.device)\n","       # print(\"\\n evaluate function parameter1 args: - \", args)\n","        #print(\"\\n evaluate function parameter2 model : - \", model)\n","        #print(\"\\n evaluate function parameters3 tokenizer :- \", tokenizer)\n","        #print(\"\\n evaluate function parameter4 labels : - \", labels)\n","        #print(\"\\n  evaluate fnctions parameter 5 pad_token_label\", pad_token_label_id)\n","        result, predictions = evaluate(\n","            args, model, tokenizer, labels, pad_token_label_id, mode=\"test\"\n","        )\n","        # Save results\n","        output_test_results_file = os.path.join(args.output_dir, \"test_results.txt\")\n","        with open(output_test_results_file, \"w\") as writer:\n","            for key in sorted(result.keys()):\n","                writer.write(\"{} = {}\\n\".format(key, str(result[key])))\n","        # Save predictions\n","        output_test_predictions_file = os.path.join(\n","            args.output_dir, \"test_predictions.txt\"\n","        )\n","        with open(output_test_predictions_file, \"w\", encoding=\"utf8\") as writer:\n","            with open(\n","                os.path.join(args.data_dir, \"test.txt\"), \"r\", encoding=\"utf8\"\n","            ) as f:\n","                example_id = 0\n","                for line in f:\n","                    if line.startswith(\"-DOCSTART-\") or line == \"\" or line == \"\\n\":\n","                        writer.write(line)\n","                        if not predictions[example_id]:\n","                            example_id += 1\n","                    elif predictions[example_id]:\n","                        output_line = (\n","                            line.split()[0]\n","                            + \" \"\n","                            + predictions[example_id].pop(0)\n","                            + \"\\n\"\n","                        )\n","                        writer.write(output_line)\n","                    else:\n","                        logger.warning(\n","                            \"Maximum sequence length exceeded: No prediction for '%s'.\",\n","                            line.split()[0],\n","                        )\n","\n","    return results"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dyuDpXgT8E4u","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IKnlp0Emwdyj","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1599475031654,"user_tz":-330,"elapsed":11628,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"4059a209-091e-4733-c8f4-70215751b7be"},"source":["if __name__ == \"__main__\":\n","    main(args)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["num_labels 10\n"],"name":"stdout"},{"output_type":"stream","text":["\n","\n","\n","\n","\n","\n","\n","\n","Epoch:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","\n","\n","\n","\n","\n","Iteration:   0%|          | 0/150 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"],"name":"stderr"},{"output_type":"stream","text":["\n"," The arguments in the embedding layer :- \n","\n","\n","\n","\n"," input_ids in embedding layer :-  tensor([[  101,  2013,  1024,  2000,  1024, 10507,  1024,  2865,  2686,  1013,\n","          3609,  9141,  1049,  1012,  8945,  2869,  5498,  8648,  2239,  1051,\n","         26775,  1011, 11703,  2184,  1013,  2340,  1013,  6109,  1020,  1013,\n","          2382,  1013,  6365,  2120,  1002,  1012,  2753,  5308,  1022,  1013,\n","          5757,  1013,  6109,  6391, 19841,  6205,  2575,  3770,  1003,  6205,\n","         23499, 17134, 10790,  3884,  1038,  8648,  2239,  3642,  8819,  2433,\n","         12159,  3050,  9956,  1056,  1012,  3016, 11983,  1010,  1048,  1012,\n","         21025,  8551,  6761,  1058,  1012, 11409,  5104,  3051,  1010,  1046,\n","          1012,  8040,  7295, 12879,  2546,  1010,  1055,  1012,  9680, 14268,\n","         12190,  4305,  1010,  1050,  1010,  5860,  2368,  4143,  1041, 21860,\n","          4059, 13653,  1011, 12017, 26718,  2015, 18133,  3619,  9639,  1006,\n","          1055,  1007, 12711, 13653,  1011, 12017,  3277,  6075,  1013,  2095,\n","          8648,  2239,  3277,  3058,  8648,  2239,  4654, 16781,  3058,  1016,\n","          1010,  5594,  2475,  1010,  2531,  1011,  4700,  1012,  3263, 10056,\n","          2181,  1006,  1055,  1007,  8648,  2239,  3643,  5308,  1998,  2030,\n","          1013, 11122,  2239,  6475,  5541,  4323,  8085,  1997,  1999, 29050,\n","          4263,  3058,  7531, 17826,  5918,  1024,  2005,  2491,  2224,  2069,\n","          1024,  3642,  4137,  1024,  3105,  2193,  1024,  9765,  1010, 18434,\n","          1024,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n","             0,     0]])\n","\n"," position_ids in embedding layer :-  None\n","\n"," token_type_ids in embedding layer :-  tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","         0, 0, 0, 0, 0, 0, 0, 0]])\n","\n"," input_ids in embedding layer :- \n","\n","\n"],"name":"stdout"},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-303-ed4bd21d196e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-302-7232dbe67ee9>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    117\u001b[0m         )\n\u001b[1;32m    118\u001b[0m         global_step, tr_loss = train(\n\u001b[0;32m--> 119\u001b[0;31m             \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad_token_label_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         )\n\u001b[1;32m    121\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\" global_step = %s, average loss = %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtr_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-288-0258346bc2d5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, train_dataset, model, tokenizer, labels, pad_token_label_id)\u001b[0m\n\u001b[1;32m    203\u001b[0m                 \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_type\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"bert\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"layoutlm\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             )  # RoBERTa don\"t use segment_ids\n\u001b[0;32m--> 205\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m             \u001b[0;31m# model outputs are always tuple in pytorch-transformers (see doc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-286-1683b16216b7>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, bbox, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels)\u001b[0m\n\u001b[1;32m    247\u001b[0m                 \u001b[0mactive_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m                 \u001b[0mactive_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactive_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    250\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    946\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m--> 948\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2421\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2422\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2216\u001b[0m                          .format(input.size(0), target.size(0)))\n\u001b[1;32m   2217\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2218\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2219\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2220\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: Target 12 is out of bounds."]}]},{"cell_type":"code","metadata":{"id":"7ccG9bBVaX9o","colab_type":"code","colab":{}},"source":["args = sequence_labelling_data_input(data_dir = \"data\",model_type = \"layoutlm\",output_dir = \"output_dir92\",model_name_or_path = 'bert-base-uncased',do_lower_case = True,max_seq_length = 512,do_predict = True,labels = \"/content/gdrive/My Drive/unilm/layoutlm/examples/seq_labeling/data/labels.txt\",fp16 = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oBmYRZOojiEt","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":589},"executionInfo":{"status":"ok","timestamp":1599465312876,"user_tz":-330,"elapsed":108106,"user":{"displayName":"Shubham Kotal","photoUrl":"","userId":"17346389805356489008"}},"outputId":"760578a0-778e-4658-d5ae-cb7818fdc465"},"source":["if __name__ == \"__main__\":\n","    main(args)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\n","\n","\n","\n","Evaluating:   0%|          | 0/7 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Evaluating:  14%|        | 1/7 [00:14<01:26, 14.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Evaluating:  29%|       | 2/7 [00:28<01:12, 14.49s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Evaluating:  43%|     | 3/7 [00:43<00:58, 14.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Evaluating:  57%|    | 4/7 [00:58<00:43, 14.53s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Evaluating:  71%|  | 5/7 [01:12<00:29, 14.52s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Evaluating:  86%| | 6/7 [01:27<00:14, 14.51s/it]\u001b[A\u001b[A\u001b[A\u001b[A\n","\n","\n","\n","Evaluating: 100%|| 7/7 [01:34<00:00, 13.51s/it]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"qCGON-SljfSu","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"2s3WVfCvj6MI","colab_type":"code","colab":{}},"source":["with open(\"custom_data/test.txt\",\"r\") as fp:\n","  text=fp.readlines()\n","  print(text)\n","\n","\n","  \n"],"execution_count":null,"outputs":[]}]}